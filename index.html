<meta charset="utf-8"/>
<html>
<head>
	<meta charset="utf-8">
	<meta name="description" content="欢迎来到ENNUI - 一个优雅的神经网络用户界面，让您能够轻松设计、训练和可视化神经网络。">
	<title>ENNUI ~ 优雅的神经网络用户界面 ~</title>

	<!-- MathJax cdn to render latex -->
	<script type="text/javascript" async
		src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  	</script>

	<!-- JSON-LD markup generated by Google Structured Data Markup Helper. -->
	<script type="application/ld+json">
	{
	  "@context" : "http://schema.org",
	  "@type" : "SoftwareApplication",
	  "name" : "ENNUI ~ 优雅的神经网络用户界面 ~'/8",
	  "author" : [ {
		"@type" : "Person",
		"name" : ""
	  }, {
		"@type" : "Person",
		"name" : "Zack Holbrook"
	  }, {
		"@type" : "Person",
		"name" : "Stefan Grosser"
	  }, {
		"@type" : "Person",
		"name" : "Hendrik Strobelt"
	  }, {
		"@type" : "Person",
		"name" : "Rikhav Shah"
	  } ]
	}
	</script>

	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-133726432-1"></script>

	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());
		gtag('config', 'UA-133726432-1');
	</script>

	<link rel="icon" type="image/x-icon" sizes="16x16" href="favicon.ico">
	<link rel='stylesheet' href='src/ui/style.css'>
	<script src='dist/bundle.js'></script>
</head>

<body>

<h1 style="display:none">ENNUI ~ 优雅的神经网络教学平台 ~</h1>
<p style="display:none">ENNUI 通过构建、训练和在浏览器中可视化深度神经网络，帮助人们了解深度学习。它具有易于使用的拖放界面。当您准备开始编码时，可以导出网络以生成Python或Julia代码！ </p>


<h6 style="display:none">关于 ENNUI</h6>
<p style="display:none">
	ENNUI为深度学习开发的所有阶段提供多种工具。画布提供了一个拖放界面，用于设计神经网络架构。这个设计可以通过导出到链接与朋友和同事轻松分享。
	您不仅可以设计神经网络，还可以在多个数据集上训练它们：MNIST、CIFAR-10等！在训练期间，您可以在进度标签中跟踪您的网络损失和准确率，还可以查看混淆矩阵。
	一旦训练完成，ENNUI提供了一套神经网络可视化工具，以更好地理解您的架构。
	ENNUI不断更新新功能，所以请继续关注！
</p>
<div id = 'main'>

	<!-- The leftmost strip to select tabs -->
	<div id = 'tabselector'>
		<div id = 'blanktab' class='top_neighbor_tab-selected'> </div>
		<div title = '神经网络' class = 'tab-selected option tab-option' id = 'network' data-optionValue = 'network'>
			<svg class = 'icon' xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path fill="none" d="M0 0h24v24H0V0z"/><path d="M11.99 18.54l-7.37-5.73L3 14.07l9 7 9-7-1.63-1.27zM12 16l7.36-5.73L21 9l-9-7-9 7 1.63 1.27L12 16zm0-11.47L17.74 9 12 13.47 6.26 9 12 4.53z"/></svg>
		</div>
		<div title = '训练过程' class = 'option tab-option bottom_neighbor_tab-selected' id = 'progress' data-optionValue = 'progress'>
			<svg class = 'icon' xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path fill="none" d="M0 0h24v24H0V0z"/><path d="M13.5 13.48l-4-4L2 16.99l1.5 1.5 6-6.01 4 4L22 6.92l-1.41-1.41z"/></svg>
		</div>
		<div title = '结果可视化' class = 'option tab-option' id = 'visualization' data-optionValue = 'visualization'>
			<svg class = 'icon' xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path fill="none" d="M0 0h24v24H0z"/><path d="M11 9h2v2h-2V9zm-2 2h2v2H9v-2zm4 0h2v2h-2v-2zm2-2h2v2h-2V9zM7 9h2v2H7V9zm12-6H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 18H7v-2h2v2zm4 0h-2v-2h2v2zm4 0h-2v-2h2v2zm2-7h-2v2h2v2h-2v-2h-2v2h-2v-2h-2v2H9v-2H7v2H5v-2h2v-2H5V5h14v6z"/></svg>
		</div>
		<div id = 'middleblanktab' > </div>

		<div title = '教学' class = 'option tab-option' id = 'education' data-optionValue = 'education'>
			<svg class = 'icon' xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path fill="none" d="M0 0h24v24H0V0z"/><path d="M12 3L1 9l4 2.18v6L12 21l7-3.82v-6l2-1.09V17h2V9L12 3zm6.82 6L12 12.72 5.18 9 12 5.28 18.82 9zM17 15.99l-5 2.73-5-2.73v-3.72L12 15l5-2.73v3.72z"/></svg>
		</div>
		<div id = 'bottomblanktab' > </div>
	</div>

	<!-- The left panel (menu) -->
	<div id = 'menu'>
		<div id = 'networkMenu'>
			<!-- The task -->
			<div id="tasks" class="category">
				<div class="categoryTitle" data-expanded="true">
					<div class="expander">
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class="categoryTitleText">
						教学任务
					</div>
				</div>
				<div class="option select-option" data-optionValue="MLP">多层感知机</div>
				<div class="option select-option" data-optionValue="CNN">卷积神经网络</div>
				<div class="option select-option" data-optionValue="RNN">循环神经网络</div>
			</div>

			<div id = 'layers' class = 'category'>
				<div class = 'categoryTitle' data-expanded = 'true'>
					<div class='expander'>
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class='categoryTitleText'>
						神经网络层
					</div>

				</div>
				<div class = 'option select-option' data-optionValue = 'dense'> Dense </div>
				<div class = 'option select-option' data-optionValue = 'conv2D'> Convolution </div>
				<div class = 'option select-option' data-optionValue = 'maxPooling2D'> Max Pooling </div>


				<div class = 'option-dropdown'>
					<div style="float:left">更多</div>
					<div style="float:right">〉</div>
					<div class='dropdown-content left'>
						<div title = '在训练过程中修改一批数据，使其更相似，从而加快收敛并获得更好的结果。'
							 class = 'option select-option' data-optionValue = 'batchnorm'> Batch Normalization </div>
						<div title = '在每个批次中忽略一部分随机的权重，以获得更好的泛化能力并加快训练。'
							 class = 'option select-option' data-optionValue = 'dropout'> Dropout </div>
						<div title = '将一组二维图像展平为一维特征向量。'
							 class = 'option select-option' data-optionValue = 'flatten'> Flatten </div>
						<div title = '将两个或多个输入（它们可以是1D或2D）连接起来'
							 class = 'option select-option' data-optionValue = 'concatenate'> Concatenate </div>
						<div title = '将两个或多个输入相加。'
							 class = 'option select-option last-dropdown' data-optionValue = 'add'> Add </div>
					</div>
				</div>
			</div>

			<div id = 'activations' class = 'category'>
				<div class = 'categoryTitle' data-expanded = 'true'>
					<div class='expander'>
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class='categoryTitleText'>
						激活函数
					</div>
				</div>
				<div class = 'option select-option' data-optionValue = 'relu'> ReLU </div>
				<div class = 'option select-option' data-optionValue = 'sigmoid'> Sigmoid </div>
				<div class = 'option select-option' data-optionValue = 'tanh'> Tanh </div>
			</div>
			<div id = 'templates' class = 'bottomCategory'>
				<div class = 'categoryTitle' data-expanded = 'true'>
					<div class='expander'>
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class='categoryTitleText'>
						模板
					</div>
				</div>
				<div class = 'option select-option' data-optionValue = 'blank'> 清空 </div>
				<div class = 'option select-option' data-optionValue = 'default'> 默认模板 </div>
				<div class = 'option select-option' data-optionValue = 'resnet'> ResNet残差网络 </div>
			</div>
		</div>

		<div id = 'progressMenu' style="display: none">
			<div id = 'optimizers' class = 'category'>
				<div class = 'categoryTitle' data-expanded = 'true'>
					<div class='expander'>
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class='categoryTitleText'>
						优化器
					</div>
				</div>
				<div id = "defaultOptimizer" class = 'option select-option selected' id = 'sgd' data-optionValue = 'sgd'> SGD </div>
				<div id = 'rmsprop' class = 'option select-option' data-optionValue = 'rmsprop'> RMSprop </div>
				<div id = 'adagrad' class = 'option select-option' data-optionValue = 'adagrad'> Adagrad </div>
				<div id = 'adam' class = 'option select-option' data-optionValue = 'adam'> Adam </div>
			</div>
			<div id = 'losses' class = 'category'>
				<div class = 'categoryTitle' data-expanded = 'true'>
					<div class='expander'>
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class='categoryTitleText'>
						损失函数
					</div>
				</div>
				<div id = 'defaultLoss' class = 'option select-option selected' data-optionValue = 'categoricalCrossentropy'>CrossEntropy</div>
				<div id = 'hinge' class = 'option select-option' data-optionValue = 'hinge'> Hinge </div>
				<div id = 'meanSquaredError' class = 'option select-option' data-optionValue = 'meanSquaredError'> MSE </div>
				<div id = 'meanAbsoluteError' class = 'option select-option' data-optionValue = 'meanAbsoluteError'> MAE </div>
			</div>
		</div>

		<div id = 'visualizationMenu' style="display: none">
			<div id = 'classes' class = 'category'>
				<div class = 'categoryTitle' data-expanded = 'true'>
					<div class='expander'>
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class='categoryTitleText'>
						分类
					</div>
				</div>
				<div class = 'option select-option selected' data-optionValue = 'all'> ALL </div>
				<div class = 'option select-option' data-optionValue = '0'> 0 </div>
				<div class = 'option select-option' data-optionValue = '1'> 1 </div>
				<div class = 'option select-option' data-optionValue = '2'> 2 </div>
				<div class = 'option select-option' data-optionValue = '3'> 3 </div>
				<div class = 'option select-option' data-optionValue = '4'> 4 </div>
				<div class = 'option select-option' data-optionValue = '5'> 5 </div>
				<div class = 'option select-option' data-optionValue = '6'> 6 </div>
				<div class = 'option select-option' data-optionValue = '7'> 7 </div>
				<div class = 'option select-option' data-optionValue = '8'> 8 </div>
				<div class = 'option select-option' data-optionValue = '9'> 9 </div>
			</div>
		</div>

		<div id = 'educationMenu' style="display: none">
			<div id = 'educationLayers' class = 'category'>
				<div class = 'categoryTitle' data-expanded = 'true'>
					<div class='expander'>
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class='categoryTitleText'>
						相关文章
					</div>

				</div>

				<div class = 'option select-option education-option' data-optionValue = 'Overview'> 概述 </div>
				<div class = 'option select-option education-option' data-optionValue = 'Overfitting'> Overfitting</div>
				<div class = 'option select-option education-option' data-optionValue = 'ResNets'>ResNets</div>
			</div>

			<div id = 'educationStory' class = 'category'>
				<div class = 'categoryTitle' data-expanded = 'true'>
					<div class='expander'>
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class='categoryTitleText'>
						神经网络层
					</div>
				</div>

				<div class = 'option select-option education-option' data-optionValue = 'Concatenate'> Concatenate </div>
				<div class = 'option select-option education-option' data-optionValue = 'Convolution'> Convolution </div>
				<div class = 'option select-option education-option' data-optionValue = 'Dropout'> Dropout </div>
				<div class = 'option select-option education-option' data-optionValue = 'Flatten'> Flatten </div>

			</div>
			<div id = 'educationModel' class = 'category'>
				<div class = 'categoryTitle data-expanded' = 'true'>
					<div class = 'expander'>
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class='categoryTitleText'>
						模型教学
					</div>
				</div>
				<div class = 'option select-option education-option' data-optionValue = 'MLP'> 感知机 </div>
				<div class = 'option select-option education-option' data-optionValue = 'CNN'> 卷积神经网络 </div>
				<div class = 'option select-option education-option' data-optionValue = 'newCNN'> 现代卷积神经网络 </div>
				<div class = 'option select-option education-option' data-optionValue = 'RNN'> 循环神经网络 </div>
				<div class = 'option select-option education-option' data-optionValue = 'newRNN'> 现代循环神经网络 </div>
			</div>
		</div>
		

	</div>

	

	<!-- The middle canvas -->
	<div id = 'middle'>
		<div id="taskSteps">
			<div id="taskTitle" class="task-title" onclick="toggleTaskSteps()">
				<span id="taskTitleText">未选择任务</span>
				<span id="arrow" class="arrow">&#9660;</span> <!-- 初始为收起箭头 -->
			</div>
			<div id="taskContent" class="task-content" style="display: none;">
				<ul id="stepsList">
					<!-- 这里将动态显示步骤内容 -->
				</ul>
			</div>
		</div>

		<div id = 'networkTab'>
			<svg id = 'svg'> </svg>
		</div>

		<div id = 'progressTab' style="display: none">

			<div id="loss-canvas"></div>

			<div id="accuracy-canvas"></div>

			<div id="confusion-matrix-canvas"></div>
		</div>

		<div id = 'visualizationTab' style="display: none">
			<div id='visulaization'></div>
			<div id='images'></div>
		</div>

		<div id = 'informationOverlay'>
			<div id='information'>欢迎来到ENNUI
				<div id="informationBody">~ 优雅的神经网络教学平台 ~</div>
				<div class="informationRow">
					<div class="informationColumn">
						自主创建神经网络 <br></br>
						<svg class = 'icon' xmlns="http://www.w3.org/2000/svg" width="30%" max-height="30%" viewBox="0 0 24 24"><path fill="none" d="M0 0h24v24H0V0z"/><path d="M11.99 18.54l-7.37-5.73L3 14.07l9 7 9-7-1.63-1.27zM12 16l7.36-5.73L21 9l-9-7-9 7 1.63 1.27L12 16zm0-11.47L17.74 9 12 13.47 6.26 9 12 4.53z"/></svg>
					</div>
					<div class="informationBlankColumn"></div>
					<div id="informationEducation" class="informationColumn">
						开始学习如何创建神经网络 <br></br>
						<svg class = 'icon' xmlns="http://www.w3.org/2000/svg" width="30%" max-height="30%" viewBox="0 0 24 24"><path fill="none" d="M0 0h24v24H0V0z"/><path d="M12 3L1 9l4 2.18v6L12 21l7-3.82v-6l2-1.09V17h2V9L12 3zm6.82 6L12 12.72 5.18 9 12 5.28 18.82 9zM17 15.99l-5 2.73-5-2.73v-3.72L12 15l5-2.73v3.72z"/></svg>
					</div>
				</div>
				<div id = 'acknowledgements'>
					<br>
					袁子茜，崔雯嘉，周艺瑶，田桓钟，姜逸涵 <br>
					开源资源<a class="overlayLinks" href="https://github.com/sunyia123/bbvdle" target="_blank">GitHub</a>.
				</div>
			</div>
		</div>

		<div id = 'educationTab' style="display: none">
			<div id="educationOverview">
				<div class="educationTitle" style="padding-top: 0px"> 关于深度学习神经网络 </div>
				<div class="educationSection"> 介绍ENNUI </div>
				<div class="educationContent">
					如果你第一次进入我们的教学平台, 
					可以参考以下的快速入门教学开始自己的神经网络学习之旅：
				</div>
				<!--<iframe class="educationVideo" src="https://www.youtube.com/embed/m0YnwAtPbb8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->

				<div style="text-align: center; font-size: 25px;"><a href="https://www.youtube.com/watch?v=m0YnwAtPbb8" target="blank">ENNUI 教学</a></div>
				<div class="educationSection"> 深度学习的基本知识 </div>
				<div class="educationContent">
					如果您不熟悉机器学习，下面的讲座视频是一个很好的介绍。
				</div>
				<div style="text-align: center; font-size: 25px;"><a href="https://video.odl.mit.edu/videos/9101a72a7d994d53800d1398fd885b88/embed/?start=339" target="blank">Gilbert Strang: Deep Learning</a></div>
				<!-- <iframe class="educationVideo" src="https://video.odl.mit.edu/videos/9101a72a7d994d53800d1398fd885b88/embed/?start=339" scrolling="no" frameborder="0" allowfullscreen></iframe> -->
			</div>

			<div id="educationConvolution">
				<div class="educationTitle"> 卷积网络 </div>
				<div class="educationAuthor">作者 <i>Gilbert Strang</i></div>

				<div class="educationContent">
					<p><strong>权重共享</strong> 这一词是卷积神经网络（CNNs）的核心思想。连接一层到另一层的权重矩阵<span class="math inline">\(A\)</span>  只有少数几个独立元素。 因此，优化这些权重的速度比全连接（密集）架构更快。</p>
					<p>在一维问题中，假设输入层（第0层）由一个向量<span class="math inline">\(v = (v_1,...,v_n)\)</span>表示。 卷积层将 <span class="math inline">\(v\)</span> 与一个具有常数对角线的权重矩阵 <span class="math inline">\(A\)</span> 相乘。然后，整个层中将重复使用相同的权重集（假设是 3 个权重）： </p>
					<p><span class="math display">\[A=
					\begin{bmatrix}
						a_{-1} &amp; a_0 &amp; a_1 &amp;  &amp;  \\
						&amp; a_{-1} &amp; a_0 &amp; a_1 &amp;  \\
						&amp; &amp; a_{-1} &amp; a_0 &amp; a_1 \\
					\end{bmatrix}\]</span> 这个矩阵 <span class="math inline">\(A\)</span> 有 <span class="math inline">\(n = 5\)</span> 个输出和 <span class="math inline">\(m = 3\)</span> 个输出。 A 是<strong>平移不变</strong>的: 卷积 = 滤波器 = Toeplitz 矩阵。卷积对于有许多像素的图像尤其重要。那 <span class="math inline">\(3\)</span> 个独立的权重<span class="math inline">\(a_{-1}, a_0, a_1\)</span> 可能在二维中变成 <span class="math inline">\(3 \times 3=9\)</span> 个权重。这 9 个数是 <span class="math inline">\(a_{ij}\)</span> 其中 <span class="math inline">\(i = -1,0,1\)</span> 和 <span class="math inline">\(j = -1,0,1\)</span>。一个包含其 8 个邻居（在一个 <span class="math inline">\(3 \times 3\)</span> 的方块中的输入）将与 <span class="math inline">\(a_{00}\)</span> 及其   <span class="math inline">\(8\)</span> 个邻居相乘——这 9 项的和给出一个输出 <span class="math inline">\(Av\)</span>。像往常一样，偏置向量  <span class="math inline">\(b\)</span> 会被加到结果中，并且  <span class="math inline">\(Av + b\)</span> 的每个分量都会通过像 ReLU 这样的函数进行激活（或不激活）：新层包含 <span class="math inline">\(\textrm{ReLU}(Av + b)\)</span>.</p>
					<p>这个二维矩阵 <span class="math inline">\(A\)</span> 并不容易显示。你应该可以看到，围绕一个大小为 <span class="math inline">\(n \times n\)</span> 的输入向量 <span class="math inline">\(v\)</span> 的一个 <span class="math inline">\(3 \times 3\)</span> 方块将产生一个大小为 <span class="math inline">\((n - 2) \times (n - 2)\)</span> 的输出，就像在一维中 5 个输入产生了 3 个输出一样。注意，在二维中我们只有 <span class="math inline">\(3 \times 3 = 9\)</span>（或者也许是 <span class="math inline">\(5 \times 5 = 25\)</span>）个独立的权重，因为卷积不仅是 <strong>共享权重</strong>，而且是 <strong>局部的</strong>。</p>
				</div>
			</div>

			<div id="educationResNets">
				<div class="educationTitle"> 残差网络 (ResNets) </div>
				<div class="educationAuthor">作者 <i>Zack Holbrook</i> and <i>Jesse Michel</i></div>

				<div class="educationContent">
					<p>2015 年，微软的一个研究团队凭借 ResNet 在 <a href="http://www.image-net.org/challenges/LSVRC/">ImageNet 大规模视觉识别挑战赛</a> 中获得了创纪录的表现。自 2015 年以来，ResNet 的变种一直主导着这一竞赛，超过了人类在该任务中的表现。它们已成为图像识别任务中广泛采用的架构，并且相对容易实现和训练。</p>
					<div class="educationSection">ResNet 架构</div>
					<p>ResNet 是一种卷积神经网络（CNN），具有 <strong>恒等捷径（identity shortcuts）</strong>，这是通过跳过某些层创建的网络路径，从而在网络中创建捷径。下面我们提供一个典型的 ResNet 示例：
				
						<img class="educationImage" src="dist/resnet.png" alt="Resnet image" width="50%">
				
						<div class="modelLink">
							<a class="modelLink" target="_newtab" href="http://math.mit.edu/ennui/#%7B%22graph%22:%5B%7B%22layer_name%22:%22Input%22,%22children_ids%22:%5B5,9%5D,%22parent_ids%22:%5B%5D,%22params%22:%7B%22dataset%22:%22mnist%22%7D,%22id%22:0,%22xPosition%22:100,%22yPosition%22:377%7D,%7B%22layer_name%22:%22Conv2D%22,%22children_ids%22:%5B6%5D,%22parent_ids%22:%5B0%5D,%22params%22:%7B%22filters%22:16,%22kernelSize%22:%5B3,3%5D,%22strides%22:%5B1,1%5D,%22activation%22:%22relu%22%7D,%22id%22:5,%22xPosition%22:169,%22yPosition%22:280%7D,%7B%22layer_name%22:%22Add%22,%22children_ids%22:%5B7,10%5D,%22parent_ids%22:%5B0,6%5D,%22params%22:%7B%22activation%22:%22relu%22%7D,%22id%22:9,%22xPosition%22:276,%22yPosition%22:411%7D,%7B%22layer_name%22:%22Conv2D%22,%22children_ids%22:%5B9%5D,%22parent_ids%22:%5B5%5D,%22params%22:%7B%22filters%22:16,%22kernelSize%22:%5B3,3%5D,%22strides%22:%5B1,1%5D%7D,%22id%22:6,%22xPosition%22:294,%22yPosition%22:280%7D,%7B%22layer_name%22:%22Conv2D%22,%22children_ids%22:%5B8%5D,%22parent_ids%22:%5B9%5D,%22params%22:%7B%22filters%22:16,%22kernelSize%22:%5B3,3%5D,%22strides%22:%5B1,1%5D,%22activation%22:%22relu%22%7D,%22id%22:7,%22xPosition%22:414,%22yPosition%22:280%7D,%7B%22layer_name%22:%22Add%22,%22children_ids%22:%5B11%5D,%22parent_ids%22:%5B9,8%5D,%22params%22:%7B%22activation%22:%22relu%22%7D,%22id%22:10,%22xPosition%22:521,%22yPosition%22:412%7D,%7B%22layer_name%22:%22Conv2D%22,%22children_ids%22:%5B10%5D,%22parent_ids%22:%5B7%5D,%22params%22:%7B%22filters%22:16,%22kernelSize%22:%5B3,3%5D,%22strides%22:%5B1,1%5D%7D,%22id%22:8,%22xPosition%22:541,%22yPosition%22:280%7D,%7B%22layer_name%22:%22Flatten%22,%22children_ids%22:%5B12%5D,%22parent_ids%22:%5B10%5D,%22params%22:%7B%7D,%22id%22:11,%22xPosition%22:708,%22yPosition%22:463%7D,%7B%22layer_name%22:%22Dense%22,%22children_ids%22:%5B13%5D,%22parent_ids%22:%5B11%5D,%22params%22:%7B%22units%22:32,%22activation%22:%22relu%22%7D,%22id%22:12,%22xPosition%22:702,%22yPosition%22:434%7D,%7B%22layer_name%22:%22Dropout%22,%22children_ids%22:%5B1%5D,%22parent_ids%22:%5B12%5D,%22params%22:%7B%22rate%22:0.5%7D,%22id%22:13,%22xPosition%22:778,%22yPosition%22:365%7D,%7B%22layer_name%22:%22Output%22,%22children_ids%22:%5B%5D,%22parent_ids%22:%5B13%5D,%22params%22:%7B%7D,%22id%22:1,%22xPosition%22:900,%22yPosition%22:377%7D%5D,%22hyperparameters%22:%7B%22learningRate%22:0.01,%22batchSize%22:64,%22optimizer_id%22:%22defaultOptimizer%22,%22epochs%22:6,%22loss_id%22:%22defaultLoss%22%7D%7D">
								模型链接
							</a>
						</div>
				
						恒等捷径意味着学习到的参数是残差。数学上，如果 <span class="math inline">\(R(x)\)</span> 是一系列卷积层与 ReLU 组合，称为 <strong>残差块</strong>，例如，假设 <span class="math display">\[R(x) = \textrm{Conv}(\textrm{ReLU}(\textrm{Conv}(x))).\]</span> 那么，残差块的输出将是 <span class="math inline">\(R(x) + x\)</span>，其中 <span class="math inline">\(x\)</span> 是恒等传递。如果神经网络试图逼近某个函数 <span class="math inline">\(F(x)\)</span>，那么一个完美的残差块 <span class="math inline">\(R^*(x)\)</span> 会使得 <span class="math inline">\(R^*(x) = F(x) - x\)</span>，这正是通过减去输入图像得到的残差。</p>
					<div class="educationSection">ResNet 的优势</div>
					<p>ResNet 的一个惊人特性是它的良好扩展性，使得深层神经网络仍然能够良好地训练。当网络变得更大时，许多问题会出现。</p>
					<p>大规模网络往往训练速度较慢，但 CNN 的 <strong>权重共享</strong> 意味着每个残差块需要训练的参数相对较少。大规模网络还往往面临 <strong>梯度消失</strong> 问题——在梯度下降中，权重更新会逐渐变得微不足道，导致即使有更多的训练时间，网络也无法改进。ResNet 中的恒等捷径为梯度提供了流动路径，从而避免了梯度消失的问题。</p>
				</div>
			</div>

			<div id="educationFlatten">
				<div class="educationTitle"> 展平层（Flatten） </div>
				<div class="educationAuthor">作者 <i>Zack Holbrook</i> and <i>Jesse Michel</i></div>

				<div class="educationContent">
					<p>展平层接受一个多维输入并产生一个一维输出。例如，CIFAR 数据集是一个包含图像的集合，它是三维的，因为它由32x32像素的二维图像组成，并且有3个颜色通道（红色、绿色、蓝色）。一个展平层可以将该数据集的数据作为输入，输出一个大小为 32*32*3 = 3072 的一维向量。</p>
				</div>
				
			</div>

			<div id="educationConcatenate">
				<div class="educationTitle">拼接层（Concentrate）</div>
				<div class="educationAuthor">作者<i>Zack Holbrook</i> 和 <i>Jesse Michel</i></div>
			
				<div class="educationContent">
					<p>拼接层接受两个或更多层，并通过将输入堆叠在一起将它们的输出拼接成一个单一输出。例如，它可以将两个大小为 10 的向量拼接成一个大小为 20 的向量，方法是将一个向量堆叠在另一个向量的上面。</p>
				</div>
			</div>
			

			<div id="educationDropout">
				<div class="educationTitle">丢弃层（Dropout）</div>
				<div class="educationAuthor">作者<i>Stefan Grosser</i> 和 <i>Jesse Michel</i></div>
			
				<div class="educationContent">
					<p>丢弃层在训练期间忽略一部分输入单元。例如，如果丢弃率为 0.1，则在每次前向传播中，丢弃层会随机选择 10% 的权重并将它们设为 0。添加一个丢弃率为 0 的丢弃层不会对网络产生任何影响，而丢弃率为 1 时，丢弃层将输出 0。</p>
			
					<p>丢弃通常用于防止<strong>过拟合</strong>（有关更多信息，请参阅我们的相关文章）。可以将丢弃视为让网络学习一组弱分类器，在测试时将它们结合起来形成一个更强的分类器。对于熟悉这个术语的人来说，这类似于使用集成模型的提升方法。丢弃层还有一个方便的特点，即加速训练，因为每次前向传播时所需的权重较少。</p>
				</div>
			</div>
			

			<div id="educationOverfitting">
				<div class="educationTitle">过拟合（Overfitting）</div>
				<div class="educationAuthor">作者 <i>Stefan Grosser</i> 和 <i>Jesse Michel</i></div>
				<div class="educationContent">
			
					<p>神经网络有时会学习得过于精确。它识别出仅仅是训练数据中特定的趋势，因此无法<strong>泛化</strong>。这种过度拟合训练数据的问题称为<strong>过拟合</strong>。下图展示了决策边界——决定分类器预测的曲线——在欠拟合、拟合良好（正常）和过拟合的情况下的变化。</p>
			
					<img class="educationImage" src="dist/overfitti_ng.png" alt="可能的决策边界" />
					<div class="modelLink">
						<a class="modelLink" target="_newtab" href="http://mlwiki.org/index.php/Overfitting">
							来源: ML Wiki
						</a>
					</div>
					<p>当分类器发生过拟合时，它在训练数据上的表现远远优于测试数据。因此，训练准确度会远高于验证准确度，训练损失会远低于验证损失。我们在下方提供了这个例子的可视化。</p>
			
					<img class="educationImage" style="float: left; max-width: 50%;" src="dist/loss_overfit.png" title="图：训练过程中的过拟合可视化" alt="训练过程中过拟合的可视化" />
			
					<img class="educationImage" style="float: right; max-width: 50%;" src="dist/accuracy_overfit.png" title="图：训练过程中的过拟合可视化" alt="训练过程中过拟合的可视化" />
			
					<div style="margin-top:10px;">
						该示例所用的架构如下所示：
					</div>
			
					<div class="figure">
						<img class="educationImage" style="max-width: 50%;" src="dist/overfitting_network.png" alt="网络架构" >
			
						<div class="modelLink">
							<a class="modelLink" target="_newtab" href="https://math.mit.edu/ennui/#%7B%22graph%22:%5B%7B%22layer_name%22:%22Input%22,%22children_ids%22:%5B2%5D,%22parent_ids%22:%5B%5D,%22params%22:%7B%22dataset%22:%22cifar%22%7D,%22id%22:0,%22xPosition%22:100,%22yPosition%22:399%7D,%7B%22layer_name%22:%22Conv2D%22,%22children_ids%22:%5B3%5D,%22parent_ids%22:%5B0%5D,%22params%22:%7B%22filters%22:16,%22kernelSize%22:%5B3,3%5D,%22strides%22:%5B1,1%5D,%22kernelRegularizer%22:%22none%22,%22regScale%22:0.1,%22activation%22:%22relu%22%7D,%22id%22:2,%22xPosition%22:261,%22yPosition%22:453%7D,%7B%22layer_name%22:%22Flatten%22,%22children_ids%22:%5B1%5D,%22parent_ids%22:%5B2%5D,%22params%22:%7B%7D,%22id%22:3,%22xPosition%22:585,%22yPosition%22:484%7D,%7B%22layer_name%22:%22Output%22,%22children_ids%22:%5B%5D,%22parent_ids%22:%5B3%5D,%22params%22:%7B%7D,%22id%22:1,%22xPosition%22:900,%22yPosition%22:399%7D%5D,%22hyperparameters%22:%7B%22learningRate%22:0.1,%22batchSize%22:64,%22optimizer_id%22:%22defaultOptimizer%22,%22epochs%22:15,%22loss_id%22:%22defaultLoss%22%7D%7D">
								模型链接
							</a>
						</div>
					</div>
					<br/><br/>
					<p>过拟合展示了交叉验证为何如此重要；如果没有验证集，我们将无法识别模型无法泛化的问题。</p>
					<p>那么，如何应对过拟合，确保模型找到可以泛化的特征呢？</p>
					<div class="educationSection">正则化</div>
					<p>防止过拟合的一种方法是正则化，它通过加入一个新的项来引导模型走向更简单的解决方案。回想一下，在分类问题中，我们从一对对输入和它们的分类 <span class="math display">\[(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n).\]</span> 开始。我们希望找到一个函数 <span class="math inline">\(f\)</span>，它能准确预测新数据样本的类别。因此，如果我们的原始问题是 <span class="math display">\[\min_f \sum_{i=1}^{n} C(f(x_i), y_i),\]</span> 其中 <span class="math inline">\(C\)</span> 计算当预测 <span class="math inline">\(f(x_i)\)</span> 时的代价，当真实值是 <span class="math inline">\(y_i\)</span>，那么正则化损失将是 <span class="math display">\[\min_f \sum_{i=1}^{n} C(f(x_i), y_i) + \lambda R(f),\]</span> 其中 <span class="math inline">\(R(f)\)</span> 是正则化项，定义为当 <span class="math inline">\(f\)</span> 更复杂时它会变大，而 <span class="math inline"> \(\lambda>0\) </span> 是一个可调节的参数，控制正则化的程度。层的复杂性有多种定义，但在我们的案例中，我们会说，具有较低 <span class="math inline">\(L2\)</span>-范数的层较为简单。正式地，我们将 <span class="math inline">\(L2\)</span>-范数定义为 <span class="math display">\[\text{norm}(A) = \sqrt{\sum_i \sum_j a_{ij}^2}.\]</span> 例如，给定矩阵 <span class="math display">\[A =
					\begin{bmatrix}
					1 & 2 \\
					0 & -2
					\end{bmatrix},\]</span> 其 L2-范数为 <span class="math display">\[||A||_2 = \sqrt{1^2 + 2^2 + 0^2 + (-2)^2} = 3.\]</span></p>
					<p>有几个原因解释了为何惩罚增大的 <span class="math inline">\(L2\)</span>-范数是一个合理的做法。如果我们假设分类器会发生过拟合，那么加入惩罚项 <span class="math inline">\(\lambda R(f)\)</span> 将会引导决策边界远离这一状态。这可以看作是给分类器增加“摆动空间”。此外，这种高 <span class="math inline">\(L2\)</span>-范数的惩罚是鼓励丢弃无用信息的一种方式。惩罚项促使层的权重变小，而层的权重越接近零，其作为特征的影响就越小。</p>
					<p>这种复杂性的概念导致了 <span class="math inline">\(L1\)</span>-和 <span class="math inline">\(L2\)</span>-范数成为正则化的一种形式。在 <span class="math inline">\(L2\)</span>-正则化的情况下，我们可以将 <span class="math inline">\(\lambda ||W||_2\)</span> 加入到损失函数中，针对给定层 <span class="math inline">\(W.\)</span> 当然，还有其他的正则化方式，但现在我们来看看另一种方法。</p>
					<!-- TODO: 以后可能解释 L1-正则化 -->
					<div class="educationSection">Dropout</div>
					<p>防止过拟合的另一种方法是名为 dropout 的技术。Dropout 层在训练期间忽略一部分输入单元（有关更多信息，请参阅我们的 dropout 层解释）。有两种直觉可以解释为什么 dropout 有助于防止过拟合。Dropout 可以看作是一种集成学习——将一组弱（欠拟合）分类器的结果进行某种方式的组合，例如采用多数类别。在每个批次中，网络的一个新部分作为弱分类器进行训练。在验证阶段，整个网络都会被使用，从而有效地将所有分类器结合起来提供一个单一的结果。另一种看法是，经过多次运行后，dropout 强制网络架构的所有部分都被使用。因此，训练集的任何一个特征都不会过于影响网络，避免网络集中在仅对训练集特有的伪影上。</p>
					<div class="educationSection">结论</div>
					<p>过拟合会妨碍分类器在未见数据上的表现。正则化和 dropout 是两种广泛使用且容易实现的防止过拟合的方法。结合这些方法与交叉验证，使得构建更具泛化能力的模型变得更加容易。</p>
				</div>
			
			</div>
			<div id="educationMLP">
				<div class="educationTitle">感知机</div>
				<div class="educationContent">
					<div class="educationSection">（一）概述</div>
					<p>
						多层感知机（MLP：Multi-Layer Perceptron）由感知机(PLA: Perceptron Learning Algorithm)推广而来。它最主要的特点是有多个神经元层，因此也叫深度神经网络(DNN: Deep Neural Networks)。
					</p>
					<p>
						感知机是单个神经元模型，是较大神经网络的前身。神经网络的强大之处在于它们能够学习训练数据中的表示，以及如何将其与想要预测的输出变量联系起来。从数学上讲，它们能够学习任何映射函数，并且已经被证明是一种通用的近似算法。
					</p>
					<p>
						神经网络的预测能力来自网络的分层或多层结构。而多层感知机是指具有至少三层节点（输入层，一些中间层和输出层）的神经网络。每一层中的节点与相邻层的节点完全连接。
					</p>
					<div class="educationSection">（二）各层说明</div>
					<p><strong>1．输入层：</strong> 每个输入特征对应一个神经元，输入层不涉及计算。</p>
					<p><strong>2．单个或多个隐藏层：</strong></p>
					<ul>
						<li>介绍</li>
						<p>至少有一个隐藏层，通常包含多个神经元。每一层的神经元与前一层的神经元连接。</p>
						<p>隐藏层负责对输入数据进行非线性转换。在每个神经元中，输入值会与相应的权重相乘，再加上偏置，最后通过激活函数进行变换。这样每一层都能够提取特征。</p>
						<li>神经元的计算</li>
						<ul>
							<li>对于第L层的第j个神经元，其输出为：
								<span class="math inline">\[z_j=W_j\cdot a^{(1-1)}+b_j\]</span>
								<span class="math inline">\[a_j{=}f(z_j)\]</span>
								<p>其中,<span class="math inline">\(W_j\)</span>是权重，<span class=",math inline">\(b_j\)</span>是偏置，<span class=",math inline">\(a^{(l-1)}\)</span>是前一层的输出，<span class="math inline">\(f(z_j)\)</span>是激活函数 (如ReLU) 。 假设隐藏层有3个神经元，并且我们使用 ReLU 激活函数。每个神经元的输入是输入层的输出(x1, x2),它们会被分别与权重和偏置进行加权和偏置计算。</p>
							</li>
							<li>隐藏层神经元的计算过程：
								<p>对于第一个神经元：<span class="math inline">\[z_1=W_1\cdot x+b_1=w_1\cdot x_1+w_{12}\cdot x_2+b_1\]</span></p>
								<p>激活函数(ReLU)会将计算结果输出：<span class="math inline">\[a_1{=}\mathrm{ReLU}(z_1)\]</span></p>
								<p>对于第二个和第三个神经元，同样的计算方式。</p>
							</li>
						</ul>
						<li>激活函数的选择：</li>
						<ul>
							<li>ReLU：适用于隐藏层，能够加速训练，避免梯度消失问题。</li>
							<li>Sigmoid：可以用于隐藏层，但在深层网络中容易导致梯度消失。</li>
							<li>Tanh：适用于隐藏层，能够解决一些 Sigmoid 的问题，具有对称性。</li>
						</ul>
					</ul>
					<p><strong>3．输出层：</strong> </p>

					<ul>
						<li>介绍</li>
						<p>输出层负责将隐藏层的结果转化为最终预测值。</p>
						<li>神经元计算（以 Sigmoid 为例）
							<p>输出层的计算可以表示为：</p>
							<span class="math inline">\[a^{(\mathrm{hidden})}+b_{\mathrm{out}}\]</span>
							<p>其中，<span class="math inline">\(\mathbf{a^{(hiddcn)}}\)</span>是隐藏层的输出,<span class="math inline">\(W_{\mathrm{out}}\)</span>是输出层的权重，<span class="math inline">\(\widehat{y}=\sigma(z_{\mathrm{out}})=\frac1{1+e^{-z_{\mathrm{out}}}}\)</span>
								,其中<span class="math inline">\(\widehat{y}\)</span>是模型的预测输出，代表样本属于某一类的概率。
								</p>
						</li>
					</ul>
					<div class="educationSection">（三）思考题</div>
					<ol>
						<li>
							<strong>过拟合问题：</strong> 为什么 Dropout 可以防止过拟合？除此之外还有哪些方法？
						</li>
						<li>
							<strong>可解释性问题：</strong> 为什么深度学习被称为“黑箱”模型？如何提高其可解释性？
						</li>
						<li>
							<strong>激活函数问题：</strong> 为什么需要使用非线性激活函数？
						</li>
					</ol>
				</div>

			</div>
			<div id="educationCNN">
				<div class="educationTitle">卷积神经网络</div>
				<div class="educationContent">
					<div class="educationSection">（一）概述</div>
					<p>卷积神经网络（Convolutional Neural Network，简称 CNN）是一种特别适合处理和分析图像的深度学习模型。CNN 的结构和原理借鉴了生物视觉系统，尤其是人类视觉皮层的工作方式，使它能够有效提取图像中的各种特征，如边缘、纹理和形状。与传统的神经网络不同，CNN 通过卷积层和池化层来提取特征，并用全连接层对提取到的特征进行分类或回归。</p>
					<div class="educationSection">（二）CNN模型提出</div>
					<ol>
						<li>CNN 发展背景与基本思想</li>
						<p>在 1980 年代，计算机视觉领域的研究者发现传统的机器学习方法在图像处理任务上表现不佳。图像中包含的大量像素信息使得简单的机器学习模型难以有效提取有用的特征，且数据维度高、参数多。这些挑战使得研究者们开始寻找新的方法来自动从图像数据中提取特征。</p>
						<p>此时，研究人员借鉴了生物视觉系统的工作原理。生物学家发现，人类视觉皮层在处理视觉信息时，会逐层提取图像中的不同层次信息，从而形成对图像内容的理解。基于这种启发，研究者们开始设计一种模拟生物视觉系统的层次化结构模型，即卷积神经网络。</p>
						<li>Yann LeCun 和 LeNet 的诞生</li>
						<p>Yann LeCun 是法国计算机科学家，被誉为“卷积神经网络之父”。他在 1989 年提出了一个简单的卷积神经网络模型，用于手写数字识别任务。LeCun 的工作受生物视觉系统启发，并基于如下两个关键原则来设计 CNN：</p>
						<ul>
							<li>局部连接：LeCun 提出的 CNN 只对图像的局部区域进行处理，而不是全图连接。这种方法不仅减少了参数数量，还能更有效地学习图像的局部特征。</li>
							<li> 权重共享：在卷积操作中，CNN 的每一个卷积核在图像的不同位置共享相同的权重，从而减少了需要学习的参数数量。这种结构可以让网络自动识别图像中重复的模式，比如边缘或特定的形状。</li>
						</ul>
						<p>这两个设计思想极大降低了模型复杂度，使得 CNN 能够在当时有限的计算资源下运行。</p>
						<li>LeNet-5 架构（1998 年）</li>
						<p>在 1998 年，Yann LeCun 等人提出了 LeNet-5，这是一个多层的卷积神经网络结构，主要用于手写数字识别（例如识别 0-9 的手写数字）。LeNet-5 的结构包括：</p>
						<ul>
							<li>卷积层：用于提取图像的局部特征。</li>
							<li>池化层：通过下采样操作减少特征的尺寸，从而降低模型的计算量。</li>
							<li>全连接层：用于将卷积和池化提取到的特征综合，用于最终的分类。</li>

						</ul>
						<p>LeNet-5 的诞生标志着 CNN 的第一次成功应用，并在手写数字识别任务上取得了令人瞩目的表现。然而，由于当时的计算能力和数据集规模有限，CNN 的进一步发展受到限制，无法应用于更大规模的任务。</p>
						<li>深度学习和计算能力的推动</li>
						<p>2000 年代，随着 GPU 的出现和计算能力的提升，深度学习的研究逐渐复兴。大规模数据集（如 ImageNet）的出现为 CNN 的训练提供了丰富的数据。AlexNet 的提出将 CNN 推向了新的高度。</p>
						<li>AlexNet 和 CNN 的重大突破（2012 年）</li>
						<p>在 2012 年，由 Alex Krizhevsky 等人设计的 AlexNet 模型参加了 ImageNet 大规模视觉识别竞赛（ILSVRC），并在分类任务上取得了巨大的成功，准确率远超其他方法。AlexNet 的结构和 LeNet-5 相似，但 AlexNet 增加了深度层数、使用了 ReLU 激活函数以及 Dropout 正则化等技术来提升性能。</p>
						<p>AlexNet 的成功证明了 CNN 的潜力，并在计算机视觉领域掀起了深度学习的热潮。此后，更多深层次的 CNN 架构被提出，如 VGG、GoogLeNet、ResNet 等，使 CNN 成为计算机视觉的核心模型。</p>
					</ol>
					<div class="educationSection">（三）CNN模型的核心</div>
					<image class="educationImage" src="resources/educationimages/CNN_1.png"  alt="CNN_1"></image>
					<ol>
						<li>输入层</li>
						<p>输入层通常接收图像数据，图像的像素值组成三维数据张量：宽度、高度和通道（如彩色图像有红、绿、蓝 3 个通道）。</p>
						<p>图像数据通过 CNN 的层次结构处理，从低层的边缘特征逐渐提取到高层的复杂形状和结构特征。</p>
						<li>卷积层（Convolutional Layer）</li>
						<ol>
							<li><strong >卷积运算：</strong>卷积层的核心操作是卷积运算。在图像处理领域中，卷积运算可以通过小矩阵（卷积核或滤波器）在图像上滑动，提取局部区域的特征。例如，一个 3×33×3 的卷积核可以在图像上移动，对每个 3×33×3 的子区域进行点积运算，生成特征图（Feature Map）。</li>
							<li><strong >特征提取：</strong>卷积核通过学习得到特定的权重，能够提取不同的特征，例如边缘、纹理或颜色。每一个卷积核代表一种特定的图像特征，多个卷积核的堆叠则可以提取图像的多种特征。</li>
							<li><strong>特征图的输出：</strong>经过卷积操作的结果会生成新的特征图，将其传递给下一层。特征图的深度等于卷积核的数量</li>
						</ol>
						<li>激活函数（Activation Function）</li>
						<ol>
							<li><strong>(Rectified Linear Unit)：</strong>在卷积操作之后，通常会对特征图应用激活函数。ReLU 是一种常用的激活函数，它将所有负值置零，保持正值不变，从而引入非线性因素。</li>
							<li><strong>非线性特征提取：</strong>激活函数的作用是提高模型的表达能力，让 CNN 可以学习复杂的非线性特征。在 CNN 中，ReLU 函数的计算效率高且可以有效防止梯度消失。</li>
						</ol>
					
						<li>池化层（Pooling Layer）</li>
						<ol>
							<li><strong>池化运算：</strong>池化层的主要作用是降低特征图的尺寸，减少计算量，并且通过特征的抽象增加模型的鲁棒性。常见的池化方法是最大池化（Max Pooling），它从特征图的每个局部区域中取最大值，以保留最显著的特征。</li>
							<li><strong>降低维度与增强平移不变性：</strong>降低维度与增强平移不变性：池化层有助于减少模型的计算需求，同时使得模型对图像中的细微平移更加不敏感。例如，一个 2×22×2 的最大池化操作可以将每 2×22×2 区域缩小为一个值，从而减少特征图的大小。</li>
						</ol>
						<li>多层卷积与池化的组合</li>
						<p>在实际 CNN 结构中，通常会堆叠多个卷积层和池化层以构建深层网络。低层卷积提取图像的基础特征，如边缘和简单形状。中层卷积提取更复杂的特征，如局部模式或图案，而高层卷积则学习全局的结构特征。每一层卷积和池化的输出特征图被传递到下一层，逐步形成更加抽象的特征表示。</p>
						<li>展平层（Flattening Layer）</li>
						<p>卷积和池化层的输出是一个三维的特征图张量。为了便于全连接层处理，需要将三维特征展平成一维向量，这个过程称为“展平”。展平后的特征向量包含了图像的高级特征，并准备传递到全连接层进行进一步处理。</p>
						<li>全连接层（Fully Connected Layer）</li>
						<ol>
							<li><strong>全连接操作：</strong>全连接层将展平的特征向量输入到一个或多个全连接的神经网络层中。这些层将每一个输入特征与输出类别进行加权组合，从而输出预测结果。</li>
							<li><strong>输出结果：</strong>最后一层全连接层通常使用 Softmax 激活函数，用于多分类任务，输出一个概率分布，代表图像属于各类别的可能性。</li>
						</ol>
						<li>损失函数与反向传播</li>
						<ol>
							<li><strong>损失函数：</strong>在训练过程中，CNN 的输出会与真实标签进行比较，计算损失值（例如交叉熵损失），表示预测结果与实际结果的差异。</li>
							<li><strong>反向传播和梯度下降：</strong>通过反向传播算法计算损失函数相对于每层参数的梯度，进而通过梯度下降算法更新卷积核和全连接层的参数，使得 CNN 逐渐优化，从而提高预测准确率。</li>
				
						</ol>
						<li>模型输出</li>
						<p>CNN 在处理一张图像后输出一个分类或预测结果。对于图像分类任务，输出层通常表示预测类别及其概率。例如，如果是手写数字识别任务，输出会是一个 0-9 的数字。</p>
					</ol>
				</div>
			</div>	
			 
			<div id = "educationnewCNN">
				<div class="educationTitle">现代卷积神经网络</div>
				<div class="educationContent">
					<div class="educationSection">（一）概述</div>
					<p>虽然深度神经网络的概念非常简单——将神经网络堆叠在一起。但由于不同的网络架构和超参数选择，这些神经网络的性能会发生很大变化。本章将按照时间顺序介绍以下模型：</p>
					<ul>
						<li><strong>AlexNet。</strong>它是第一个在大规模视觉竞赛中击败传统计算机视觉模型的大型神经网络；</li>
						<li><strong>使用重复块的网络（VGG）。</strong>它重复使用由卷积层和卷积层（用来代替全连接层）来构建深层网络;</li>
						<li><strong>网络中的网络（NiN）。</strong>它重复使用由卷积层和卷积层（用来代替全连接层）来构建深层网络;</li>
						<li><strong>含并行连结的网络（GoogLeNet）。</strong>它使用并行连结的网络，通过不同窗口大小的卷积层和最大汇聚层来并行抽取信息；</li>
						<li><strong>残差网络（ResNet）。</strong>它通过残差块构建跨层的数据通道，是计算机视觉中最流行的体系架构；</li>
						<li><strong>稠密连接网络（DenseNet）。</strong>它的计算成本很高，但给我们带来了更好的效果。</li>
					</ul>

					<div class="educationSection">（二）深度卷积神经网络（AlexNet）</div>
					<p>2012年，AlexNet横空出世。它首次证明了学习到的特征可以超越手工设计的特征。它一举打破了计算机视觉研究的现状。 AlexNet使用了8层卷积神经网络，并以很大的优势赢得了2012年ImageNet图像识别挑战赛。</p>
					<p>AlexNet和LeNet的设计理念非常相似，但也存在显著差异。首先，AlexNet比相对较小的LeNet5要深得多。AlexNet由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。其次，AlexNet使用ReLU而不是sigmoid作为其激活函数。</p>
					<image class="educationImage" src="resources/educationimages/newCNN_1.png"  alt="newCNN_1" title="从LeNet（左）到AlexNet(右)" ></image>
					<p>在AlexNet的第一层，卷积窗口的形状是11×11。由于ImageNet中大多数图像的宽和高比MNIST图像的多10倍以上，因此，需要一个更大的卷积窗口来捕获目标。第二层中的卷积窗口形状被缩减为5×5，然后是3×3。此外，在第一层、第二层和第五层卷积层之后，加入窗口形状为3×3、步幅为2的最大汇聚层。而且，AlexNet的卷积通道数目是LeNet的10倍。</p>
					<p>在最后一个卷积层后有两个全连接层，分别有4096个输出。 这两个巨大的全连接层拥有将近1GB的模型参数。由于早期GPU显存有限，原版的AlexNet采用了双数据流设计，使得每个GPU只负责存储和计算模型的一半参数。幸运的是，现在GPU显存相对充裕，所以现在很少需要跨GPU分解模型。</p>
					<p>此外，AlexNet将sigmoid激活函数改为更简单的ReLU激活函数。 一方面，ReLU激活函数的计算更简单，它不需要如sigmoid激活函数那般复杂的求幂运算。 另一方面，当使用不同的参数初始化方法时，ReLU激活函数使训练模型更加容易。 当sigmoid激活函数的输出非常接近于0或1时，这些区域的梯度几乎为0，因此反向传播无法继续更新一些模型参数。 相反，ReLU激活函数在正区间的梯度总是1。 因此，如果模型参数没有正确初始化，sigmoid函数可能在正区间内得到几乎为0的梯度，从而使模型无法得到有效的训练。</p>
					
					<div class ="educationSection">（三）使用块的网络（VGG）</div>
					<p>虽然AlexNet证明深层神经网络卓有成效，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络。使用块的想法首先出现在牛津大学的视觉几何组（visual geometry group）的VGG网络中。通过使用循环和子程序，可以很容易地在任何现代深度学习框架的代码中实现这些重复的架构。</p>
					<p>经典卷积神经网络的基本组成部分是下面的这个序列：（1）带填充以保持分辨率的卷积层；（2）非线性激活函数，如ReLU；（3）汇聚层，如最大汇聚层。而一个VGG块与之类似，由一系列卷积层组成，后面再加上用于空间下采样的最大汇聚层。在最初的VGG论文中 (Simonyan and Zisserman, 2014)，
						作者使用了带有3×3卷积核、填充为1（保持高度和宽度）的卷积层，和带有2×2汇聚窗口、步幅为2（每个块后的分辨率减半）的最大汇聚层 。
						与AlexNet、LeNet一样，VGG网络可以分为两部分：第一部分主要由卷积层和汇聚层组成，第二部分由全连接层组成。</p>
					<image class="educationImage" src="resources/educationimages/newCNN_2.png"  alt="newCNN_2" title = " 从AlexNet到VGG"></image>
					<p>原始VGG网络有5个卷积块，其中前两个块各有一个卷积层，后三个块各包含两个卷积层。 第一个模块有64个输出通道，每个后续模块将输出通道数量翻倍，直到该数字达到512。由于该网络使用8个卷积层和3个全连接层，因此它通常被称为VGG-11。</p>
					
					<div class="educationSection">（四）网络中的网络（NiN）</div>
					<p>LeNet、AlexNet和VGG都有一个共同的设计模式：通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。 AlexNet和VGG对LeNet的改进主要在于如何扩大和加深这两个模块。 或者，可以想象在这个过程的早期使用全连接层。然而，如果使用了全连接层，可能会完全放弃表征的空间结构。 网络中的网络（NiN）提供了一个非常简单的解决方案：在每个像素的通道上分别使用多层感知机。</p>
					<p>卷积层的输入和输出由四维张量组成，张量的每个轴分别对应样本、通道、高度和宽度。 另外，全连接层的输入和输出通常是分别对应于样本和特征的二维张量。 NiN的想法是在每个像素位置（针对每个高度和宽度）应用一个全连接层。 如果我们将权重连接到每个空间位置，我们可以将其视为1×1卷积层，或作为在每个像素位置上独立作用的全连接层。 从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征（feature）。 </p>
					<p> NiN块以一个普通卷积层开始，后面是两个1×1的卷积层。这两个1×1卷积层充当带有ReLU激活函数的逐像素全连接层。 第一层的卷积窗口形状通常由用户设置。 随后的卷积窗口形状固定为1×1。 </p>
					<image class="educationImage" src="resources/educationimages/newCNN_3.png"  alt="newCNN_3" title = " VGG和NiN的块之间主要的架构差异"></image>
					<p>最初的NiN网络是在AlexNet后不久提出的，显然从中得到了一些启示。 NiN使用窗口形状为11×11、5×5和3×3的卷积层，输出通道数量与AlexNet中的相同。 每个NiN块后有一个最大汇聚层，汇聚窗口形状为3×3，步幅为2。</p>
					<p>NiN和AlexNet之间的一个显著区别是NiN完全取消了全连接层。 相反，NiN使用一个NiN块，其输出通道数等于标签类别的数量。最后放一个全局平均汇聚层（global average pooling layer），生成一个对数几率 （logits）。NiN设计的一个优点是，它显著减少了模型所需参数的数量。然而，在实践中，这种设计有时会增加训练模型的时间。</p>

					<div class = "educationSection">（五）含并行连结的网络（GoogLeNet）</div>
					<p>GoogLeNet吸收了NiN中串联网络的思想，并在此基础上做了改进。 这篇论文的一个重点是解决了什么样大小的卷积核最合适的问题。 毕竟，以前流行的网络使用小到1×1，大到11×11的卷积核。 本文的一个观点是，有时使用不同大小的卷积核组合是有利的。 </p>
					<p>在GoogLeNet中，基本的卷积块被称为Inception块（Inception block），Inception块由四条并行路径组成。 前三条路径使用窗口大小为1×1、3×3和5×5的卷积层，从不同空间大小中提取信息。 中间的两条路径在输入上执行1×1卷积，以减少通道数，从而降低模型的复杂性。 第四条路径使用3×3最大汇聚层，然后使用1×1卷积层来改变通道数。 这四条路径都使用合适的填充来使输入与输出的高和宽一致，最后我们将每条线路的输出在通道维度上连结，并构成Inception块的输出。在Inception块中，通常调整的超参数是每层输出通道数。 </p>
					<image class="educationImage" src="resources/educationimages/newCNN_4.png"  alt="newCNN_4" title = " Inception块的架构"></image>
					<p>GoogLeNet一共使用9个Inception块和全局平均汇聚层的堆叠来生成其估计值。Inception块之间的最大汇聚层可降低维度。 第一个模块类似于AlexNet和LeNet，Inception块的组合从VGG继承，全局平均汇聚层避免了在最后使用全连接层。</p>
					<image class="educationImage" src="resources/educationimages/newCNN_5.png"  alt="newCNN_5" title = "GoogLeNet架构"></image>

					<div class = "educationSection">（六）残差网络（ResNet）</div>
					<p>假设我们的原始输入为x，而希望训练出的理想映射为f(x)（作为 图3.6上方激活函数的输入）。 图3.6左图虚线框中的部分需要直接拟合出该映射f(x)，而右图虚线框中的部分则需要拟合出残差映射f(x)−x。 残差映射在现实中往往更容易优化。 我们只需将 图3.6中右图虚线框内上方的加权运算（如仿射）的权重和偏置参数设成0，那么f(x)即为恒等映射。 实际中，当理想映射f(x)极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。图3.6右图是ResNet的基础架构–残差块（residual block）。 在残差块中，输入可通过跨层数据线路更快地向前传播。 </p>
					<image class="educationImage" src="resources/educationimages/newCNN_6.png"  alt="newCNN_6" title = "正常块（左）和残差块（右）"></image>
					<p>ResNet沿用了VGG完整的3×3卷积层设计。 残差块里首先有2个有相同输出通道数的3×3卷积层。 每个卷积层后接一个批量规范化层和ReLU激活函数。 然后我们通过跨层数据通路，跳过这2个卷积运算，将输入直接加在最后的ReLU激活函数前。 这样的设计要求2个卷积层的输出与输入形状一样，从而使它们可以相加。 如果想改变通道数，就需要引入一个额外的1×1卷积层来将输入变换成需要的形状后再做相加运算。 </p>
					<image class="educationImage" src="resources/educationimages/newCNN_7.png"  alt="newCNN_7" title = " 包含和不包含1×1卷积层的残差块"></image>
					<p>ResNet的前两层跟之前介绍的GoogLeNet中的一样： 在输出通道数为64、步幅为2的7×7卷积层后，接步幅为2的3×3的最大汇聚层。 不同之处在于ResNet每个卷积层后增加了批量规范化层。</p>
					<p>GoogLeNet在后面接了4个由Inception块组成的模块。 ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。 第一个模块的通道数同输入通道数一致。 由于之前已经使用了步幅为2的最大汇聚层，所以无须减小高和宽。 之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。</p>
					<image class="educationImage" src="resources/educationimages/newCNN_8.png"  alt="newCNN_8" title = " ResNet-18架构"></image>

					<div class = "educationSection">（七）稠密连接网络（DenseNet）</div>
					<p>ResNet将f分解为两个部分：一个简单的线性项和一个复杂的非线性项，根据泰勒展开式，如果向将f拓展成超过两部分信息，一种解决方案就是DenseNet。</p>
					<image class="educationImage" src="resources/educationimages/newCNN_9.png"  alt="newCNN_9" title = " ResNet&DenseNet"></image>
					<p>上图ResNet（左）和DenseNet（右）在跨层连接上的主要区别：使用相加和使用连结ResNet和DenseNet的关键区别在于，
						DenseNet输出是连接（用图中的[,]表示）而不是如ResNet的简单相加。 因此，在应用越来越复杂的函数序列后，我们执行从x到其展开式的映射：
						<span class="math inline">\[\mathbf{x}\to[\mathbf{x},f_1(\mathbf{x}),f_2([\mathbf{x},f_1(\mathbf{x})]),f_3([\mathbf{x},f_1(\mathbf{x}),f_2([\mathbf{x},f_1(\mathbf{x})])]),\ldots].\]</span>
						</p>
					<p>稠密网络主要由2部分构成：稠密块（dense block）和过渡层（transition layer）。 前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。</p>
					<p>DenseNet首先使用同ResNet一样的单卷积层和最大汇聚层。接下来，类似于ResNet使用的4个残差块，DenseNet使用的是4个稠密块。 与ResNet类似，我们可以设置每个稠密块使用多少个卷积层。在每个模块之间，ResNet通过步幅为2的残差块减小高和宽，DenseNet则使用过渡层来减半高和宽，并减半通道数。与ResNet类似，最后接上全局汇聚层和全连接层来输出结果。</p>


					

				</div>
				<div id = "educationRNN">
					<div class="educationTitle">循环神经网络</div>
					<div class="educationContent">
						<div class="educationSection">（一）概述</div>
						<p>循环神经网络 (RNN) 是一种人工神经网络，旨在处理顺序数据，例如时间序列或自然语言。它们具有反馈连接，使它们能够保留先前时间步骤的信息，从而能够捕获时间依赖性。这使得 RNN 非常适合语言建模、语音识别和顺序数据分析等任务。苹果的Siri和谷歌的语音搜索都使用RNN。</p>
						<image class="educationImage" src="resources/educationimages/RNN_1.png"  alt="RNN_1" ></image>
						<p>RNN 是一种可用于对序列数据建模的神经网络。 RNN 由前馈网络组成，其行为与人脑相似。简而言之，循环神经网络可以以其他算法无法做到的方式预测顺序数据。标准神经网络中的所有输入和输出都是相互独立的，但是在某些情况下，例如在预测短语的下一个单词时，前面的单词是必要的，因此必须记住前面的单词。结果，RNN 应运而生，它使用隐藏层来克服这个问题。 RNN 最重要的组成部分是隐藏状态，它记住有关序列的特定信息。</p>
						<p>RNN 有一个内存，用于存储有关计算的所有信息。它对每个输入采用相同的设置，因为它通过在所有输入或隐藏层上执行相同的任务来产生相同的结果。</p>
						<image class="educationImage" src="resources/educationimages/RNN_2.png"  alt="RNN_2" title = ></image>
						<p>循环神经网络对序列的每个元素使用相同的权重，从而减少了参数的数量，并允许模型泛化为不同长度的序列。由于其设计，RNN 泛化到序列数据以外的结构化数据，例如地理或图形数据。与许多其他深度学习技术一样，循环神经网络相对较旧。它们最初是在 20 世纪 80 年代开发的，但直到最近我们才充分认识到它们的潜力。 20 世纪 90 年代长短期记忆 (LSTM) 的出现，加上计算能力的提高和我们现在必须处理的大量数据，确实将 RNN 推到了最前沿。</p>
						
						<div class ="educationSection">（二）RNN模型提出</div>
						<ol>
							<li><strong>基本RNN结构：</strong>为了解决普通DNN无法有效获取上下文信息的缺点，RNN最基本的改良点在于增加一个“模块”用于存储上下文信息。下图为一个典型RNN的结构示意图：</li>
							<image class="educationImage" src="resources/educationimages/RNN_3.png"  alt="RNN_3" ></image>
							<p>上图是一个典型的RNN结构图，初看可能会不太理解。理解首先不看右侧的矩阵，只看左侧的顺序网络，即图(b)，表示的就是一个普通的前馈神经网络。 接下来回头看图(a)，RNN相比于一般前馈网络，增加了一个保存上下文信息的权重矩阵，也即每次计算输出不仅要考虑当前输入数据，还要考虑序列数据的上下文信息。</p>
							<li><strong>RNN展开结构：</strong>
								我们知道了RNN模型增加了一个权重矩阵用于存储输入序列的上下文信息，接下来我们来介绍RNN结构如何进行模型计算以及上下文信息如何应用到RNN结构。为了更好地理解RNN计算方式，下图是一个序列展开的RNN示意图（即上图a的时序展开图）：
							</li>
							<image class="educationImage" src="resources/educationimages/RNN_4.png"  alt="RNN_4"  ></image>
							<p>其中表示时刻的模型输入，表示对应的输入结果。RNN模型计算公式如下：
								<span class="math inline">\[\begin{array}{l}{y_{i}=g(Vh_{i})}\\{h_{i}=f(Ux_{i}+Wh_{i-1})}\end{array}\]</span>
								由计算公式可以看出，隐藏层的输出隐变量在RNN中既与当前时刻输入有关，又与上一时刻的隐变量有关。因此可以认为包含了影响当前输入信息的“上下文”信息，而可学习的参数矩阵决定了上下文信息对当前影响程度。 值得注意的是，在整个模型处理期间，参数矩阵是使用的同一个矩阵。
							</p>
							<li>
								<strong>时间反向传播（Backpropagation Through Time, BPTT）：</strong>
								<p>BPTT是训练RNN的核心算法，它将反向传播算法扩展到时间序列，以学习时间相关的信息。由于RNN具有循环结构，BPTT的关键在于将误差在时间维度上展开，使每个时间步都能调整相应的参数。</p>
								<p>BPTT算法的基本思想是将RNN在时间维度上“展开”（Unroll），将一个循环结构的网络拆解为多个时序步骤的等效神经网络，这样每个时间步都可以看作一个全连接层的传播。通过这种展开，RNN在每个时间步的状态和输出变得“独立”，可以使用常规反向传播算法在时间维度上计算误差和梯度。</p>
							</li>
						</ol>
						<div class ="educationSection">（三）RNN模型结构变化</div>
						<p>根据输入长度与输出序列长度的不同，可以将RNN模型结构分为N to N，N to 1, 1 to N，及N to M四种:</p>
						<ol>
							<li><strong>N to N结构RNN模型</strong></li>
							<p>第一种是常见的输入长度与输出长度相同的RNN结构，也就表示每一个输入数据都有对应的一个输出值，可以用于逐序列判断或分类任务，如序列标注、视频帧分类、NER及分词等任务。其结构图如下：</p>
							<image class="educationImage" src="resources/educationimages/RNN_5.png"  alt="RNN_5" ></image>
							<span class = "math inline">\[\begin{array}{l}{y_{i}=g(Vh_{i})}\\{h_{i}=f(Ux_{i}+Wh_{i-1})}\end{array}\]</span>
							<li><strong>N to 1结构RNN模型</strong> </li>
							<p>表示输入一个序列只生成一个输出值(通常用尾数据对应输出值)。其意义是序列的输出结果蕴含整条序列数据的语义信息及上下文信息。常见应用：文字分类、文章分类及图像分类等任务。</p>
							<image class="educationImage" src="resources/educationimages/RNN_6.png"  alt="RNN_6" ></image>
							<span class = "math inline">\[Y = Y_{\text{smallN}} = g(Vh_N)/h_i = f(Ux_i + Wh_{i-1})\]</span>
							<li><strong> 1 to N结构RNN模型</strong></li>
							<p> 表示一个输入数据对应输出一个序列的模型。其意义表示一个起始状态或者种子数据，生成一个序列的输出结果。常见应用包括由图像生成文章，由类别生成音乐、文章及代码等，由种子数据生成序列的任务。</p>
							<image class="educationImage" src="resources/educationimages/RNN_7.png"  alt="RNN_7" ></image>
							<image class="educationImage" src="resources/educationimages/RNN_8.png"  alt="RNN_8" ></image>
							<p>1 to N结构RNN模型根据输入只有一个向量，输入位置的不同，可以分为只在首个时刻输入(上左图)和在每个时刻均输入(上右图)两种结构。其中第一种结构计算方法如下：</p>
							<span class = "math inline">\[\begin{aligned}
								y_i &= g(Vh_i) \\
								h_i &=
								\begin{cases} 
								f(Wh_{i-1}) & \text{where } i > 1 \\ 
								f(Ux_1 + Wh_0) & \text{where } i = 1 
								\end{cases}
								\end{aligned}\]</span>
							<p>类似地，第二种结构计算方法如下：</p>
							<span class = "math inline"></span>
							<li><strong> N to M结构RNN模型</strong></li>
							<p>即输入及输出序列不等长的结构。N和M分别为输入序列长度及输出序列长度，该结构我们采用一个N to 1结构RNN及一个1 to M结构组合来实现，详细结构如下图：</p>
							<image class="educationImage" src="resources/educationimages/RNN_9.png"  alt="RNN_9" ></image>
							<p> 由上图可以看出，将两个不同长度的RNN进行组合，能够控制模型的输出序列长度。在两个模型之间，增加了一个上下文向量，由第一个RNN模型的输出计算得来，向量包含着输入序列的语义信息与序列信息。在上图中是将上下文向量作为了第二个RNN模型的输入数据，并在第二个RNN模型对于初始隐藏变量进行随机初始化。通常将第一个RNN称为encoder（编码器），第二个RNN称为decoder（解码器）此外，还可以利用上下文向量对第二个RNN模型的隐藏变量进行初始化，结构如下：</p>
							<image class="educationImage" src="resources/educationimages/RNN_10.png"  alt="RNN_10" ></image>
							<p>通过N to M结构RNN模型，可以适应各类序列处理任务，常见的如机器翻译、语音识别、文本摘要及阅读理解等任务。由于输入输出都是序列，该模型也称为seq2seq模型。常用的上下文向量的计算方法包含如下三种：</p>
							<span class = "math inline"></span>
							<p> 其中，第一种计算方法为直接将encoder的输出作为上下文向量；第二种计算方法为通过变化encoder的输出计算得到；第三种计算方法为利用一个encoder的输出序列计算得到。此外，由于encoder的输出只变换成 上下文向量传入decoder进行了计算，难免造成decoder计算序列加长导致的上下文信息衰减。由此，人们引入了注意力机制（Attention）来增强数据信息，我们在attention机制部分进行详解。</p>

						</ol>
						<div class ="educationSection">（四）梯度爆炸和梯度消失</div>
						<ol>
							<li><strong>什么是梯度爆炸和梯度消失</strong></li>
							<p>就其输入而言，梯度是偏导数，梯度量化了当输入稍微改变时函数的输出变化的程度。函数的斜率也称为梯度。斜率越陡，模型学习的速度越快，梯度就越高。另一方面，如果斜率为零，模型将停止学习。梯度用于测量所有权重相对于误差变化的变化。</p>
							<image class="educationImage" src="resources/educationimages/RNN_11.png"  alt="RNN_11" ></image>
							<ul>
								<li><strong>梯度爆炸：</strong>当算法无缘无故地给权重赋予荒谬的高优先级时，就会发生梯度爆炸。幸运的是，截断或压缩梯度是解决此问题的简单方法。</li>
								<li><strong>梯度消失：</strong>当梯度值太小时，就会发生梯度消失，导致模型停止学习或花费太长时间。这是 20 世纪 90 年代的一个大问题，而且它比梯度爆炸更难解决。</li>

							</ul>
							<li><strong>如何解决RNN的梯度爆炸或梯度消失</strong></li>
							<ul>
								<li><strong>解决梯度爆炸：</strong>梯度裁剪，即为梯度更新时的梯度设置上限，当超过阈值将强制裁剪，避免出现过高阈值。</li>
								<li><strong>解决梯度消失：</strong>使用Relu激活函数解决梯度消失的原理是，Relu函数在自变量大于0是，因变量恒为1，由此避免梯度过小；改用变种版本的RNN结构，常见的包括LSTM模型及GRU模型。</li>

							</ul>
						</ol>
						<div class ="educationSection">（五）RNN优缺点</div>
						<ol>
							<li><strong>RNN的优点</strong></li>
							<ul>
								<li>有效处理顺序数据，包括文本、语音和时间序列。</li>
								<li>与前馈神经网络不同，处理任意长度的输入。</li>
								<li>跨时间步共享权重，提高训练效率。</li>
								
							</ul>
							<li><strong>RNN的缺点</strong></li>
							<ul>
								<li>容易出现梯度消失和爆炸问题，阻碍学习。</li>
								<li>训练可能具有挑战性，尤其是对于长序列。</li>
								<li>计算速度比其他神经网络架构慢。</li>
							</ul>
						</ol>

						<div class = "educationSection">（六）RNN实战</div>
						<ol>
							<li>理论实现</li>
							<ul>
								<li>输入层</li>
								<ol>
									<li><strong>当前事件输入：</strong>记为<span class ="math inline">\(\mathbf{x}_{\mathbf{t}}\)</span>，表示在时间t输入的数据。</li>
									<li><strong>隐藏状态（记忆）：</strong>记为\(\mathbf{h}_{\mathbf{t-1}}\)，表示从前一个时间步<span class ="math inline">\(t-1\)</span>传递下来的隐藏状态，包含历史信息。</li>
								</ol>
								<li>隐藏层</li>
								<p>RNN的核心是通过隐藏状态来保持之前时间步的信息，并与当前的输入结合。标准RNN隐藏层的更新公式如下：</p>
								<span class ="math inline">\[\mathrm{h}_t=\phi(W_\mathrm{h}\mathrm{h}_{t-1}+W_xx_t+b_\mathrm{h})\]</span>
								<p><span class ="math inline">\(W_h\)</span>:隐藏层权重矩阵，用于调整前一时间步的隐藏状态<span class ="math inline">\(h_t-1\)</span>的影响。</p>
								<p><span class ="math inline">\(W_x\)</span>:：输入权重矩阵，用于调整当前输入<span class ="math inline">\(x_t\)</span>的影响。</p>
								<p><span class ="math inline">\(b_h\)</span>:偏置项，用于对输出进行平移。</p>
								<p><span class ="math inline">\(phi)</span>：激活函数，通常选择<span class ="math inline">\(\text{tanh}\)</span>的影响。或 ReLU激活函数，使得网络具有非线性表达能力。
									该更新公式的核心是将历史隐藏状态和当前输入线性组合，再通过激活函数更新为当前时间步的隐藏状态<span class ="math inline">\(h_t\)</span>。每一时间步都会执行该更新，使得RNN可以逐步积累时间序列信息。
								</p>
								<li>输出层</li>
								<p>输出层根据隐藏状态生成每个时间步的输出 <span class ="math inline">\(o_t\)</span>：<span class ="math inline">\(o_t=W_0h_t+b_0\)</span></p>
								<p><span class ="math inline">\(W_o\)</span>：输出层权重矩阵，将隐藏状态映射到输出空间。</p>
								<p><span class ="math inline">\(b_o\)</span>：输出层偏置项。</p>
								<p>RNN的输出可以是每个时间步的输出（适合序列输出）或最终时间步的隐藏状态（适合序列分类）。</p>
								<li>前向传播过程</li>
								<p>标准RNN的前向传播过程是一个循环递归的过程。RNN层会从 t=1t=1t=1 一直传播到 TTT（时间步数），逐步计算每个时间步的隐藏状态和输出。这种递归使得RNN能够在较短的时间序列中捕捉依赖关系。</p>
								
							</ul>
							<li>代码实现</li>
							<p>RNN及其变体是非常经典且有意义的工作，故代码实现有多种方式，总体来说分为自购建与API调用。</p>
							<image class="educationImage" src="resources/educationimages/RNN_12.png"  alt="RNN_12" ></image>
							<ul>
								<li><strong>自构建</strong></li>
								<ol>
									<li><strong>独热编码：</strong>即NLP中的基本操作one-hot encoding，将文本预处理（string->num），并将索引映射为互补相同的单位向量，方便后续模型读入。</li>
									<li><strong>初始化模型参数：</strong>需要定义隐藏层参数（重要）、输出层参数、附加梯度等模型参数。</li>
									<li><strong>模型/网络定义：</strong>根据需求与RNN定义去搭建模型，包括隐状态返回（初始化时）、计算与输出，以及模型的激活与迭代。</li>
									<li><strong>预测：</strong>定义预测函数来生成prefix（一个用户提供的包含多个字符的字符串）之后的新字符。</li>
									<li><strong>梯度裁剪：</strong>正常的RNN反向传播会产生O（T）的矩阵乘法链，T较大时可能导致梯度爆炸或消失，故需要进行梯度裁剪。</li>
									<li><strong>训练：</strong>将处理后数据“喂”给模型，进行迭代训练（顺序分区/随机抽样），以困惑度或epoch作为停止训练指标。</li>
									<li><strong>输出：</strong>训练好的模型/文本预测结果</li>

								</ol>
								<li><strong>API调用</strong></li>
								<p>自购建的方式可以实现不同方案/策略的RNN，但无论是代码实现难度、效率/性能都不是最优选择，由于RNN类模型是经典模型，故Tensorflow、Pytorch等主流框架中均做了定义（API）与优化，便于我们快速搭建模型并应用，通过API的代码实现非常简洁，全流程为数据集读入->模型定义/引入（通过API）->训练与预测。代码核心即模型的引入。</p>
							</ul>
							
						</ol>

					</div>

				</div>
				<div id = "educationnewRNN">
					<div class="educationTitle">现代循环神经网络</div>
					<div class="educationContent">
						<div class="educationSection">（一）概述</div>
						<p>在上一章中，我们介绍了如何利用循环神经网络（RNN）来建立语言模型以处理文本数据。然而，面对当今日益复杂的序列学习任务，传统RNN模型可能会遇到一些困难。一个突出的问题是数值不稳定性，特别是在长序列数据中，RNN模型的梯度很容易发生消失或爆炸，使得模型难以有效学习长时依赖。尽管我们可以通过梯度裁剪等技巧来缓解这个问题，但它们并不能完全解决问题。为此，我们需要引入一些更强大的模型设计，来让RNN更具表现力和稳定性。本章将按顺序介绍以下网络结构：</p>
						<ol>
							<li><strong>GRU（门控循环单元）：</strong>GRU是一种改进的RNN结构，利用更新门和重置门来控制信息流，简化了长时依赖的处理，使训练更高效。</li>
							<li><strong>LSTM（长短期记忆网络）：</strong>LSTM通过引入遗忘门、输入门和输出门，能够更有效地捕捉长时依赖关系，适合处理长序列数据。</li>
							<li><strong>深度RNN：</strong>深度RNN由多层隐藏层堆叠而成，使模型能够逐层捕捉更复杂的序列特征，提升模型的表达能力。</li>
							<li><strong>双向RNN：</strong>双向RNN通过同时进行正向和反向计算，可以结合前后文信息，使得模型在处理自然语言任务时更具上下文感知能力。</li>
							<li><strong>Transformer：</strong>Transformer采用自注意力机制，允许模型在序列的任意位置间建立直接依赖关系，从而高效处理长序列，极大提升了训练速度和并行计算能力。</li>

						</ol>
						<div class="educationSection">（二）门控循环单元（GRU）</div>
						<p>GRU（门控循环单元）和LSTM（长短期记忆网络）是两种广泛应用的改进型RNN架构。它们通过引入“门”结构，允许模型更有效地控制信息流，选择性地保留或忘记特定信息。GRU和LSTM能缓解梯度消失问题，更好地捕捉长程依赖。GRU结构相对简单，计算速度快；而LSTM在处理更复杂的依赖关系时更具优势，因此这两种结构都成为序列建模中的重要工具，本节中讲介绍GRU单元。</p>
						<p>GRU单元的工作方式类似一条“智能传送带”，它能根据需要对输入信息和隐藏状态进行“放行”或“拦截”操作，来过滤掉不重要的信息并保存旧的隐藏状态。具体来说，它通过重置门（reset gate）和更新门（update gate）来控制信息的流动，其中门机制都是带有激活函数的全连接层。</p>
						<ol>
							<li><strong>重置门</strong></li>
							<p>重置门可以看作是一个“拦截”开关，它会根据当前输入内容来选择性地忘记部分旧隐藏状态的信息。如果重置门检测到某些过时的信息（比如过早的背景信息）可以忽略，它就会“清除”这些内容，削弱过去的影响，这样模型就可以更专注于新输入的数据。</p>
							<span class = "math inline">\[\mathbf{R}_t=\sigma(\mathbf{X}_t\mathbf{W}_{xr}+\mathbf{H}_{t-1}\mathbf{W}_{hr}+\mathbf{b}_r)\]</span>
							<ul>
								<li><span class = "math inline">\(R_t\)</span>是重置门在时间步t的输出。</li>
								<li>σ一般是Sigmoid激活函数，其输出范围在0到1之间。</li>
								<li><span class = "math inline">\(\mathbf{W}_{xr}\)</span>和<span class = "math inline">\(\mathbf{W}_{hr}\)</span>分别是重置门的输入和递归权重矩阵。</li>
								<li><span class = "math inline">\(H_t-1\)</span>是前一时刻的隐藏状态。</li>
								<li><span class = "math inline">\(X_t\)</span>是当前时刻的输入。</li>
							</ul>
							<li><strong>更新门</strong></li>
							<p>更新门负责决定当前时刻的输入信息和之前记忆中的隐藏状态信息，哪一部分应该被“放行”到下一步。换句话说，更新门会选择保留多少旧信息，以及吸收多少新信息，从而帮助GRU在新旧信息之间找到平衡。</p>
							<span class = "math inline">\[\mathbf{Z}_t=\sigma(\mathbf{X}_t\mathbf{W}_{xz}+\mathbf{H}_{t-1}\mathbf{W}_{hz}+\mathbf{b}_z)\]</span>
							<ul>
								<li><span class = "math inline">\(Z_t\)</span>是更新门在时间步<span class = "math inline">\(t\)</span>的输出。</li>
								<li><span class = "math inline">\(\mathbf{W}_{xz}\)</span>和<span class = "math inline">\(\mathbf{W}_{hz}\)</span>分别是更新门的输入和递归权重矩阵。</li>

							</ul>
							<li><strong>候选隐状态</strong></li>
							<p>当输入一个新的数据时，GRU单元会让更新门和重置门协同工作，首先通过重置门基于当前输入和前一时刻隐藏状态计算得到候选隐藏状态， 每当重置门中的值接近1时，网络倾向于结合更多前面的记忆，效果更接近RNN。对于重置门中的值接近0，任何预先存在的隐状态都会被重置为默认值，效果更接近多层感知机。</p>
							<span class = "math inline">\[\tilde{\mathcal{H}}_t=\tanh(\mathbf{X}_t\mathbf{W}_{x\mathbf{h}}+(\mathbf{R}_t\odot\mathbf{H}_{t-1})\mathbf{W}_{h\mathbf{h}}+\mathbf{b}_h)\]</span>
							<ul>
								<li><span class = "math inline">\(\tilde{\mathcal{H}}_t\)</span>是候选隐状态。</li>
								<li><span class = "math inline">\(\tanh\)</span>是双曲正切激活函数，其输出范围在-1到1之间。</li>
								<li><span class = "math inline">\(\mathbf{W}_{xh}\)</span>和<span class = "math inline">\(\mathbf{W}_{hh}\)</span>分别是候选隐状态的输入和递归权重矩阵。</li>
								<li><span class = "math inline">\(\odot\)</span>表示逐元素乘法（Hadamard乘积）。</li>
							</ul>
							<li><strong>更新隐状态<span class = "math inline">\(H_t\)</span></strong></li>
							<p>在最后一步，网络需要计算<span class = "math inline">\(H_t\)</span>，该向量将根据更新门<span class = "math inline">\(Z_t\)</span>决定当前隐藏状态
								<span class = "math inline">\(H_t\)</span>的更新比例，即要更新多少新的隐藏状态，并传递到下一个单元中。在这个过程中，我们需要使用更新门，
								它决定了当前记忆内容<span class = "math inline">\(\tilde{\mathcal{H}}_t\)</span>和前一时间步<span class = "math inline">\(H_t-1\)</span>中需要保留和收集的信息是什么。这一过程可以表示为：</p>
								<span class = "math inline">\[\mathcal{H}_t=\mathbf{Z}_t\odot\mathbf{H}_{t-1}+(1-\mathbf{Z}_t)\odot\tilde{\mathbf{H}}_t\]</span>
								<p>这个公式表明，当前时刻的隐藏状态是前一时刻的隐藏状态（通过更新门调整）和候选隐状态的线性组合。如果更新门
									<span class = "math inline">\(Z_t\)</span>接近1，那么<span class = "math inline">\(H_t\)</span>将接近 <span class = "math inline">\(H_t-1\)</span>，
									即前一时刻的隐藏状态将几乎不变地传递到当前时刻，完全忽略<span class = "math inline">\(X_t\)</span>的影响。如果更新门 
									<span class = "math inline">\(Z_t\)</span>接近0，那么 <span class = "math inline">\(H_t\)</span>将接近 
									<span class = "math inline">\(\tilde{\mathcal{H}}_t\)</span>，即当前时刻的隐藏状态将主要由候选隐状态决定。</p>
									<image class="educationImage" src="resources/educationimages/newRNN_1.png"  alt="newRNN_1" ></image>

						</ol>
						<div class="educationSection">（三）长短期记忆网络（LSTM）</div>
						<p>GRU（门控循环单元）和LSTM（长短期记忆网络）是两种广泛应用的改进型RNN架构。它们通过引入“门”结构，允许模型更有效地控制信息流，选择性地保留或忘记特定信息。GRU和LSTM能缓解梯度消失问题，更好地捕捉长程依赖。GRU结构相对简单，计算速度快；而LSTM在处理更复杂的依赖关系时更具优势，因此这两种结构都成为序列建模中的重要工具，本节中讲介绍LSTM网络。</p>
						<p>LSTM（长短期记忆网络）单元的工作方式与GRU类似，但实现方式是通过三个关键的门机制：遗忘门（forget gate）、输入门（input gate）和输出门（output gate），并引入记忆元（memory cell）来实现信息的流动和记忆管理，其中门机制都是带有激活函数的全连接层。</p>
						<ol>
							<li><strong>门机制</strong></li>
							<p>遗忘门决定了哪些旧的记忆信息可以被“遗忘”。它会根据当前输入和上一个时刻的隐藏状态来选择性地保留或忘记部分记忆。比如，当遗忘门检测到某些过时的背景信息时，就会让这些信息在记忆中被“清除”，这样可以减轻旧信息的干扰。</p>
							<p>输入门控制哪些新信息可以被“储存”到记忆中，并决定当前时刻的输入内容将如何影响记忆单元。输入门的输出会与候选记忆相乘，从而筛选出当前要存入的有效信息。输出门则会在最终决定使用多少更新的记忆元信息。</p>
							<span class= "math inline">\[F_t=\sigma(X_tW_{xf}+H_{t-1}W_{hf}+b_f)\]</span>
							<span class= "math inline">\[I_t=\sigma(X_tW_{xi}+H_{t-1}W_{hi}+b_i)\]</span>
							<span class= "math inline">\[O_t=\sigma(X_tW_{xo}+H_{t-1}W_{ho}+b_o)\]</span>

							<li><strong>候选记忆内容<span class = "math inline">\(\tilde{\mathcal{C}}_t\)</span></strong></li>
							<p>候选记忆元<span class = "math inline">\(\tilde{\mathcal{C}}_t\)</span>用于生成潜在的新的记忆内容，根据输入数据
								<span class = "math inline">\(X_t\)</span>和前一隐藏状态
								<span class = "math inline">\(\tilde{\mathcal{H}}_t-1\)</span>生成候选记忆元，然后通过输入门将有效信息添加到记忆单元中。它决定了新的信息进入记忆的程度，其本质等同于RNN中的隐状态
								<span class = "math inline">\(\tilde{\mathcal{H}}_t\)</span>。</p>
								<span class = "math inline">\[\tilde{C}_t=\tanh(X_tW_{xc}+H_{t-1}W_{hc}+b_c)\]</span>

							<li><strong>记忆元</strong></li>
							<p>记忆元<span class = "math inline">\(\tilde{\mathcal{C}}_t\)</span>是LSTM的核心，结合了前一时刻的记忆（经过遗忘门筛选）和当前时刻的候选记忆（经过输入门筛选），从而产生更新后的记忆。对比GRU的隐状态不同的是：1.
								<span class = "math inline">\(\tilde{\mathcal{C}}_t\)</span>的值没有范围限制；2.控制过去记忆的参数与候选记忆的参数相互独立，而GRU的是此消彼长的关系；3.
								<span class = "math inline">\(\tilde{\mathcal{C}}_t\)</span>独立于
								<span class = "math inline">\(\tilde{\mathcal{H}}_t\)</span>进行传递，即LSTM有
								<span class = "math inline">\(\tilde{\mathcal{C}}_t\)</span>作为长期记忆不断累积，
								<span class = "math inline">\(\tilde{\mathcal{H}}_t\)</span>负责短期更新，而GRU只不断选择性更新
								<span class = "math inline">\(\tilde{\mathcal{H}}_t\)</span>。</p>
								<span class = "math inline">\[C_t=F_t\odot C_{t-1}+I_t\odot\tilde{C}_t\]</span>

							<li><strong>隐藏状态<span class = "math inline">\(H_t\)</span></strong></li>
							<p>最后，输出门决定当前时刻要输出多少隐藏状态。输出门的值会控制隐藏状态的输出比例，让模型可以更灵活地选择输出多少信息。同时，当前隐藏状态
								<span class = "math inline">\(\tilde{\mathcal{H}}_t\)</span>是经过过滤的记忆内容，用于传递给下一个单元。</p>
							<span class = "math inline">\[H_t=0_t\odot\tanh(C_t)\]</span>
							<p>公式中的tanh函数确保<span class = "math inline">\(H_t\)</span>始终在（-1,1）之间。</p>
							<p>过这些机制，LSTM可以对每一时刻的输入进行灵活处理，决定哪些信息该记住、哪些该忘记，并将重要信息传递到后续的时刻，因而能够更有效地捕捉长时依赖关系，其图形化演示如下：</p>
							<image class="educationImage" src="resources/educationimages/newRNN_2.png"  alt="newRNN_2" ></image>
							<image class="educationImage" src="resources/educationimages/newRNN_3.png"  alt="newRNN_3" ></image>
						</ol>

						<div class="educationSection">（四）深度循环神经网络</div>
						<p>单层RNN在建模复杂的序列关系时可能不够灵活。通过增加RNN的层数，我们可以构建“深度”RNN，逐层捕捉数据中不同层次的模式。这种深层架构可以提取出更复杂的序列特征，显著提升模型的表达能力，适应更多类型的任务需求，但由于深度结构引入了更多的参数，训练过程更容易出现梯度消失或梯度爆炸问题，因此通常需要配合梯度裁剪、正则化等技术。</p>
						<p>深度RNN包含多层RNN单元，每一层的输出作为下一层的输入，从而形成更深的网络结构。随着层数的增加，模型的容量也会增加，使其能够更好地表示复杂的时序模式和长时依赖关系。底层RNN层通常捕捉较低级的特征（例如基本的时序模式），而随着网络层次的加深，逐层的高层RNN可以学习更加复杂的模式或长时依赖关系，其图形化显示如下：</p>
						<image class="educationImage" src="resources/educationimages/newRNN_4.png"  alt="newRNN_4" ></image>
						<p>形象地说，<span class = "math inline">\(H_t^{(l)}\)</span>同时受到<span class = "math inline">\(H_t-1^{(l)}\)</span>和<span class = "math inline">\(H_t^{(l-1)}\)</span>的影响。</p>
						<p>在深度RNN中，每一时刻t的输入不仅通过第一层RNN进行处理，得到的隐藏状态还会传递到下一层。假设深度RNN包含L层，记第l层的计算公式为：</p>
						<span class = "math inline">\[H_t^{(1)}=\sigma\left(W_{xh}^{(1)}\cdot H_t^{(1-1)}+W\ln h^{(1)}\cdot H_{t-1}^{(1)}+b^{(1)}\right)\]</span>

						<div class="educationSection">（五）双向循环神经网络</div>
						<p>在很多序列任务中，不仅需要考虑过去的信息，有时还需考虑未来的上下文。例如，在命名实体识别等任务中，一个词的含义可能受到前后文的影响。双向RNN通过正向和反向传播同时处理序列，可以更全面地捕捉上下文信息，提高模型的整体表现。</p>
						<p>在双向RNN中，每一时刻的隐藏状态包括前向隐藏状态和后向隐藏状态，两者的输出会被合并，作为该时刻的输出，其图形化显示如下：</p>
						<image class="educationImage" src="resources/educationimages/newRNN_5.png"  alt="newRNN_5" ></image>
						<p>假设输入序列为<span class = "math inline">\(\mathbf{X=\{x1,x2,\cdots,xT\}}\)</span>，则双向RNN的计算过程如下：</p>
						<ol>
							<li><strong>前向隐藏状态：</strong>沿正序方向，从前到后计算隐藏状态。</li>
							<span class = "math inline">\[\overrightarrow{\mathrm{h}_t}=f\left(W^{(f)}x_t+U^{(f)}\overrightarrow{\mathrm{h}_{t-1}}+b^{(f)}\right)\]</span>
							<li><strong>后向隐藏状态：</strong>沿反向方向，从后到前计算隐藏状态。</li>
							<span class = "math inline">\[\overset{\leftarrow}{\operatorname*{h}_{t}}=f\left(W^{(b)}x_{t}+U^{(b)}\overset{\leftarrow}{\operatorname*{h}_{t+1}}+b^{(b)}\right)\]</span>

							<li><strong>合并前向和后向隐藏状态：</strong>将前向和后向的隐藏状态合并，通常是将它们连接起来作为最终的输出。</li>
							<span class = "math inline">\[\mathbf{h}_t=\begin{bmatrix}\to\leftarrow\\\mathbf{h}_t;\mathbf{h}_t\end{bmatrix}\]</span>

							<li><strong>其中：</strong></li>
							<p><span class = "math inline">\(\mathrm{W(f)}\)</span>和<span class = "math inline">\(\mathrm{W(b)}\)</span>分别是前向和后向的输入权重矩阵。</p>

							<p><span class = "math inline">\(\mathrm{U(f)}\)</span>和<span class = "math inline">\(\mathrm{U(b)}\)</span>分别是前向和后向的递归权重矩阵。</p>
							<p><span class = "math inline">\(\mathrm{b(f)}\)</span>和<span class = "math inline">\(\mathrm{b(b)}\)</span>是偏置项。</p>
							<p>f是激活函数，通常为tanh或ReLU。</p>

						</ol>
						<div class="educationSection">（六）注意力机制（Transformer模型）</div>
						<p>尽管改进后的RNN模型能够更好地解决长序列依赖和数值不稳定性问题，但在处理非常长的序列时仍会面临计算效率和依赖距离的挑战。Transformer模型引入了自注意力机制，允许模型在一个序列的任意两个位置之间建立直接的依赖关系，极大地提高了并行计算效率，并有效解决了远距离依赖问题。正因如此，Transformer成为当下序列建模和自然语言处理领域的核心架构。</p>
					</div>
				</div>

			</div>
			
			
			


			<div style="height:100px;"> </div>


		</div>

		<div id = 'loadingDataTab' style="display: none">
			<div id='loadingMNIST'>
				加载中 <span id="datasetLoadingName">MNIST</span> 数据集
			</div>
		</div>

		<!-- Error popup -->
		<div id = 'error' style="display: none">
			<svg id = 'x' xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path fill="none" d="M0 0h24v24H0V0z"/><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
			<div id = 'errorMessage'> </div>
		</div>

	</div>


	<!-- The right panel -->
	<div id = 'paramshell'>
		<div class = 'trainbox' id = 'trainbox'>
			<div id = 'train' class = 'train' data-actionType = 'json'> 训练 </div>
		</div>

		<div class = 'category' id = 'kerasinfo'>
			<div class = 'categoryTitle' data-expanded = 'true'>
				<div class='expander'>
					<svg height="24px" width="24px">
						<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
					</svg>
				</div>
				<div class='categoryTitleText'>
					模型状态
				</div>
			</div>
			<div class = 'parambox'>
				<div id = 'ti_training' class = 'paramline'>
					<div class = 'paramname'>训练中</div>
					<div class = 'paramvalue'>否</div>
				</div>
				<div id = 'ti_acc' class = 'paramline'>
					<div class = 'paramname'>准确率：</div>
					<div class = 'paramvalue'>N/A</div>
				</div>
				<div id = 'ti_loss' class = 'paramline'>
					<div class = 'paramname'>损失率:</div>
					<div class = 'paramvalue'>N/A</div>
				</div>
				<div id = 'ti_vacc' class = 'paramline'>
					<div class = 'paramname'>验证集准确率:</div>
					<div class = 'paramvalue'>N/A</div>
				</div>
				<div id = 'ti_vloss' class = 'paramline'>
					<div class = 'paramname'>验证集损失率:</div>
					<div class = 'paramvalue'>N/A</div>
				</div>
			</div>
		</div>
		<div class="category">
			<div class = 'categoryTitle' data-expanded = 'true'>
				<div class='expander'>
					<svg height="24px" width="24px">
						<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
					</svg>
				</div>
				<div class='categoryTitleText'>
					导出代码
				</div>
			</div>
			<div id="exportPython" class="select-option right-option">导出 Python</div>
			<div id="exportJulia" class="select-option right-option">导出 Julia</div>
			<div id="copyModel" class="select-option right-option">模型链接</div>
		</div>
		<div id = 'networkParamshell'>

			<div class = 'category' id='paramtruck'>
				<div class = 'categoryTitle' data-expanded = 'true'>
					<div class='expander'>
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class='categoryTitleText'>
						参数
					</div>
				</div>
				<div id='defaultparambox' class = 'parambox'>点击一个层以查看和更改其参数。</div>
			</div>
		</div>

		<div id = 'progressParamshell' style="display: none">
			<div class = 'category' id='paramtruck'>
				<div class = 'categoryTitle' data-expanded = 'true'>
					<div class='expander'>
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class='categoryTitleText'>
						超参数
					</div>
				</div>
				<div class = 'parambox'>
					<div class = 'paramline'>
						<div class="paramname" data-name="lr">Learning Rate: </div>
						<input id="learningRate" class="paramvalue hyperparamvalue" value="0.01">
					</div>
					<div class = 'paramline'>
						<div class="paramname" data-name="epochs">Epochs: </div>
						<input id="epochs" class="paramvalue hyperparamvalue" value="6">
					</div>
					<div class = 'paramline'>
						<div class="paramname" data-name="lr">Batch Size: </div>
						<input id="batchSize" class="paramvalue hyperparamvalue" value="64">
					</div>
				</div>
			</div>
		</div>

		<div id = 'visualizationParamshell' style="display: none">
		</div>

		<div id = 'educationParamshell' style="display: none">
		</div>

	</div>

</div>
</body>
</html>
