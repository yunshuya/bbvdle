<meta charset="utf-8"/>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta name="description" content="欢迎来到BBVDLE，让您能够智能化、可视化地学习神经网络。">
	<title>BBVDLE ~ 基于可视化积木编程的深度学习教学平台 ~</title>

	<!-- MathJax cdn to render latex -->
	<script type="text/javascript" async
		src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  	</script>

	<!-- JSON-LD markup generated by Google Structured Data Markup Helper. -->
	<script type="application/ld+json">
	{
	  "@context" : "http://schema.org",
	  "@type" : "SoftwareApplication",
	  "name" : "BBVDLE ~ 基于可视化积木编程的深度学习教学平台 ~'/8",
	  "author" : [ {
		"@type" : "Person",
		"name" : ""
	  }, {
		"@type" : "Person",
		"name" : "Zack Holbrook"
	  }, {
		"@type" : "Person",
		"name" : "Stefan Grosser"
	  }, {
		"@type" : "Person",
		"name" : "Hendrik Strobelt"
	  }, {
		"@type" : "Person",
		"name" : "Rikhav Shah"
	  } ]
	}
	</script>

	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-133726432-1"></script>

	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());
		gtag('config', 'UA-133726432-1');
	</script>

	<link rel="icon" type="image/x-icon" sizes="16x16" href="favicon.ico">
	<link rel='stylesheet' href='src/ui/style.css'>
	<script src='dist/bundle.js'></script>
    <script>
        // Fix for exerciseMenu staying visible on other tabs and paramshell visibility/position
        document.addEventListener('DOMContentLoaded', function() {
            // Get all tab links
            const tabs = ['network', 'progress', 'visualization', 'education', 'exercise'];
            
            // Function to update layout based on current tab
            function updateLayout(tab) {
                const middle = document.getElementById('middle');
                const paramshell = document.getElementById('paramshell');
                
                // Hide exerciseMenu by default
                document.getElementById('exerciseMenu').style.display = 'none';
                
                if (tab === 'network' || tab === 'progress' || tab === 'visualization') {
                    // Show paramshell on these tabs
                    paramshell.style.display = 'block';
                    // Set middle width to account for paramshell
                    middle.style.width = 'calc(100% - 430px)';
                    // Ensure correct positioning
                    middle.style.float = 'left';
                    paramshell.style.float = 'right';
                } else if (tab === 'exercise') {
                    // Hide paramshell on exercise tab and remove its space
                    paramshell.style.display = 'none';
                    paramshell.style.width = '0';
                    paramshell.style.padding = '0';
                    paramshell.style.margin = '0';
                    paramshell.style.float = 'none';
                    // Show exerciseMenu first and position it on the right
                    const exerciseMenu = document.getElementById('exerciseMenu');
                    if (exerciseMenu) {
                        exerciseMenu.style.display = 'block';
                        exerciseMenu.style.float = 'right';
                        exerciseMenu.style.width = '450px';
                        exerciseMenu.style.height = '100%';
                        exerciseMenu.style.position = 'relative';
                        exerciseMenu.style.zIndex = '10';
                    }
                    // Set middle width: left menu 250px, exerciseMenu floats on the right
                    middle.style.width = 'calc(100% - 250px)';
                    middle.style.float = 'left';
                    middle.style.marginRight = '0';
                } else if (tab === 'education') {
                    // Hide paramshell on education tab
                    paramshell.style.display = 'none';
                    // Set middle width to full width
                    middle.style.width = 'calc(100% - 250px)';
                    middle.style.float = 'left';
                }
            }
            
            // Initialize layout based on current selected tab
            const currentTab = document.querySelector('.tab-selected').id;
            updateLayout(currentTab);
            
            // Add click event listeners to all tab links
            tabs.forEach(tab => {
                const tabElement = document.getElementById(tab);
                if (tabElement) {
                    tabElement.addEventListener('click', function() {
                        updateLayout(tab);
                    });
                }
            });
        });
    </script>
</head>


<body>

<h1 style="display:none">ENNUI ~ 优雅的神经网络教学平台 ~</h1>
<p style="display:none">ENNUI 通过构建、训练和在浏览器中可视化深度神经网络，帮助人们了解深度学习。它具有易于使用的拖放界面。当您准备开始编码时，可以导出网络以生成Python或Julia代码！ </p>


<h6 style="display:none">关于 ENNUI</h6>
<p style="display:none">
	ENNUI为深度学习开发的所有阶段提供多种工具。画布提供了一个拖放界面，用于设计神经网络架构。这个设计可以通过导出到链接与朋友和同事轻松分享。
	您不仅可以设计神经网络，还可以在多个数据集上训练它们：MNIST、CIFAR-10等！在训练期间，您可以在进度标签中跟踪您的网络损失和准确率，还可以查看混淆矩阵。
	一旦训练完成，ENNUI提供了一套神经网络可视化工具，以更好地理解您的架构。
	ENNUI不断更新新功能，所以请继续关注！
</p>

<!-- 登录页面（应用入口） -->
<div id="loginPage" class="login-page">
	<div class="login-page-background">
		<div class="login-page-gradient"></div>
	</div>
	<div class="login-page-content">
		<div class="login-page-header">
			<h1 class="login-page-title">欢迎来到 BBVDLE</h1>
			<p class="login-page-subtitle">基于可视化积木编程的深度学习教学平台</p>
		</div>
		<div id="loginPageAuthBox" class="login-auth-box">
			<!-- 登录表单 -->
			<div id="loginPageLoginForm" class="login-page-form">
				<h2 class="login-form-title">登录</h2>
				<div id="loginPageError" class="login-page-error hidden"></div>
				<form id="loginPageLoginFormElement" class="login-page-form-element">
					<div class="login-form-group">
						<label for="loginPageUsername">用户名或邮箱</label>
						<input type="text" id="loginPageUsername" name="username" required autocomplete="username" placeholder="请输入用户名或邮箱">
					</div>
					<div class="login-form-group">
						<label for="loginPagePassword">密码</label>
						<input type="password" id="loginPagePassword" name="password" required autocomplete="current-password" placeholder="请输入密码">
					</div>
					<div class="login-form-group">
						<label class="login-checkbox-label">
							<input type="checkbox" id="loginPageRememberMe" name="rememberMe">
							<span>记住我</span>
						</label>
					</div>
					<button type="submit" class="login-button login-button-primary">
						<span class="login-button-text">登录</span>
					</button>
				</form>
				<div style="margin-top: 15px;">
					<button type="button" id="skipLoginButton" class="login-button" style="background-color: transparent; border: 1px solid rgba(255, 255, 255, 0.3); color: rgba(255, 255, 255, 0.8);">
						<span class="login-button-text">跳过登录，直接进入</span>
					</button>
				</div>
				<div class="login-page-switch">
					<span>还没有账号？</span>
					<a href="#" id="loginPageSwitchToRegister">立即注册</a>
				</div>
			</div>

			<!-- 注册表单 -->
			<div id="loginPageRegisterForm" class="login-page-form hidden">
				<h2 class="login-form-title">注册</h2>
				<div id="loginPageRegisterError" class="login-page-error hidden"></div>
				<form id="loginPageRegisterFormElement" class="login-page-form-element">
					<div class="login-form-group">
						<label for="loginPageRegisterUsername">用户名</label>
						<input type="text" id="loginPageRegisterUsername" name="username" required 
							   pattern="[a-zA-Z0-9_]{3,20}" 
							   title="3-20个字符，只能包含字母、数字、下划线"
							   autocomplete="username" placeholder="3-20个字符，字母、数字、下划线">
					</div>
					<div class="login-form-group">
						<label for="loginPageRegisterEmail">邮箱</label>
						<input type="email" id="loginPageRegisterEmail" name="email" required autocomplete="email" placeholder="请输入邮箱地址">
					</div>
					<div class="login-form-group">
						<label for="loginPageRegisterPassword">密码</label>
						<input type="password" id="loginPageRegisterPassword" name="password" required 
							   minlength="6"
							   autocomplete="new-password" placeholder="至少6个字符">
					</div>
					<div class="login-form-group">
						<label for="loginPageRegisterPasswordConfirm">确认密码</label>
						<input type="password" id="loginPageRegisterPasswordConfirm" name="passwordConfirm" required 
							   autocomplete="new-password" placeholder="请再次输入密码">
					</div>
					<button type="submit" class="login-button login-button-primary">
						<span class="login-button-text">注册</span>
					</button>
				</form>
				<div class="login-page-switch">
					<span>已有账号？</span>
					<a href="#" id="loginPageSwitchToLogin">立即登录</a>
				</div>
			</div>
		</div>
	</div>
</div>

<div id = 'main' class="hidden">
	<!-- 用户认证状态栏（主应用内） -->
	<div id="userAuthBar" class="user-auth-bar">
		<div id="userInfo" class="user-info hidden">
			<span id="userName" class="user-name"></span>
			<button id="logoutButton" class="auth-button-small" title="登出">登出</button>
		</div>
		<!-- 移除登录和注册按钮，登出后返回登录页面 -->
	</div>

	<!-- The leftmost strip to select tabs -->
	<div id = 'tabselector'>
		<div id = 'blanktab' class='top_neighbor_tab-selected'> </div>
		<div title = '神经网络' class = 'tab-selected option tab-option' id = 'network' data-optionValue = 'network'>
			<svg class = 'icon' xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path fill="none" d="M0 0h24v24H0V0z"/><path d="M11.99 18.54l-7.37-5.73L3 14.07l9 7 9-7-1.63-1.27zM12 16l7.36-5.73L21 9l-9-7-9 7 1.63 1.27L12 16zm0-11.47L17.74 9 12 13.47 6.26 9 12 4.53z"/></svg>
		</div>
		<div title = '训练过程' class = 'option tab-option bottom_neighbor_tab-selected' id = 'progress' data-optionValue = 'progress'>
			<svg class = 'icon' xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path fill="none" d="M0 0h24v24H0V0z"/><path d="M13.5 13.48l-4-4L2 16.99l1.5 1.5 6-6.01 4 4L22 6.92l-1.41-1.41z"/></svg>
		</div>
		<div title = '结果可视化' class = 'option tab-option' id = 'visualization' data-optionValue = 'visualization'>
			<svg class = 'icon' xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path fill="none" d="M0 0h24v24H0z"/><path d="M11 9h2v2h-2V9zm-2 2h2v2H9v-2zm4 0h2v2h-2v-2zm2-2h2v2h-2V9zM7 9h2v2H7V9zm12-6H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 18H7v-2h2v2zm4 0h-2v-2h2v2zm4 0h-2v-2h2v2zm2-7h-2v2h2v2h-2v-2h-2v2h-2v-2h-2v2H9v-2H7v2H5v-2h2v-2H5V5h14v6z"/></svg>
		</div>
		<div id = 'middleblanktab' > </div>

		<div title = '教学' class = 'option tab-option' id = 'education' data-optionValue = 'education'>
			<svg class = 'icon' xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path fill="none" d="M0 0h24v24H0V0z"/><path d="M12 3L1 9l4 2.18v6L12 21l7-3.82v-6l2-1.09V17h2V9L12 3zm6.82 6L12 12.72 5.18 9 12 5.28 18.82 9zM17 15.99l-5 2.73-5-2.73v-3.72L12 15l5-2.73v3.72z"/></svg>
		</div>
		<div title = '练习' class = 'option tab-option' id = 'exercise' data-optionValue = 'exercise'>
			<svg class = 'icon' xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path fill="none" d="M0 0h24v24H0V0z"/><path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zm-7 14h-2v-4H8v-2h2V9h2v2h2v2h-2v4zm4-8h-2V7h2v2zm0 4h-2v-2h2v2z"/></svg>
		</div>
		<div id = 'bottomblanktab' > </div>
	</div>

	<!-- The left panel (menu) -->
	<div id = 'menu'>
		<div id = 'networkMenu'>
			<!-- The task -->
			<div id="tasks" class="category">
				<div class="categoryTitle" data-expanded="true">
					<div class="expander">
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class="categoryTitleText">
						教学任务
					</div>
				</div>
				<div class="option select-option" data-optionValue="MLP">多层感知机</div>
				<div class="option select-option" data-optionValue="CNN">卷积神经网络</div>
				<div class="option select-option" data-optionValue="RNN">循环神经网络</div>
			</div>

			<div id = 'layers' class = 'category'>
				<div class = 'categoryTitle' data-expanded = 'true'>
					<div class='expander'>
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class='categoryTitleText'>
						神经网络层
					</div>

				</div>
				<div class = 'option select-option' data-optionValue = 'dense'> Dense </div>
				<div class = 'option select-option' data-optionValue = 'conv2D'> Convolution </div>
				<div class = 'option select-option' data-optionValue = 'maxPooling2D'> Max Pooling </div>
				<div class = 'option select-option' data-optionValue = 'recurrent'> Recurrent </div>
				<div class = 'option select-option' data-optionValue = 'lstm'> LSTM </div>
				<div class = 'option select-option' data-optionValue = 'reshape'> Reshape </div>


				<div class = 'option-dropdown'>
					<div style="float:left">更多</div>
					<div style="float:right">〉</div>
					<div class='dropdown-content left'>
						<div title = '在训练过程中修改一批数据，使其更相似，从而加快收敛并获得更好的结果。'
							 class = 'option select-option' data-optionValue = 'batchnorm'> Batch Normalization </div>
						<div title = '在每个批次中忽略一部分随机的权重，以获得更好的泛化能力并加快训练。'
							 class = 'option select-option' data-optionValue = 'dropout'> Dropout </div>
						<div title = '将一组二维图像展平为一维特征向量。'
							 class = 'option select-option' data-optionValue = 'flatten'> Flatten </div>
						<div title = '将两个或多个输入（它们可以是1D或2D）连接起来'
							 class = 'option select-option' data-optionValue = 'concatenate'> Concatenate </div>
						<div title = '将两个或多个输入相加。'
							 class = 'option select-option last-dropdown' data-optionValue = 'add'> Add </div>
					</div>
				</div>
			</div>

			<div id = 'activations' class = 'category'>
				<div class = 'categoryTitle' data-expanded = 'true'>
					<div class='expander'>
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class='categoryTitleText'>
						激活函数
					</div>
				</div>
				<div class = 'option select-option' data-optionValue = 'relu'> ReLU </div>
				<div class = 'option select-option' data-optionValue = 'sigmoid'> Sigmoid </div>
				<div class = 'option select-option' data-optionValue = 'tanh'> Tanh </div>
				<div class = 'option select-option' data-optionValue = 'softmax'> Softmax </div>
			</div>
			<div id = 'templates' class = 'bottomCategory'>
				<div class = 'categoryTitle' data-expanded = 'true'>
					<div class='expander'>
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class='categoryTitleText'>
						模板
					</div>
				</div>
				<div class = 'option select-option' data-optionValue = 'blank'> 清空 </div>
				<div class = 'option select-option' data-optionValue = 'default'> 默认模板 </div>
				<div class = 'option select-option' data-optionValue = 'rnn'> 循环神经网络 </div>
				<div class = 'option select-option' data-optionValue = 'resnet'> ResNet残差网络 </div>
				<div class = 'option select-option' data-optionValue = 'lstm'> 长短期记忆网络 </div>
				<div class = 'option select-option' data-optionValue = 'lstmStructure' title='LSTM结构模版（适用于AirPassengers数据集训练）'> LSTM结构模版 </div>
				<div class = 'option select-option' data-optionValue = 'lstmFullInternal' title='LSTM完整内部结构（展示所有门控机制和计算过程）'> LSTM完整内部结构 </div>
			</div>
		</div>

		<div id = 'progressMenu' style="display: none">
			<div id = 'optimizers' class = 'category'>
				<div class = 'categoryTitle' data-expanded = 'true'>
					<div class='expander'>
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class='categoryTitleText'>
						优化器
					</div>
				</div>
				<div id = "defaultOptimizer" class = 'option select-option selected' id = 'sgd' data-optionValue = 'sgd'> SGD </div>
				<div id = 'rmsprop' class = 'option select-option' data-optionValue = 'rmsprop'> RMSprop </div>
				<div id = 'adagrad' class = 'option select-option' data-optionValue = 'adagrad'> Adagrad </div>
				<div id = 'adam' class = 'option select-option' data-optionValue = 'adam'> Adam </div>
			</div>
			<div id = 'losses' class = 'category'>
				<div class = 'categoryTitle' data-expanded = 'true'>
					<div class='expander'>
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class='categoryTitleText'>
						损失函数
					</div>
				</div>
				<div id = 'defaultLoss' class = 'option select-option selected' data-optionValue = 'categoricalCrossentropy'>CrossEntropy</div>
				<div id = 'hinge' class = 'option select-option' data-optionValue = 'hinge'> Hinge </div>
				<div id = 'meanSquaredError' class = 'option select-option' data-optionValue = 'meanSquaredError'> MSE </div>
				<div id = 'meanAbsoluteError' class = 'option select-option' data-optionValue = 'meanAbsoluteError'> MAE </div>
			</div>
		</div>

		<div id = 'visualizationMenu' style="display: none">
			<div id = 'classes' class = 'category'>
				<div class = 'categoryTitle' data-expanded = 'true'>
					<div class='expander'>
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class='categoryTitleText'>
						分类
					</div>
				</div>
				<div class = 'option select-option selected' data-optionValue = 'all'> ALL </div>
				<div class = 'option select-option' data-optionValue = '0'> 0 </div>
				<div class = 'option select-option' data-optionValue = '1'> 1 </div>
				<div class = 'option select-option' data-optionValue = '2'> 2 </div>
				<div class = 'option select-option' data-optionValue = '3'> 3 </div>
				<div class = 'option select-option' data-optionValue = '4'> 4 </div>
				<div class = 'option select-option' data-optionValue = '5'> 5 </div>
				<div class = 'option select-option' data-optionValue = '6'> 6 </div>
				<div class = 'option select-option' data-optionValue = '7'> 7 </div>
				<div class = 'option select-option' data-optionValue = '8'> 8 </div>
				<div class = 'option select-option' data-optionValue = '9'> 9 </div>
			</div>
		</div>


		<div id = 'educationMenu' style="display: none">
			<div id = 'educationLayers' class = 'category'>
				<div class = 'categoryTitle' data-expanded = 'true'>
					<div class='expander'>
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class='categoryTitleText'>
						相关文章
					</div>

				</div>

				<div class = 'option select-option education-option' data-optionValue = 'Overview'> 概述 </div>
				<div class = 'option select-option education-option' data-optionValue = 'Overfitting'> Overfitting</div>
			</div>

			<div id = 'educationStory' class = 'category'>
				<div class = 'categoryTitle' data-expanded = 'true'>
					<div class='expander'>
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class='categoryTitleText'>
						神经网络层
					</div>
				</div>
                <div class = 'option select-option education-option' data-optionValue = 'ResNets'>ResNets</div>
				<div class = 'option select-option education-option' data-optionValue = 'Add'>Add</div>
				<div class = 'option select-option education-option' data-optionValue = 'BatchNorm'>Batch Norm</div>
				<div class = 'option select-option education-option' data-optionValue = 'Concatenate'> Concatenate </div>
				<div class = 'option select-option education-option' data-optionValue = 'Convolution'> Convolution </div>
				<div class = 'option select-option education-option' data-optionValue = 'MaxPooling'>Max Pooling</div>
				<div class = 'option select-option education-option' data-optionValue = 'Dropout'> Dropout </div>
				<div class = 'option select-option education-option' data-optionValue = 'Flatten'> Flatten </div>
				<div class = 'option select-option education-option' data-optionValue = 'Dense'>Dense</div>

			</div>
			<div id = 'educationModel' class = 'category'>
				<div class = 'categoryTitle data-expanded' = 'true'>
					<div class = 'expander'>
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class='categoryTitleText'>
						模型教学
					</div>
				</div>
				<div class = 'option select-option education-option' data-optionValue = 'MLP'> 感知机 </div>
				<div class = 'option select-option education-option' data-optionValue = 'CNN'> 卷积神经网络 </div>
				<div class = 'option select-option education-option' data-optionValue = 'newCNN'> 现代卷积神经网络 </div>
				<div class = 'option select-option education-option' data-optionValue = 'RNN'> 循环神经网络 </div>
				<div class = 'option select-option education-option' data-optionValue = 'newRNN'> 现代循环神经网络 </div>
				<div class = 'option select-option education-option' data-optionValue = 'Transformer'>Transformer</div>
			</div>


			<div id = 'educationAct' class = 'category'>
				<div class = 'categoryTitle data-expanded' = 'true'>
					<div class = 'expander'>
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class='categoryTitleText'>
						激活函数
					</div>
				</div>
				<div class = 'option select-option education-option' data-optionValue = 'ReLU'> 
					 <a href="#educationReLU" style="text-decoration: none; color: inherit; display: block;">
                      ReLU
                     </a>
				</div>
				<div class = 'option select-option education-option' data-optionValue = 'Sigmoid'> 
					 <a href="#educationSigmoid" style="text-decoration: none; color: inherit; display: block;">
                     Sigmoid
                     </a>
				</div>
				<div class = 'option select-option education-option' data-optionValue = 'Tanh'>
					 <a href="#educationTanh" style="text-decoration: none; color: inherit; display: block;">
                     Tanh
                    </a>
				</div>
                <div class = 'option select-option education-option' data-optionValue = 'Softmax'>
					 <a href="#educationSoftmax" style="text-decoration: none; color: inherit; display: block;">
                     Softmax
                    </a>
		</div>
		
				<div class = 'option select-option education-option' data-optionValue = 'empty'>
					 <a href="#educationempty" style="text-decoration: none; color: inherit; display: block;">
                          
                    </a>
				</div>
			</div>
		</div>
	</div>

	<!-- The middle canvas -->
	<div id = 'middle'>
		<!-- 添加 AI 助手的按钮 -->
		<div id="aiAssistantButton" class="floating-button">
			<svg width="50" height="50" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
				<circle cx="12" cy="12" r="10" fill="rgb(128,108,183)"/>
				<text x="12" y="16" text-anchor="middle" fill="white" font-size="14px" font-family="Arial" >AI</text>
			</svg>
		</div>

		<!-- AI 助手蒙版（覆盖全页背景） -->
		<div id="aiBackdrop" class="ai-backdrop hidden"></div>

		<!-- 添加对话框 -->
		<div id="aiAssistantDialog" class="ai-dialog hidden">
			<div class="dialog-header">
				<div class="dialog-title">AI 助手</div>
				<div class="dialog-tools">
					<select id="aiConversationSelect" class="ai-conversation-select"></select>
					<button id="aiNewConversation" class="ai-new-conversation">新对话</button>
					<button id="closeAiDialog" class="ai-dialog-close" title="关闭">×</button>
				</div>
			</div>
			<div class="dialog-content" id="dialogContent"></div>
			<div id="aiContextAttachment" class="ai-context hidden">
				<div class="ai-context-main">
					<div class="ai-context-text"></div>
					<div id="aiContextActions" class="ai-context-actions hidden">
						<button class="ai-context-action" data-action="explain">解释说明</button>
						<button class="ai-context-action" data-action="summarize">概括内容</button>
						<button class="ai-context-action" data-action="quiz">出题测验</button>
					</div>
				</div>
				<button id="clearAiContext" class="ai-context-remove" title="移除附件">×</button>
			</div>
			<div class="dialog-footer">
				<button id="voiceInputButton" class="voice-input-button" title="语音输入">
					<svg width="32" height="32" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
						<path d="M12 1C10.34 1 9 2.34 9 4V12C9 13.66 10.34 15 12 15C13.66 15 15 13.66 15 12V4C15 2.34 13.66 1 12 1Z" stroke="currentColor" stroke-width="2" fill="none"/>
						<path d="M5 10V12C5 16.42 8.58 20 13 20C17.42 20 21 16.42 21 12V10" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round"/>
						<path d="M10 22H14" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
					</svg>
				</button>
				<input type="text" id="dialogInput" placeholder="输入消息..." />
				<button id="sendButton">发送</button>
			</div>
		</div>

		<div id="taskSteps">
			<div id="taskTitle" class="task-title" onclick="toggleTaskSteps()">
				<span id="taskTitleText">未选择任务</span>
				<span id="arrow" class="arrow">&#9660;</span> <!-- 初始为收起箭头 -->
			</div>
			<div id="taskContent" class="task-content" style="display: none;">
				<ul id="stepsList">
					<!-- 这里将动态显示步骤内容 -->
				</ul>
			</div>
		</div>

		<div id = 'networkTab'>
			<svg id = 'svg'> </svg>
		</div>

		<div id = 'progressTab' style="display: none">

			<div id="loss-canvas"></div>

			<div id="accuracy-canvas"></div>

			<div id="confusion-matrix-canvas"></div>
		</div>

		<div id = 'visualizationTab' style="display: none">
			<div id='visulaization'></div>
			<div id='images'></div>
		</div>

		<div id = 'informationOverlay'>
			<div id='information'>欢迎来到BBVDLE
				<div id="informationBody">~ 基于可视化积木编程的深度学习教学平台 ~</div>
				<div class="informationRow">
					<div class="informationColumn">
						自主创建神经网络 <br></br>
						<svg class = 'icon' xmlns="http://www.w3.org/2000/svg" width="30%" max-height="30%" viewBox="0 0 24 24"><path fill="none" d="M0 0h24v24H0V0z"/><path d="M11.99 18.54l-7.37-5.73L3 14.07l9 7 9-7-1.63-1.27zM12 16l7.36-5.73L21 9l-9-7-9 7 1.63 1.27L12 16zm0-11.47L17.74 9 12 13.47 6.26 9 12 4.53z"/></svg>
					</div>
					<div class="informationBlankColumn"></div>
					<div id="informationEducation" class="informationColumn">
						开始学习如何创建神经网络 <br></br>
						<svg class = 'icon' xmlns="http://www.w3.org/2000/svg" width="30%" max-height="30%" viewBox="0 0 24 24"><path fill="none" d="M0 0h24v24H0V0z"/><path d="M12 3L1 9l4 2.18v6L12 21l7-3.82v-6l2-1.09V17h2V9L12 3zm6.82 6L12 12.72 5.18 9 12 5.28 18.82 9zM17 15.99l-5 2.73-5-2.73v-3.72L12 15l5-2.73v3.72z"/></svg>
					</div>
				</div>
				<div id = 'acknowledgements'>
					<br>
					牛珂、方妍、刘倩文、王冉、朱婧怡 <br>
					开源资源<a class="overlayLinks" href="https://github.com/sunyia123/bbvdle" target="_blank">BBVDLE</a>.
					改进自<a class="overlayLinks" href="https://github.com/martinjm97/ENNUI" target="_blank">ENNUI</a>.
					项目代码<a class="overlayLinks" href="https://github.com/yunshuya/bbvdle" target="_blank">yunshuya</a>.
				</div>
			</div>
		</div>


		<div id = 'educationTab' style="display: none">
			<div id="educationOverview">
				<div class="educationTitle" style="padding-top: 0px"> 关于深度学习神经网络 </div>
				<div class="educationContent">
					如果你第一次进入我们的教学平台, 
					可以参考以下的快速入门教学开始自己的神经网络学习之旅：
				</div>
				<!--<iframe class="educationVideo" src="https://www.youtube.com/embed/m0YnwAtPbb8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->

				<div style="text-align: center; font-size: 25px;"><a href="https://www.youtube.com/watch?v=m0YnwAtPbb8" target="blank">ENNUI 教学</a></div>
				
				<div class="educationSection"> 深度学习的基本知识 </div>
				<div class="educationContent">
					如果您不熟悉机器学习，下面的讲座视频是一个很好的介绍。
				</div>
				<div style="text-align: center; font-size: 25px;"><a href="https://video.odl.mit.edu/videos/9101a72a7d994d53800d1398fd885b88/embed/?start=339" target="blank">Gilbert Strang: Deep Learning</a></div>
				<!-- <iframe class="educationVideo" src="https://video.odl.mit.edu/videos/9101a72a7d994d53800d1398fd885b88/embed/?start=339" scrolling="no" frameborder="0" allowfullscreen></iframe> -->
				<div style="text-align: center; font-size: 25px;"><a href="https://web.stanford.edu/class/cs224n/" target="blank">Natural Language Processing with Deep Learning</a></div>
			</div>

			<div id="educationOverfitting">
				<div class="educationTitle">过拟合（Overfitting）</div>
				<div class="educationAuthor">作者 <i>Stefan Grosser</i> 和 <i>Jesse Michel</i></div>
				<div class="educationContent">
			
					<p>神经网络有时会学习得过于精确。它识别出仅仅是训练数据中特定的趋势，因此无法<strong>泛化</strong>。这种过度拟合训练数据的问题称为<strong>过拟合</strong>。下图展示了决策边界——决定分类器预测的曲线——在欠拟合、拟合良好（正常）和过拟合的情况下的变化。</p>
			
					<img class="educationImage" src="dist/overfitti_ng.png" alt="可能的决策边界" />
					<div class="modelLink">
						<a class="modelLink" target="_newtab" href="http://mlwiki.org/index.php/Overfitting">
							来源: ML Wiki
						</a>
					</div>
					<p>当分类器发生过拟合时，它在训练数据上的表现远远优于测试数据。因此，训练准确度会远高于验证准确度，训练损失会远低于验证损失。我们在下方提供了这个例子的可视化。</p>
			
					<img class="educationImage" style="float: left; max-width: 50%;" src="dist/loss_overfit.png" title="图：训练过程中的过拟合可视化" alt="训练过程中过拟合的可视化" />
			
					<img class="educationImage" style="float: right; max-width: 50%;" src="dist/accuracy_overfit.png" title="图：训练过程中的过拟合可视化" alt="训练过程中过拟合的可视化" />
			
					<div style="margin-top:10px;">
						该示例所用的架构如下所示：
					</div>
			
					<div class="figure">
						<img class="educationImage" style="max-width: 50%;" src="dist/overfitting_network.png" alt="网络架构" >
			
						<div class="modelLink">
							<a class="modelLink" target="_newtab" href="https://math.mit.edu/ennui/#%7B%22graph%22:%5B%7B%22layer_name%22:%22Input%22,%22children_ids%22:%5B2%5D,%22parent_ids%22:%5B%5D,%22params%22:%7B%22dataset%22:%22cifar%22%7D,%22id%22:0,%22xPosition%22:100,%22yPosition%22:399%7D,%7B%22layer_name%22:%22Conv2D%22,%22children_ids%22:%5B3%5D,%22parent_ids%22:%5B0%5D,%22params%22:%7B%22filters%22:16,%22kernelSize%22:%5B3,3%5D,%22strides%22:%5B1,1%5D,%22kernelRegularizer%22:%22none%22,%22regScale%22:0.1,%22activation%22:%22relu%22%7D,%22id%22:2,%22xPosition%22:261,%22yPosition%22:453%7D,%7B%22layer_name%22:%22Flatten%22,%22children_ids%22:%5B1%5D,%22parent_ids%22:%5B2%5D,%22params%22:%7B%7D,%22id%22:3,%22xPosition%22:585,%22yPosition%22:484%7D,%7B%22layer_name%22:%22Output%22,%22children_ids%22:%5B%5D,%22parent_ids%22:%5B3%5D,%22params%22:%7B%7D,%22id%22:1,%22xPosition%22:900,%22yPosition%22:399%7D%5D,%22hyperparameters%22:%7B%22learningRate%22:0.1,%22batchSize%22:64,%22optimizer_id%22:%22defaultOptimizer%22,%22epochs%22:15,%22loss_id%22:%22defaultLoss%22%7D%7D">
								模型链接
							</a>
						</div>
					</div>
					<br/><br/>
					<p>过拟合展示了交叉验证为何如此重要；如果没有验证集，我们将无法识别模型无法泛化的问题。</p>
					<p>那么，如何应对过拟合，确保模型找到可以泛化的特征呢？</p>
					<div class="educationSection">正则化</div>
					<p>防止过拟合的一种方法是正则化，它通过加入一个新的项来引导模型走向更简单的解决方案。回想一下，在分类问题中，我们从一对对输入和它们的分类 <span class="math display">\[(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n).\]</span> 开始。我们希望找到一个函数 <span class="math inline">\(f\)</span>，它能准确预测新数据样本的类别。因此，如果我们的原始问题是 <span class="math display">\[\min_f \sum_{i=1}^{n} C(f(x_i), y_i),\]</span> 其中 <span class="math inline">\(C\)</span> 计算当预测 <span class="math inline">\(f(x_i)\)</span> 时的代价，当真实值是 <span class="math inline">\(y_i\)</span>，那么正则化损失将是 <span class="math display">\[\min_f \sum_{i=1}^{n} C(f(x_i), y_i) + \lambda R(f),\]</span> 其中 <span class="math inline">\(R(f)\)</span> 是正则化项，定义为当 <span class="math inline">\(f\)</span> 更复杂时它会变大，而 <span class="math inline"> \(\lambda>0\) </span> 是一个可调节的参数，控制正则化的程度。层的复杂性有多种定义，但在我们的案例中，我们会说，具有较低 <span class="math inline">\(L2\)</span>-范数的层较为简单。正式地，我们将 <span class="math inline">\(L2\)</span>-范数定义为 <span class="math display">\[\text{norm}(A) = \sqrt{\sum_i \sum_j a_{ij}^2}.\]</span> 例如，给定矩阵 <span class="math display">\[A =
					\begin{bmatrix}
					1 & 2 \\
					0 & -2
					\end{bmatrix},\]</span> 其 L2-范数为 <span class="math display">\[||A||_2 = \sqrt{1^2 + 2^2 + 0^2 + (-2)^2} = 3.\]</span></p>
					<p>有几个原因解释了为何惩罚增大的 <span class="math inline">\(L2\)</span>-范数是一个合理的做法。如果我们假设分类器会发生过拟合，那么加入惩罚项 <span class="math inline">\(\lambda R(f)\)</span> 将会引导决策边界远离这一状态。这可以看作是给分类器增加“摆动空间”。此外，这种高 <span class="math inline">\(L2\)</span>-范数的惩罚是鼓励丢弃无用信息的一种方式。惩罚项促使层的权重变小，而层的权重越接近零，其作为特征的影响就越小。</p>
					<p>这种复杂性的概念导致了 <span class="math inline">\(L1\)</span>-和 <span class="math inline">\(L2\)</span>-范数成为正则化的一种形式。在 <span class="math inline">\(L2\)</span>-正则化的情况下，我们可以将 <span class="math inline">\(\lambda ||W||_2\)</span> 加入到损失函数中，针对给定层 <span class="math inline">\(W.\)</span> 当然，还有其他的正则化方式，但现在我们来看看另一种方法。</p>
					<!-- TODO: 以后可能解释 L1-正则化 -->
					<div class="educationSection">Dropout</div>
					<p>防止过拟合的另一种方法是名为 dropout 的技术。Dropout 层在训练期间忽略一部分输入单元（有关更多信息，请参阅我们的 dropout 层解释）。有两种直觉可以解释为什么 dropout 有助于防止过拟合。Dropout 可以看作是一种集成学习——将一组弱（欠拟合）分类器的结果进行某种方式的组合，例如采用多数类别。在每个批次中，网络的一个新部分作为弱分类器进行训练。在验证阶段，整个网络都会被使用，从而有效地将所有分类器结合起来提供一个单一的结果。另一种看法是，经过多次运行后，dropout 强制网络架构的所有部分都被使用。因此，训练集的任何一个特征都不会过于影响网络，避免网络集中在仅对训练集特有的伪影上。</p>
					<div class="educationSection">结论</div>
					<p>过拟合会妨碍分类器在未见数据上的表现。正则化和 dropout 是两种广泛使用且容易实现的防止过拟合的方法。结合这些方法与交叉验证，使得构建更具泛化能力的模型变得更加容易。</p>
				</div>
			</div>

			<div id="educationResNets">
				<div class="educationTitle"> 残差网络 (ResNets) </div>
				<div class="educationAuthor">作者 <i>Zack Holbrook</i> and <i>Jesse Michel</i></div>

				<div class="educationContent">
					<p>2015 年，微软的一个研究团队凭借 ResNet 在 <a href="http://www.image-net.org/challenges/LSVRC/">ImageNet 大规模视觉识别挑战赛</a> 中获得了创纪录的表现。自 2015 年以来，ResNet 的变种一直主导着这一竞赛，超过了人类在该任务中的表现。它们已成为图像识别任务中广泛采用的架构，并且相对容易实现和训练。</p>
					<div class="educationSection">ResNet 架构</div>
					<p>ResNet 是一种卷积神经网络（CNN），具有 <strong>恒等捷径（identity shortcuts）</strong>，这是通过跳过某些层创建的网络路径，从而在网络中创建捷径。下面我们提供一个典型的 ResNet 示例：
				
						<img class="educationImage" src="dist/resnet.png" alt="Resnet image" width="50%">
				
						<div class="modelLink">
							<a class="modelLink" target="_newtab" href="http://math.mit.edu/ennui/#%7B%22graph%22:%5B%7B%22layer_name%22:%22Input%22,%22children_ids%22:%5B5,9%5D,%22parent_ids%22:%5B%5D,%22params%22:%7B%22dataset%22:%22mnist%22%7D,%22id%22:0,%22xPosition%22:100,%22yPosition%22:377%7D,%7B%22layer_name%22:%22Conv2D%22,%22children_ids%22:%5B6%5D,%22parent_ids%22:%5B0%5D,%22params%22:%7B%22filters%22:16,%22kernelSize%22:%5B3,3%5D,%22strides%22:%5B1,1%5D,%22activation%22:%22relu%22%7D,%22id%22:5,%22xPosition%22:169,%22yPosition%22:280%7D,%7B%22layer_name%22:%22Add%22,%22children_ids%22:%5B7,10%5D,%22parent_ids%22:%5B0,6%5D,%22params%22:%7B%22activation%22:%22relu%22%7D,%22id%22:9,%22xPosition%22:276,%22yPosition%22:411%7D,%7B%22layer_name%22:%22Conv2D%22,%22children_ids%22:%5B9%5D,%22parent_ids%22:%5B5%5D,%22params%22:%7B%22filters%22:16,%22kernelSize%22:%5B3,3%5D,%22strides%22:%5B1,1%5D%7D,%22id%22:6,%22xPosition%22:294,%22yPosition%22:280%7D,%7B%22layer_name%22:%22Conv2D%22,%22children_ids%22:%5B8%5D,%22parent_ids%22:%5B9%5D,%22params%22:%7B%22filters%22:16,%22kernelSize%22:%5B3,3%5D,%22strides%22:%5B1,1%5D,%22activation%22:%22relu%22%7D,%22id%22:7,%22xPosition%22:414,%22yPosition%22:280%7D,%7B%22layer_name%22:%22Add%22,%22children_ids%22:%5B11%5D,%22parent_ids%22:%5B9,8%5D,%22params%22:%7B%22activation%22:%22relu%22%7D,%22id%22:10,%22xPosition%22:521,%22yPosition%22:412%7D,%7B%22layer_name%22:%22Conv2D%22,%22children_ids%22:%5B10%5D,%22parent_ids%22:%5B7%5D,%22params%22:%7B%22filters%22:16,%22kernelSize%22:%5B3,3%5D,%22strides%22:%5B1,1%5D%7D,%22id%22:8,%22xPosition%22:541,%22yPosition%22:280%7D,%7B%22layer_name%22:%22Flatten%22,%22children_ids%22:%5B12%5D,%22parent_ids%22:%5B10%5D,%22params%22:%7B%7D,%22id%22:11,%22xPosition%22:708,%22yPosition%22:463%7D,%7B%22layer_name%22:%22Dense%22,%22children_ids%22:%5B13%5D,%22parent_ids%22:%5B11%5D,%22params%22:%7B%22units%22:32,%22activation%22:%22relu%22%7D,%22id%22:12,%22xPosition%22:702,%22yPosition%22:434%7D,%7B%22layer_name%22:%22Dropout%22,%22children_ids%22:%5B1%5D,%22parent_ids%22:%5B12%5D,%22params%22:%7B%22rate%22:0.5%7D,%22id%22:13,%22xPosition%22:778,%22yPosition%22:365%7D,%7B%22layer_name%22:%22Output%22,%22children_ids%22:%5B%5D,%22parent_ids%22:%5B13%5D,%22params%22:%7B%7D,%22id%22:1,%22xPosition%22:900,%22yPosition%22:377%7D%5D,%22hyperparameters%22:%7B%22learningRate%22:0.01,%22batchSize%22:64,%22optimizer_id%22:%22defaultOptimizer%22,%22epochs%22:6,%22loss_id%22:%22defaultLoss%22%7D%7D">
								模型链接
							</a>
						</div>
				
						恒等捷径意味着学习到的参数是残差。数学上，如果 <span class="math inline">\(R(x)\)</span> 是一系列卷积层与 ReLU 组合，称为 <strong>残差块</strong>，例如，假设 <span class="math display">\[R(x) = \textrm{Conv}(\textrm{ReLU}(\textrm{Conv}(x))).\]</span> 那么，残差块的输出将是 <span class="math inline">\(R(x) + x\)</span>，其中 <span class="math inline">\(x\)</span> 是恒等传递。如果神经网络试图逼近某个函数 <span class="math inline">\(F(x)\)</span>，那么一个完美的残差块 <span class="math inline">\(R^*(x)\)</span> 会使得 <span class="math inline">\(R^*(x) = F(x) - x\)</span>，这正是通过减去输入图像得到的残差。</p>
					<div class="educationSection">ResNet 的优势</div>
					<p>ResNet 的一个惊人特性是它的良好扩展性，使得深层神经网络仍然能够良好地训练。当网络变得更大时，许多问题会出现。</p>
					<p>大规模网络往往训练速度较慢，但 CNN 的 <strong>权重共享</strong> 意味着每个残差块需要训练的参数相对较少。大规模网络还往往面临 <strong>梯度消失</strong> 问题——在梯度下降中，权重更新会逐渐变得微不足道，导致即使有更多的训练时间，网络也无法改进。ResNet 中的恒等捷径为梯度提供了流动路径，从而避免了梯度消失的问题。</p>
				</div>
			</div>

            <div id="educationAdd">
				<div class="educationTitle"> 相加层（Add） </div>

				<div class="educationContent">
					<p><strong>功能：</strong>将多个相同维度的输入张量进行逐元素相加。</p>
				
					<p><strong>作用：</strong>是构建残差网络（ResNets） 的关键。通过“快捷连接”将前一层的输出直接跳过多层加到后面层的输出上，有效解决了深层网络中的梯度消失和退化问题，使得训练非常深的网络成为可能。</p>
				</div>
				</div>
				
			<div id="educationBatchNorm">
				<div class="educationTitle"> 批量归一化层（Batch Normalization） </div>

				<div class="educationContent">
					<p><strong>功能：</strong>对每一批（Batch）输入数据进行标准化处理，使其均值为0，方差为1</p>
					<p><strong>作用：</strong>加速训练过程，允许使用更大的学习率。减少对参数初始化的依赖。有一定正则化效果，可以轻微减少过拟合。</p>
					<p>通常放在卷积层或全连接层之后，激活函数之前。</p>
				</div>
			</div>

			<div id="educationConcatenate">
				<div class="educationTitle">连接层（Concatenate）</div>
			
				<div class="educationContent">
					<p><strong>功能：</strong>将多个输入张量沿着指定的轴（通常是通道轴）连接在一起。</p>
					<p><strong>作用：</strong>用于构建具有多分支结构的复杂网络（如Inception模块、U-Net等），将不同分支提取的特征融合在一起。</p>
				</div>
			</div>
			
			<div id="educationConvolution">
				<div class="educationTitle"> 卷积层（Convolution） </div>
			
				<div class="educationContent">
					<p><strong>功能：</strong> 使用一个小的“滤波器”在输入数据（通常是图像）上滑动，计算局部区域的点积。</p>
					<p><strong>作用：</strong>卓越地提取局部特征，如边缘、角点、纹理等。通过参数共享，大大减少了模型的参数量。是卷积神经网络（CNN） 的核心。</p>
					<p><strong>主要应用：</strong>图像处理、视频分析、自然语言处理。</p>
				</div>
			</div>
			
			<div id="educationMaxPooling">
				<div class="educationTitle"> 最大池化层（Max Pooling） </div>

				<div class="educationContent">
					<p><strong>功能：</strong> 对输入数据的一个小区域（如2x2窗口）进行下采样，只输出该区域内的最大值。</p>
					<p><strong>作用：</strong>神降低数据维度，减少计算量和参数，防止过拟合。保持特征的平移不变性（即无论特征在输入中的哪个位置，都能被检测到）。</p>
					<p>通常紧随卷积层之后使用。</p>
				</div>
			</div>

			<div id="educationDropout">
				<div class="educationTitle">丢弃层（Dropout）</div>
			
				<div class="educationContent">
					<p><strong>功能：</strong>在训练过程中，随机“关闭”（将其输出置为零）网络中一部分神经元。</p>
					<p><strong>作用：</strong>一种非常有效的正则化技术，用于防止过拟合。通过随机丢弃，可以强制网络不依赖于任何单个神经元，从而学习到更鲁棒、更泛化的特征。</p>
					</div>
			</div>
			
			<div id="educationFlatten">
				<div class="educationTitle"> 展平层（Flatten） </div>
			
				<div class="educationContent">
					<p><strong>功能：</strong>将多维的输入数据“压平”成一维数据。</p>
					<p><strong>作用：</strong>作为卷积层到全连接层之间的过渡。因为全连接层需要一维输入，而卷积层的输出是二维或三维的特征图。</p>
				</div>
			
					</div>
			
			<div id="educationDense">
				<div class="educationTitle"> 全连接层（Dense） </div>
				
				<div class="educationContent">
					<p><strong>功能：</strong> 最基础、最常见的层。层中的每个神经元都与上一层的所有神经元相连。</p>
					<p><strong>作用：</strong>用于学习输入特征之间的全局模式。通常用在网络的最后几层进行分类或回归。</p>
					<p><strong>类比：</strong>像一个传统的“多层感知机”。</p>
						</div>
				</div>
			
			
			<div id="educationMLP">
				<div class="educationTitle">感知机</div>
				<div class="educationContent">
					<div class="educationSection">（一）概述</div>
					<p>
						多层感知机（MLP：Multi-Layer Perceptron）由感知机(PLA: Perceptron Learning Algorithm)推广而来。它最主要的特点是有多个神经元层，因此也叫深度神经网络(DNN: Deep Neural Networks)。
					</p>
					<p>
						感知机是单个神经元模型，是较大神经网络的前身。神经网络的强大之处在于它们能够学习训练数据中的表示，以及如何将其与想要预测的输出变量联系起来。从数学上讲，它们能够学习任何映射函数，并且已经被证明是一种通用的近似算法。
					</p>
					<p>
						神经网络的预测能力来自网络的分层或多层结构。而多层感知机是指具有至少三层节点（输入层，一些中间层和输出层）的神经网络。每一层中的节点与相邻层的节点完全连接。
					</p>
					<div class="educationSection">（二）各层说明</div>
					<p><strong>1．输入层：</strong> 每个输入特征对应一个神经元，输入层不涉及计算。</p>
					<p><strong>2．单个或多个隐藏层：</strong></p>
					<ul>
						<li>介绍</li>
						<p>至少有一个隐藏层，通常包含多个神经元。每一层的神经元与前一层的神经元连接。</p>
						<p>隐藏层负责对输入数据进行非线性转换。在每个神经元中，输入值会与相应的权重相乘，再加上偏置，最后通过激活函数进行变换。这样每一层都能够提取特征。</p>
						<li>神经元的计算</li>
						<ul>
							<li>对于第L层的第j个神经元，其输出为：
								<span class="math inline">\[z_j=W_j\cdot a^{(1-1)}+b_j\]</span>
								<span class="math inline">\[a_j{=}f(z_j)\]</span>
								<p>其中,<span class="math inline">\(W_j\)</span>是权重，<span class=",math inline">\(b_j\)</span>是偏置，<span class=",math inline">\(a^{(l-1)}\)</span>是前一层的输出，<span class="math inline">\(f(z_j)\)</span>是激活函数 (如ReLU) 。 假设隐藏层有3个神经元，并且我们使用 ReLU 激活函数。每个神经元的输入是输入层的输出(x1, x2),它们会被分别与权重和偏置进行加权和偏置计算。</p>
							</li>
							<li>隐藏层神经元的计算过程：
								<p>对于第一个神经元：<span class="math inline">\[z_1=W_1\cdot x+b_1=w_1\cdot x_1+w_{12}\cdot x_2+b_1\]</span></p>
								<p>激活函数(ReLU)会将计算结果输出：<span class="math inline">\[a_1{=}\mathrm{ReLU}(z_1)\]</span></p>
								<p>对于第二个和第三个神经元，同样的计算方式。</p>
							</li>
						</ul>
						<li>激活函数的选择：</li>
						<ul>
							<li>ReLU：适用于隐藏层，能够加速训练，避免梯度消失问题。</li>
							<li>Sigmoid：可以用于隐藏层，但在深层网络中容易导致梯度消失。</li>
							<li>Tanh：适用于隐藏层，能够解决一些 Sigmoid 的问题，具有对称性。</li>
						</ul>
					</ul>
					<p><strong>3．输出层：</strong> </p>

					<ul>
						<li>介绍</li>
						<p>输出层负责将隐藏层的结果转化为最终预测值。</p>
						<li>神经元计算（以 Sigmoid 为例）
							<p>输出层的计算可以表示为：</p>
							<span class="math inline">\[a^{(\mathrm{hidden})}+b_{\mathrm{out}}\]</span>
							<p>其中，<span class="math inline">\(\mathbf{a^{(hiddcn)}}\)</span>是隐藏层的输出,<span class="math inline">\(W_{\mathrm{out}}\)</span>是输出层的权重，<span class="math inline">\(\widehat{y}=\sigma(z_{\mathrm{out}})=\frac1{1+e^{-z_{\mathrm{out}}}}\)</span>
								,其中<span class="math inline">\(\widehat{y}\)</span>是模型的预测输出，代表样本属于某一类的概率。
								</p>
						</li>
					</ul>
					<div class="educationSection">（三）思考题</div>
					<ol>
						<li>
							<strong>过拟合问题：</strong> 为什么 Dropout 可以防止过拟合？除此之外还有哪些方法？
						</li>
						<li>
							<strong>可解释性问题：</strong> 为什么深度学习被称为“黑箱”模型？如何提高其可解释性？
						</li>
						<li>
							<strong>激活函数问题：</strong> 为什么需要使用非线性激活函数？
						</li>
					</ol>
				</div>

			</div>


			<div id="educationCNN">
				<div class="educationTitle">卷积神经网络</div>
				<div class="educationContent">
					<div class="educationSection">（一）概述</div>
					<p>卷积神经网络（Convolutional Neural Network，简称 CNN）是一种特别适合处理和分析图像的深度学习模型。CNN 的结构和原理借鉴了生物视觉系统，尤其是人类视觉皮层的工作方式，使它能够有效提取图像中的各种特征，如边缘、纹理和形状。与传统的神经网络不同，CNN 通过卷积层和池化层来提取特征，并用全连接层对提取到的特征进行分类或回归。</p>
					<div class="educationSection">（二）CNN模型提出</div>
					<ol>
						<li>CNN 发展背景与基本思想</li>
						<p>在 1980 年代，计算机视觉领域的研究者发现传统的机器学习方法在图像处理任务上表现不佳。图像中包含的大量像素信息使得简单的机器学习模型难以有效提取有用的特征，且数据维度高、参数多。这些挑战使得研究者们开始寻找新的方法来自动从图像数据中提取特征。</p>
						<p>此时，研究人员借鉴了生物视觉系统的工作原理。生物学家发现，人类视觉皮层在处理视觉信息时，会逐层提取图像中的不同层次信息，从而形成对图像内容的理解。基于这种启发，研究者们开始设计一种模拟生物视觉系统的层次化结构模型，即卷积神经网络。</p>
						<li>Yann LeCun 和 LeNet 的诞生</li>
						<p>Yann LeCun 是法国计算机科学家，被誉为“卷积神经网络之父”。他在 1989 年提出了一个简单的卷积神经网络模型，用于手写数字识别任务。LeCun 的工作受生物视觉系统启发，并基于如下两个关键原则来设计 CNN：</p>
						<ul>
							<li>局部连接：LeCun 提出的 CNN 只对图像的局部区域进行处理，而不是全图连接。这种方法不仅减少了参数数量，还能更有效地学习图像的局部特征。</li>
							<li> 权重共享：在卷积操作中，CNN 的每一个卷积核在图像的不同位置共享相同的权重，从而减少了需要学习的参数数量。这种结构可以让网络自动识别图像中重复的模式，比如边缘或特定的形状。</li>
						</ul>
						<p>这两个设计思想极大降低了模型复杂度，使得 CNN 能够在当时有限的计算资源下运行。</p>
						<li>LeNet-5 架构（1998 年）</li>
						<p>在 1998 年，Yann LeCun 等人提出了 LeNet-5，这是一个多层的卷积神经网络结构，主要用于手写数字识别（例如识别 0-9 的手写数字）。LeNet-5 的结构包括：</p>
						<ul>
							<li>卷积层：用于提取图像的局部特征。</li>
							<li>池化层：通过下采样操作减少特征的尺寸，从而降低模型的计算量。</li>
							<li>全连接层：用于将卷积和池化提取到的特征综合，用于最终的分类。</li>

						</ul>
						<p>LeNet-5 的诞生标志着 CNN 的第一次成功应用，并在手写数字识别任务上取得了令人瞩目的表现。然而，由于当时的计算能力和数据集规模有限，CNN 的进一步发展受到限制，无法应用于更大规模的任务。</p>
						<li>深度学习和计算能力的推动</li>
						<p>2000 年代，随着 GPU 的出现和计算能力的提升，深度学习的研究逐渐复兴。大规模数据集（如 ImageNet）的出现为 CNN 的训练提供了丰富的数据。AlexNet 的提出将 CNN 推向了新的高度。</p>
						<li>AlexNet 和 CNN 的重大突破（2012 年）</li>
						<p>在 2012 年，由 Alex Krizhevsky 等人设计的 AlexNet 模型参加了 ImageNet 大规模视觉识别竞赛（ILSVRC），并在分类任务上取得了巨大的成功，准确率远超其他方法。AlexNet 的结构和 LeNet-5 相似，但 AlexNet 增加了深度层数、使用了 ReLU 激活函数以及 Dropout 正则化等技术来提升性能。</p>
						<p>AlexNet 的成功证明了 CNN 的潜力，并在计算机视觉领域掀起了深度学习的热潮。此后，更多深层次的 CNN 架构被提出，如 VGG、GoogLeNet、ResNet 等，使 CNN 成为计算机视觉的核心模型。</p>
					</ol>
					<div class="educationSection">（三）CNN模型的核心</div>
					<image class="educationImage" src="resources/educationimages/CNN_1.png"  alt="CNN_1"></image>
					<ol>
						<li>输入层</li>
						<p>输入层通常接收图像数据，图像的像素值组成三维数据张量：宽度、高度和通道（如彩色图像有红、绿、蓝 3 个通道）。</p>
						<p>图像数据通过 CNN 的层次结构处理，从低层的边缘特征逐渐提取到高层的复杂形状和结构特征。</p>
						<li>卷积层（Convolutional Layer）</li>
						<ol>
							<li><strong >卷积运算：</strong>卷积层的核心操作是卷积运算。在图像处理领域中，卷积运算可以通过小矩阵（卷积核或滤波器）在图像上滑动，提取局部区域的特征。例如，一个 3×33×3 的卷积核可以在图像上移动，对每个 3×33×3 的子区域进行点积运算，生成特征图（Feature Map）。</li>
							<li><strong >特征提取：</strong>卷积核通过学习得到特定的权重，能够提取不同的特征，例如边缘、纹理或颜色。每一个卷积核代表一种特定的图像特征，多个卷积核的堆叠则可以提取图像的多种特征。</li>
							<li><strong>特征图的输出：</strong>经过卷积操作的结果会生成新的特征图，将其传递给下一层。特征图的深度等于卷积核的数量</li>
						</ol>
						<li>激活函数（Activation Function）</li>
						<ol>
							<li><strong>(Rectified Linear Unit)：</strong>在卷积操作之后，通常会对特征图应用激活函数。ReLU 是一种常用的激活函数，它将所有负值置零，保持正值不变，从而引入非线性因素。</li>
							<li><strong>非线性特征提取：</strong>激活函数的作用是提高模型的表达能力，让 CNN 可以学习复杂的非线性特征。在 CNN 中，ReLU 函数的计算效率高且可以有效防止梯度消失。</li>
						</ol>
					
						<li>池化层（Pooling Layer）</li>
						<ol>
							<li><strong>池化运算：</strong>池化层的主要作用是降低特征图的尺寸，减少计算量，并且通过特征的抽象增加模型的鲁棒性。常见的池化方法是最大池化（Max Pooling），它从特征图的每个局部区域中取最大值，以保留最显著的特征。</li>
							<li><strong>降低维度与增强平移不变性：</strong>降低维度与增强平移不变性：池化层有助于减少模型的计算需求，同时使得模型对图像中的细微平移更加不敏感。例如，一个 2×22×2 的最大池化操作可以将每 2×22×2 区域缩小为一个值，从而减少特征图的大小。</li>
						</ol>
						<li>多层卷积与池化的组合</li>
						<p>在实际 CNN 结构中，通常会堆叠多个卷积层和池化层以构建深层网络。低层卷积提取图像的基础特征，如边缘和简单形状。中层卷积提取更复杂的特征，如局部模式或图案，而高层卷积则学习全局的结构特征。每一层卷积和池化的输出特征图被传递到下一层，逐步形成更加抽象的特征表示。</p>
						<li>展平层（Flattening Layer）</li>
						<p>卷积和池化层的输出是一个三维的特征图张量。为了便于全连接层处理，需要将三维特征展平成一维向量，这个过程称为“展平”。展平后的特征向量包含了图像的高级特征，并准备传递到全连接层进行进一步处理。</p>
						<li>全连接层（Fully Connected Layer）</li>
						<ol>
							<li><strong>全连接操作：</strong>全连接层将展平的特征向量输入到一个或多个全连接的神经网络层中。这些层将每一个输入特征与输出类别进行加权组合，从而输出预测结果。</li>
							<li><strong>输出结果：</strong>最后一层全连接层通常使用 Softmax 激活函数，用于多分类任务，输出一个概率分布，代表图像属于各类别的可能性。</li>
						</ol>
						<li>损失函数与反向传播</li>
						<ol>
							<li><strong>损失函数：</strong>在训练过程中，CNN 的输出会与真实标签进行比较，计算损失值（例如交叉熵损失），表示预测结果与实际结果的差异。</li>
							<li><strong>反向传播和梯度下降：</strong>通过反向传播算法计算损失函数相对于每层参数的梯度，进而通过梯度下降算法更新卷积核和全连接层的参数，使得 CNN 逐渐优化，从而提高预测准确率。</li>
				
						</ol>
						<li>模型输出</li>
						<p>CNN 在处理一张图像后输出一个分类或预测结果。对于图像分类任务，输出层通常表示预测类别及其概率。例如，如果是手写数字识别任务，输出会是一个 0-9 的数字。</p>
					</ol>
				</div>
			</div>	
			 
			<div id = "educationnewCNN">
				<div class="educationTitle">现代卷积神经网络</div>
				<div class="educationContent">
					<div class="educationSection">（一）概述</div>
					<p>虽然深度神经网络的概念非常简单——将神经网络堆叠在一起。但由于不同的网络架构和超参数选择，这些神经网络的性能会发生很大变化。本章将按照时间顺序介绍以下模型：</p>
					<ul>
						<li><strong>AlexNet。</strong>它是第一个在大规模视觉竞赛中击败传统计算机视觉模型的大型神经网络；</li>
						<li><strong>使用重复块的网络（VGG）。</strong>它重复使用由卷积层和卷积层（用来代替全连接层）来构建深层网络;</li>
						<li><strong>网络中的网络（NiN）。</strong>它重复使用由卷积层和卷积层（用来代替全连接层）来构建深层网络;</li>
						<li><strong>含并行连结的网络（GoogLeNet）。</strong>它使用并行连结的网络，通过不同窗口大小的卷积层和最大汇聚层来并行抽取信息；</li>
						<li><strong>残差网络（ResNet）。</strong>它通过残差块构建跨层的数据通道，是计算机视觉中最流行的体系架构；</li>
						<li><strong>稠密连接网络（DenseNet）。</strong>它的计算成本很高，但给我们带来了更好的效果。</li>
					</ul>

					<div class="educationSection">（二）深度卷积神经网络（AlexNet）</div>
					<p>2012年，AlexNet横空出世。它首次证明了学习到的特征可以超越手工设计的特征。它一举打破了计算机视觉研究的现状。 AlexNet使用了8层卷积神经网络，并以很大的优势赢得了2012年ImageNet图像识别挑战赛。</p>
					<p>AlexNet和LeNet的设计理念非常相似，但也存在显著差异。首先，AlexNet比相对较小的LeNet5要深得多。AlexNet由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。其次，AlexNet使用ReLU而不是sigmoid作为其激活函数。</p>
					<image class="educationImage" src="resources/educationimages/newCNN_1.png"  alt="newCNN_1" title="从LeNet（左）到AlexNet(右)" ></image>
					<p>在AlexNet的第一层，卷积窗口的形状是11×11。由于ImageNet中大多数图像的宽和高比MNIST图像的多10倍以上，因此，需要一个更大的卷积窗口来捕获目标。第二层中的卷积窗口形状被缩减为5×5，然后是3×3。此外，在第一层、第二层和第五层卷积层之后，加入窗口形状为3×3、步幅为2的最大汇聚层。而且，AlexNet的卷积通道数目是LeNet的10倍。</p>
					<p>在最后一个卷积层后有两个全连接层，分别有4096个输出。 这两个巨大的全连接层拥有将近1GB的模型参数。由于早期GPU显存有限，原版的AlexNet采用了双数据流设计，使得每个GPU只负责存储和计算模型的一半参数。幸运的是，现在GPU显存相对充裕，所以现在很少需要跨GPU分解模型。</p>
					<p>此外，AlexNet将sigmoid激活函数改为更简单的ReLU激活函数。 一方面，ReLU激活函数的计算更简单，它不需要如sigmoid激活函数那般复杂的求幂运算。 另一方面，当使用不同的参数初始化方法时，ReLU激活函数使训练模型更加容易。 当sigmoid激活函数的输出非常接近于0或1时，这些区域的梯度几乎为0，因此反向传播无法继续更新一些模型参数。 相反，ReLU激活函数在正区间的梯度总是1。 因此，如果模型参数没有正确初始化，sigmoid函数可能在正区间内得到几乎为0的梯度，从而使模型无法得到有效的训练。</p>
					
					<div class ="educationSection">（三）使用块的网络（VGG）</div>
					<p>虽然AlexNet证明深层神经网络卓有成效，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络。使用块的想法首先出现在牛津大学的视觉几何组（visual geometry group）的VGG网络中。通过使用循环和子程序，可以很容易地在任何现代深度学习框架的代码中实现这些重复的架构。</p>
					<p>经典卷积神经网络的基本组成部分是下面的这个序列：（1）带填充以保持分辨率的卷积层；（2）非线性激活函数，如ReLU；（3）汇聚层，如最大汇聚层。而一个VGG块与之类似，由一系列卷积层组成，后面再加上用于空间下采样的最大汇聚层。在最初的VGG论文中 (Simonyan and Zisserman, 2014)，
						作者使用了带有3×3卷积核、填充为1（保持高度和宽度）的卷积层，和带有2×2汇聚窗口、步幅为2（每个块后的分辨率减半）的最大汇聚层 。
						与AlexNet、LeNet一样，VGG网络可以分为两部分：第一部分主要由卷积层和汇聚层组成，第二部分由全连接层组成。</p>
					<image class="educationImage" src="resources/educationimages/newCNN_2.png"  alt="newCNN_2" title = " 从AlexNet到VGG"></image>
					<p>原始VGG网络有5个卷积块，其中前两个块各有一个卷积层，后三个块各包含两个卷积层。 第一个模块有64个输出通道，每个后续模块将输出通道数量翻倍，直到该数字达到512。由于该网络使用8个卷积层和3个全连接层，因此它通常被称为VGG-11。</p>
					
					<div class="educationSection">（四）网络中的网络（NiN）</div>
					<p>LeNet、AlexNet和VGG都有一个共同的设计模式：通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。 AlexNet和VGG对LeNet的改进主要在于如何扩大和加深这两个模块。 或者，可以想象在这个过程的早期使用全连接层。然而，如果使用了全连接层，可能会完全放弃表征的空间结构。 网络中的网络（NiN）提供了一个非常简单的解决方案：在每个像素的通道上分别使用多层感知机。</p>
					<p>卷积层的输入和输出由四维张量组成，张量的每个轴分别对应样本、通道、高度和宽度。 另外，全连接层的输入和输出通常是分别对应于样本和特征的二维张量。 NiN的想法是在每个像素位置（针对每个高度和宽度）应用一个全连接层。 如果我们将权重连接到每个空间位置，我们可以将其视为1×1卷积层，或作为在每个像素位置上独立作用的全连接层。 从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征（feature）。 </p>
					<p> NiN块以一个普通卷积层开始，后面是两个1×1的卷积层。这两个1×1卷积层充当带有ReLU激活函数的逐像素全连接层。 第一层的卷积窗口形状通常由用户设置。 随后的卷积窗口形状固定为1×1。 </p>
					<image class="educationImage" src="resources/educationimages/newCNN_3.png"  alt="newCNN_3" title = " VGG和NiN的块之间主要的架构差异"></image>
					<p>最初的NiN网络是在AlexNet后不久提出的，显然从中得到了一些启示。 NiN使用窗口形状为11×11、5×5和3×3的卷积层，输出通道数量与AlexNet中的相同。 每个NiN块后有一个最大汇聚层，汇聚窗口形状为3×3，步幅为2。</p>
					<p>NiN和AlexNet之间的一个显著区别是NiN完全取消了全连接层。 相反，NiN使用一个NiN块，其输出通道数等于标签类别的数量。最后放一个全局平均汇聚层（global average pooling layer），生成一个对数几率 （logits）。NiN设计的一个优点是，它显著减少了模型所需参数的数量。然而，在实践中，这种设计有时会增加训练模型的时间。</p>

					<div class = "educationSection">（五）含并行连结的网络（GoogLeNet）</div>
					<p>GoogLeNet吸收了NiN中串联网络的思想，并在此基础上做了改进。 这篇论文的一个重点是解决了什么样大小的卷积核最合适的问题。 毕竟，以前流行的网络使用小到1×1，大到11×11的卷积核。 本文的一个观点是，有时使用不同大小的卷积核组合是有利的。 </p>
					<p>在GoogLeNet中，基本的卷积块被称为Inception块（Inception block），Inception块由四条并行路径组成。 前三条路径使用窗口大小为1×1、3×3和5×5的卷积层，从不同空间大小中提取信息。 中间的两条路径在输入上执行1×1卷积，以减少通道数，从而降低模型的复杂性。 第四条路径使用3×3最大汇聚层，然后使用1×1卷积层来改变通道数。 这四条路径都使用合适的填充来使输入与输出的高和宽一致，最后我们将每条线路的输出在通道维度上连结，并构成Inception块的输出。在Inception块中，通常调整的超参数是每层输出通道数。 </p>
					<image class="educationImage" src="resources/educationimages/newCNN_4.png"  alt="newCNN_4" title = " Inception块的架构"></image>
					<p>GoogLeNet一共使用9个Inception块和全局平均汇聚层的堆叠来生成其估计值。Inception块之间的最大汇聚层可降低维度。 第一个模块类似于AlexNet和LeNet，Inception块的组合从VGG继承，全局平均汇聚层避免了在最后使用全连接层。</p>
					<image class="educationImage" src="resources/educationimages/newCNN_5.png"  alt="newCNN_5" title = "GoogLeNet架构"></image>

					<div class = "educationSection">（六）残差网络（ResNet）</div>
					<p>假设我们的原始输入为x，而希望训练出的理想映射为f(x)（作为 图3.6上方激活函数的输入）。 图3.6左图虚线框中的部分需要直接拟合出该映射f(x)，而右图虚线框中的部分则需要拟合出残差映射f(x)−x。 残差映射在现实中往往更容易优化。 我们只需将 图3.6中右图虚线框内上方的加权运算（如仿射）的权重和偏置参数设成0，那么f(x)即为恒等映射。 实际中，当理想映射f(x)极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。图3.6右图是ResNet的基础架构–残差块（residual block）。 在残差块中，输入可通过跨层数据线路更快地向前传播。 </p>
					<image class="educationImage" src="resources/educationimages/newCNN_6.png"  alt="newCNN_6" title = "正常块（左）和残差块（右）"></image>
					<p>ResNet沿用了VGG完整的3×3卷积层设计。 残差块里首先有2个有相同输出通道数的3×3卷积层。 每个卷积层后接一个批量规范化层和ReLU激活函数。 然后我们通过跨层数据通路，跳过这2个卷积运算，将输入直接加在最后的ReLU激活函数前。 这样的设计要求2个卷积层的输出与输入形状一样，从而使它们可以相加。 如果想改变通道数，就需要引入一个额外的1×1卷积层来将输入变换成需要的形状后再做相加运算。 </p>
					<image class="educationImage" src="resources/educationimages/newCNN_7.png"  alt="newCNN_7" title = " 包含和不包含1×1卷积层的残差块"></image>
					<p>ResNet的前两层跟之前介绍的GoogLeNet中的一样： 在输出通道数为64、步幅为2的7×7卷积层后，接步幅为2的3×3的最大汇聚层。 不同之处在于ResNet每个卷积层后增加了批量规范化层。</p>
					<p>GoogLeNet在后面接了4个由Inception块组成的模块。 ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。 第一个模块的通道数同输入通道数一致。 由于之前已经使用了步幅为2的最大汇聚层，所以无须减小高和宽。 之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。</p>
					<image class="educationImage" src="resources/educationimages/newCNN_8.png"  alt="newCNN_8" title = " ResNet-18架构"></image>

					<div class = "educationSection">（七）稠密连接网络（DenseNet）</div>
					<p>ResNet将f分解为两个部分：一个简单的线性项和一个复杂的非线性项，根据泰勒展开式，如果向将f拓展成超过两部分信息，一种解决方案就是DenseNet。</p>
					<image class="educationImage" src="resources/educationimages/newCNN_9.png"  alt="newCNN_9" title = " ResNet&DenseNet"></image>
					<p>上图ResNet（左）和DenseNet（右）在跨层连接上的主要区别：使用相加和使用连结ResNet和DenseNet的关键区别在于，
						DenseNet输出是连接（用图中的[,]表示）而不是如ResNet的简单相加。 因此，在应用越来越复杂的函数序列后，我们执行从x到其展开式的映射：
						<span class="math inline">\[\mathbf{x}\to[\mathbf{x},f_1(\mathbf{x}),f_2([\mathbf{x},f_1(\mathbf{x})]),f_3([\mathbf{x},f_1(\mathbf{x}),f_2([\mathbf{x},f_1(\mathbf{x})])]),\ldots].\]</span>
						</p>
					<p>稠密网络主要由2部分构成：稠密块（dense block）和过渡层（transition layer）。 前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。</p>
					<p>DenseNet首先使用同ResNet一样的单卷积层和最大汇聚层。接下来，类似于ResNet使用的4个残差块，DenseNet使用的是4个稠密块。 与ResNet类似，我们可以设置每个稠密块使用多少个卷积层。在每个模块之间，ResNet通过步幅为2的残差块减小高和宽，DenseNet则使用过渡层来减半高和宽，并减半通道数。与ResNet类似，最后接上全局汇聚层和全连接层来输出结果。</p>

				</div>
			</div>

					
				<div id = "educationRNN">
					<div class="educationTitle">循环神经网络</div>
					<div class="educationContent">
						<div class="educationSection">（一）概述</div>
						<p>循环神经网络 (RNN) 是一种人工神经网络，旨在处理顺序数据，例如时间序列或自然语言。它们具有反馈连接，使它们能够保留先前时间步骤的信息，从而能够捕获时间依赖性。这使得 RNN 非常适合语言建模、语音识别和顺序数据分析等任务。苹果的Siri和谷歌的语音搜索都使用RNN。</p>
						<image class="educationImage" src="resources/educationimages/RNN_1.png"  alt="RNN_1" ></image>
						<p>RNN 是一种可用于对序列数据建模的神经网络。 RNN 由前馈网络组成，其行为与人脑相似。简而言之，循环神经网络可以以其他算法无法做到的方式预测顺序数据。标准神经网络中的所有输入和输出都是相互独立的，但是在某些情况下，例如在预测短语的下一个单词时，前面的单词是必要的，因此必须记住前面的单词。结果，RNN 应运而生，它使用隐藏层来克服这个问题。 RNN 最重要的组成部分是隐藏状态，它记住有关序列的特定信息。</p>
						<p>RNN 有一个内存，用于存储有关计算的所有信息。它对每个输入采用相同的设置，因为它通过在所有输入或隐藏层上执行相同的任务来产生相同的结果。</p>
						<image class="educationImage" src="resources/educationimages/RNN_2.png"  alt="RNN_2" title = ></image>
						<p>循环神经网络对序列的每个元素使用相同的权重，从而减少了参数的数量，并允许模型泛化为不同长度的序列。由于其设计，RNN 泛化到序列数据以外的结构化数据，例如地理或图形数据。与许多其他深度学习技术一样，循环神经网络相对较旧。它们最初是在 20 世纪 80 年代开发的，但直到最近我们才充分认识到它们的潜力。 20 世纪 90 年代长短期记忆 (LSTM) 的出现，加上计算能力的提高和我们现在必须处理的大量数据，确实将 RNN 推到了最前沿。</p>
						
						<div class ="educationSection">（二）RNN模型提出</div>
						<ol>
							<li><strong>基本RNN结构：</strong>为了解决普通DNN无法有效获取上下文信息的缺点，RNN最基本的改良点在于增加一个“模块”用于存储上下文信息。下图为一个典型RNN的结构示意图：</li>
							<image class="educationImage" src="resources/educationimages/RNN_3.png"  alt="RNN_3" ></image>
							<p>上图是一个典型的RNN结构图，初看可能会不太理解。理解首先不看右侧的矩阵，只看左侧的顺序网络，即图(b)，表示的就是一个普通的前馈神经网络。 接下来回头看图(a)，RNN相比于一般前馈网络，增加了一个保存上下文信息的权重矩阵，也即每次计算输出不仅要考虑当前输入数据，还要考虑序列数据的上下文信息。</p>
							<li><strong>RNN展开结构：</strong>
								我们知道了RNN模型增加了一个权重矩阵用于存储输入序列的上下文信息，接下来我们来介绍RNN结构如何进行模型计算以及上下文信息如何应用到RNN结构。为了更好地理解RNN计算方式，下图是一个序列展开的RNN示意图（即上图a的时序展开图）：
							</li>
							<image class="educationImage" src="resources/educationimages/RNN_4.png"  alt="RNN_4"  ></image>
							<p>其中表示时刻的模型输入，表示对应的输入结果。RNN模型计算公式如下：
								<span class="math inline">\[\begin{array}{l}{y_{i}=g(Vh_{i})}\\{h_{i}=f(Ux_{i}+Wh_{i-1})}\end{array}\]</span>
								由计算公式可以看出，隐藏层的输出隐变量在RNN中既与当前时刻输入有关，又与上一时刻的隐变量有关。因此可以认为包含了影响当前输入信息的“上下文”信息，而可学习的参数矩阵决定了上下文信息对当前影响程度。 值得注意的是，在整个模型处理期间，参数矩阵是使用的同一个矩阵。
							</p>
							<li>
								<strong>时间反向传播（Backpropagation Through Time, BPTT）：</strong>
								<p>BPTT是训练RNN的核心算法，它将反向传播算法扩展到时间序列，以学习时间相关的信息。由于RNN具有循环结构，BPTT的关键在于将误差在时间维度上展开，使每个时间步都能调整相应的参数。</p>
								<p>BPTT算法的基本思想是将RNN在时间维度上“展开”（Unroll），将一个循环结构的网络拆解为多个时序步骤的等效神经网络，这样每个时间步都可以看作一个全连接层的传播。通过这种展开，RNN在每个时间步的状态和输出变得“独立”，可以使用常规反向传播算法在时间维度上计算误差和梯度。</p>
							</li>
						</ol>
						<div class ="educationSection">（三）RNN模型结构变化</div>
						<p>根据输入长度与输出序列长度的不同，可以将RNN模型结构分为N to N，N to 1, 1 to N，及N to M四种:</p>
						<ol>
							<li><strong>N to N结构RNN模型</strong></li>
							<p>第一种是常见的输入长度与输出长度相同的RNN结构，也就表示每一个输入数据都有对应的一个输出值，可以用于逐序列判断或分类任务，如序列标注、视频帧分类、NER及分词等任务。其结构图如下：</p>
							<image class="educationImage" src="resources/educationimages/RNN_5.png"  alt="RNN_5" ></image>
							<span class = "math inline">\[\begin{array}{l}{y_{i}=g(Vh_{i})}\\{h_{i}=f(Ux_{i}+Wh_{i-1})}\end{array}\]</span>
							<li><strong>N to 1结构RNN模型</strong> </li>
							<p>表示输入一个序列只生成一个输出值(通常用尾数据对应输出值)。其意义是序列的输出结果蕴含整条序列数据的语义信息及上下文信息。常见应用：文字分类、文章分类及图像分类等任务。</p>
							<image class="educationImage" src="resources/educationimages/RNN_6.png"  alt="RNN_6" ></image>
							<span class = "math inline">\[Y = Y_{\text{smallN}} = g(Vh_N)/h_i = f(Ux_i + Wh_{i-1})\]</span>
							<li><strong> 1 to N结构RNN模型</strong></li>
							<p> 表示一个输入数据对应输出一个序列的模型。其意义表示一个起始状态或者种子数据，生成一个序列的输出结果。常见应用包括由图像生成文章，由类别生成音乐、文章及代码等，由种子数据生成序列的任务。</p>
							<image class="educationImage" src="resources/educationimages/RNN_7.png"  alt="RNN_7" ></image>
							<image class="educationImage" src="resources/educationimages/RNN_8.png"  alt="RNN_8" ></image>
							<p>1 to N结构RNN模型根据输入只有一个向量，输入位置的不同，可以分为只在首个时刻输入(上左图)和在每个时刻均输入(上右图)两种结构。其中第一种结构计算方法如下：</p>
							<span class = "math inline">\[\begin{aligned}
								y_i &= g(Vh_i) \\
								h_i &=
								\begin{cases} 
								f(Wh_{i-1}) & \text{where } i > 1 \\ 
								f(Ux_1 + Wh_0) & \text{where } i = 1 
								\end{cases}
								\end{aligned}\]</span>
							<p>类似地，第二种结构计算方法如下：</p>
							<span class = "math inline"></span>
							<li><strong> N to M结构RNN模型</strong></li>
							<p>即输入及输出序列不等长的结构。N和M分别为输入序列长度及输出序列长度，该结构我们采用一个N to 1结构RNN及一个1 to M结构组合来实现，详细结构如下图：</p>
							<image class="educationImage" src="resources/educationimages/RNN_9.png"  alt="RNN_9" ></image>
							<p> 由上图可以看出，将两个不同长度的RNN进行组合，能够控制模型的输出序列长度。在两个模型之间，增加了一个上下文向量，由第一个RNN模型的输出计算得来，向量包含着输入序列的语义信息与序列信息。在上图中是将上下文向量作为了第二个RNN模型的输入数据，并在第二个RNN模型对于初始隐藏变量进行随机初始化。通常将第一个RNN称为encoder（编码器），第二个RNN称为decoder（解码器）此外，还可以利用上下文向量对第二个RNN模型的隐藏变量进行初始化，结构如下：</p>
							<image class="educationImage" src="resources/educationimages/RNN_10.png"  alt="RNN_10" ></image>
							<p>通过N to M结构RNN模型，可以适应各类序列处理任务，常见的如机器翻译、语音识别、文本摘要及阅读理解等任务。由于输入输出都是序列，该模型也称为seq2seq模型。常用的上下文向量的计算方法包含如下三种：</p>
							<span class = "math inline"></span>
							<p> 其中，第一种计算方法为直接将encoder的输出作为上下文向量；第二种计算方法为通过变化encoder的输出计算得到；第三种计算方法为利用一个encoder的输出序列计算得到。此外，由于encoder的输出只变换成 上下文向量传入decoder进行了计算，难免造成decoder计算序列加长导致的上下文信息衰减。由此，人们引入了注意力机制（Attention）来增强数据信息，我们在attention机制部分进行详解。</p>

						</ol>
						<div class ="educationSection">（四）梯度爆炸和梯度消失</div>
						<ol>
							<li><strong>什么是梯度爆炸和梯度消失</strong></li>
							<p>就其输入而言，梯度是偏导数，梯度量化了当输入稍微改变时函数的输出变化的程度。函数的斜率也称为梯度。斜率越陡，模型学习的速度越快，梯度就越高。另一方面，如果斜率为零，模型将停止学习。梯度用于测量所有权重相对于误差变化的变化。</p>
							<image class="educationImage" src="resources/educationimages/RNN_11.png"  alt="RNN_11" ></image>
							<ul>
								<li><strong>梯度爆炸：</strong>当算法无缘无故地给权重赋予荒谬的高优先级时，就会发生梯度爆炸。幸运的是，截断或压缩梯度是解决此问题的简单方法。</li>
								<li><strong>梯度消失：</strong>当梯度值太小时，就会发生梯度消失，导致模型停止学习或花费太长时间。这是 20 世纪 90 年代的一个大问题，而且它比梯度爆炸更难解决。</li>

							</ul>
							<li><strong>如何解决RNN的梯度爆炸或梯度消失</strong></li>
							<ul>
								<li><strong>解决梯度爆炸：</strong>梯度裁剪，即为梯度更新时的梯度设置上限，当超过阈值将强制裁剪，避免出现过高阈值。</li>
								<li><strong>解决梯度消失：</strong>使用Relu激活函数解决梯度消失的原理是，Relu函数在自变量大于0是，因变量恒为1，由此避免梯度过小；改用变种版本的RNN结构，常见的包括LSTM模型及GRU模型。</li>

							</ul>
						</ol>
						<div class ="educationSection">（五）RNN优缺点</div>
						<ol>
							<li><strong>RNN的优点</strong></li>
							<ul>
								<li>有效处理顺序数据，包括文本、语音和时间序列。</li>
								<li>与前馈神经网络不同，处理任意长度的输入。</li>
								<li>跨时间步共享权重，提高训练效率。</li>
								
							</ul>
							<li><strong>RNN的缺点</strong></li>
							<ul>
								<li>容易出现梯度消失和爆炸问题，阻碍学习。</li>
								<li>训练可能具有挑战性，尤其是对于长序列。</li>
								<li>计算速度比其他神经网络架构慢。</li>
							</ul>
						</ol>

						<div class = "educationSection">（六）RNN实战</div>
						<ol>
							<li>理论实现</li>
							<ul>
								<li>输入层</li>
								<ol>
									<li><strong>当前事件输入：</strong>记为<span class ="math inline">\(\mathbf{x}_{\mathbf{t}}\)</span>，表示在时间t输入的数据。</li>
									<li><strong>隐藏状态（记忆）：</strong>记为\(\mathbf{h}_{\mathbf{t-1}}\)，表示从前一个时间步<span class ="math inline">\(t-1\)</span>传递下来的隐藏状态，包含历史信息。</li>
								</ol>
								<li>隐藏层</li>
								<p>RNN的核心是通过隐藏状态来保持之前时间步的信息，并与当前的输入结合。标准RNN隐藏层的更新公式如下：</p>
								<span class ="math inline">\[\mathrm{h}_t=\phi(W_\mathrm{h}\mathrm{h}_{t-1}+W_xx_t+b_\mathrm{h})\]</span>
								<p><span class ="math inline">\(W_h\)</span>:隐藏层权重矩阵，用于调整前一时间步的隐藏状态<span class ="math inline">\(h_t-1\)</span>的影响。</p>
								<p><span class ="math inline">\(W_x\)</span>:：输入权重矩阵，用于调整当前输入<span class ="math inline">\(x_t\)</span>的影响。</p>
								<p><span class ="math inline">\(b_h\)</span>:偏置项，用于对输出进行平移。</p>
								<p><span class ="math inline">\(phi)</span>：激活函数，通常选择<span class ="math inline">\(\text{tanh}\)</span>的影响。或 ReLU激活函数，使得网络具有非线性表达能力。
									该更新公式的核心是将历史隐藏状态和当前输入线性组合，再通过激活函数更新为当前时间步的隐藏状态<span class ="math inline">\(h_t\)</span>。每一时间步都会执行该更新，使得RNN可以逐步积累时间序列信息。
								</p>
								<li>输出层</li>
								<p>输出层根据隐藏状态生成每个时间步的输出 <span class ="math inline">\(o_t\)</span>：<span class ="math inline">\(o_t=W_0h_t+b_0\)</span></p>
								<p><span class ="math inline">\(W_o\)</span>：输出层权重矩阵，将隐藏状态映射到输出空间。</p>
								<p><span class ="math inline">\(b_o\)</span>：输出层偏置项。</p>
								<p>RNN的输出可以是每个时间步的输出（适合序列输出）或最终时间步的隐藏状态（适合序列分类）。</p>
								<li>前向传播过程</li>
								<p>标准RNN的前向传播过程是一个循环递归的过程。RNN层会从 t=1t=1t=1 一直传播到 TTT（时间步数），逐步计算每个时间步的隐藏状态和输出。这种递归使得RNN能够在较短的时间序列中捕捉依赖关系。</p>
								
							</ul>
							<li>代码实现</li>
							<p>RNN及其变体是非常经典且有意义的工作，故代码实现有多种方式，总体来说分为自购建与API调用。</p>
							<image class="educationImage" src="resources/educationimages/RNN_12.png"  alt="RNN_12" ></image>
							<ul>
								<li><strong>自构建</strong></li>
								<ol>
									<li><strong>独热编码：</strong>即NLP中的基本操作one-hot encoding，将文本预处理（string->num），并将索引映射为互补相同的单位向量，方便后续模型读入。</li>
									<li><strong>初始化模型参数：</strong>需要定义隐藏层参数（重要）、输出层参数、附加梯度等模型参数。</li>
									<li><strong>模型/网络定义：</strong>根据需求与RNN定义去搭建模型，包括隐状态返回（初始化时）、计算与输出，以及模型的激活与迭代。</li>
									<li><strong>预测：</strong>定义预测函数来生成prefix（一个用户提供的包含多个字符的字符串）之后的新字符。</li>
									<li><strong>梯度裁剪：</strong>正常的RNN反向传播会产生O（T）的矩阵乘法链，T较大时可能导致梯度爆炸或消失，故需要进行梯度裁剪。</li>
									<li><strong>训练：</strong>将处理后数据“喂”给模型，进行迭代训练（顺序分区/随机抽样），以困惑度或epoch作为停止训练指标。</li>
									<li><strong>输出：</strong>训练好的模型/文本预测结果</li>

								</ol>
								<li><strong>API调用</strong></li>
								<p>自购建的方式可以实现不同方案/策略的RNN，但无论是代码实现难度、效率/性能都不是最优选择，由于RNN类模型是经典模型，故Tensorflow、Pytorch等主流框架中均做了定义（API）与优化，便于我们快速搭建模型并应用，通过API的代码实现非常简洁，全流程为数据集读入->模型定义/引入（通过API）->训练与预测。代码核心即模型的引入。</p>
							</ul>
							
						</ol>
					</div>
				</div>



				<div id = "educationnewRNN">
					<div class="educationTitle">现代循环神经网络</div>
					<div class="educationContent">
						<div class="educationSection">（一）概述</div>
						<p>在上一章中，我们介绍了如何利用循环神经网络（RNN）来建立语言模型以处理文本数据。然而，面对当今日益复杂的序列学习任务，传统RNN模型可能会遇到一些困难。一个突出的问题是数值不稳定性，特别是在长序列数据中，RNN模型的梯度很容易发生消失或爆炸，使得模型难以有效学习长时依赖。尽管我们可以通过梯度裁剪等技巧来缓解这个问题，但它们并不能完全解决问题。为此，我们需要引入一些更强大的模型设计，来让RNN更具表现力和稳定性。本章将按顺序介绍以下网络结构：</p>
						<ol>
							<li><strong>GRU（门控循环单元）：</strong>GRU是一种改进的RNN结构，利用更新门和重置门来控制信息流，简化了长时依赖的处理，使训练更高效。</li>
							<li><strong>LSTM（长短期记忆网络）：</strong>LSTM通过引入遗忘门、输入门和输出门，能够更有效地捕捉长时依赖关系，适合处理长序列数据。</li>
							<li><strong>深度RNN：</strong>深度RNN由多层隐藏层堆叠而成，使模型能够逐层捕捉更复杂的序列特征，提升模型的表达能力。</li>
							<li><strong>双向RNN：</strong>双向RNN通过同时进行正向和反向计算，可以结合前后文信息，使得模型在处理自然语言任务时更具上下文感知能力。</li>
							<li><strong>Transformer：</strong>Transformer采用自注意力机制，允许模型在序列的任意位置间建立直接依赖关系，从而高效处理长序列，极大提升了训练速度和并行计算能力。</li>

						</ol>
						<div class="educationSection">（二）门控循环单元（GRU）</div>
						<p>GRU（门控循环单元）和LSTM（长短期记忆网络）是两种广泛应用的改进型RNN架构。它们通过引入“门”结构，允许模型更有效地控制信息流，选择性地保留或忘记特定信息。GRU和LSTM能缓解梯度消失问题，更好地捕捉长程依赖。GRU结构相对简单，计算速度快；而LSTM在处理更复杂的依赖关系时更具优势，因此这两种结构都成为序列建模中的重要工具，本节中讲介绍GRU单元。</p>
						<p>GRU单元的工作方式类似一条“智能传送带”，它能根据需要对输入信息和隐藏状态进行“放行”或“拦截”操作，来过滤掉不重要的信息并保存旧的隐藏状态。具体来说，它通过重置门（reset gate）和更新门（update gate）来控制信息的流动，其中门机制都是带有激活函数的全连接层。</p>
						<ol>
							<li><strong>重置门</strong></li>
							<p>重置门可以看作是一个“拦截”开关，它会根据当前输入内容来选择性地忘记部分旧隐藏状态的信息。如果重置门检测到某些过时的信息（比如过早的背景信息）可以忽略，它就会“清除”这些内容，削弱过去的影响，这样模型就可以更专注于新输入的数据。</p>
							<span class = "math inline">\[\mathbf{R}_t=\sigma(\mathbf{X}_t\mathbf{W}_{xr}+\mathbf{H}_{t-1}\mathbf{W}_{hr}+\mathbf{b}_r)\]</span>
							<ul>
								<li><span class = "math inline">\(R_t\)</span>是重置门在时间步t的输出。</li>
								<li>σ一般是Sigmoid激活函数，其输出范围在0到1之间。</li>
								<li><span class = "math inline">\(\mathbf{W}_{xr}\)</span>和<span class = "math inline">\(\mathbf{W}_{hr}\)</span>分别是重置门的输入和递归权重矩阵。</li>
								<li><span class = "math inline">\(H_t-1\)</span>是前一时刻的隐藏状态。</li>
								<li><span class = "math inline">\(X_t\)</span>是当前时刻的输入。</li>
							</ul>
							<li><strong>更新门</strong></li>
							<p>更新门负责决定当前时刻的输入信息和之前记忆中的隐藏状态信息，哪一部分应该被“放行”到下一步。换句话说，更新门会选择保留多少旧信息，以及吸收多少新信息，从而帮助GRU在新旧信息之间找到平衡。</p>
							<span class = "math inline">\[\mathbf{Z}_t=\sigma(\mathbf{X}_t\mathbf{W}_{xz}+\mathbf{H}_{t-1}\mathbf{W}_{hz}+\mathbf{b}_z)\]</span>
							<ul>
								<li><span class = "math inline">\(Z_t\)</span>是更新门在时间步<span class = "math inline">\(t\)</span>的输出。</li>
								<li><span class = "math inline">\(\mathbf{W}_{xz}\)</span>和<span class = "math inline">\(\mathbf{W}_{hz}\)</span>分别是更新门的输入和递归权重矩阵。</li>

							</ul>
							<li><strong>候选隐状态</strong></li>
							<p>当输入一个新的数据时，GRU单元会让更新门和重置门协同工作，首先通过重置门基于当前输入和前一时刻隐藏状态计算得到候选隐藏状态， 每当重置门中的值接近1时，网络倾向于结合更多前面的记忆，效果更接近RNN。对于重置门中的值接近0，任何预先存在的隐状态都会被重置为默认值，效果更接近多层感知机。</p>
							<span class = "math inline">\[\tilde{\mathcal{H}}_t=\tanh(\mathbf{X}_t\mathbf{W}_{x\mathbf{h}}+(\mathbf{R}_t\odot\mathbf{H}_{t-1})\mathbf{W}_{h\mathbf{h}}+\mathbf{b}_h)\]</span>
							<ul>
								<li><span class = "math inline">\(\tilde{\mathcal{H}}_t\)</span>是候选隐状态。</li>
								<li><span class = "math inline">\(\tanh\)</span>是双曲正切激活函数，其输出范围在-1到1之间。</li>
								<li><span class = "math inline">\(\mathbf{W}_{xh}\)</span>和<span class = "math inline">\(\mathbf{W}_{hh}\)</span>分别是候选隐状态的输入和递归权重矩阵。</li>
								<li><span class = "math inline">\(\odot\)</span>表示逐元素乘法（Hadamard乘积）。</li>
							</ul>
							<li><strong>更新隐状态<span class = "math inline">\(H_t\)</span></strong></li>
							<p>在最后一步，网络需要计算<span class = "math inline">\(H_t\)</span>，该向量将根据更新门<span class = "math inline">\(Z_t\)</span>决定当前隐藏状态
								<span class = "math inline">\(H_t\)</span>的更新比例，即要更新多少新的隐藏状态，并传递到下一个单元中。在这个过程中，我们需要使用更新门，
								它决定了当前记忆内容<span class = "math inline">\(\tilde{\mathcal{H}}_t\)</span>和前一时间步<span class = "math inline">\(H_t-1\)</span>中需要保留和收集的信息是什么。这一过程可以表示为：</p>
								<span class = "math inline">\[\mathcal{H}_t=\mathbf{Z}_t\odot\mathbf{H}_{t-1}+(1-\mathbf{Z}_t)\odot\tilde{\mathbf{H}}_t\]</span>
								<p>这个公式表明，当前时刻的隐藏状态是前一时刻的隐藏状态（通过更新门调整）和候选隐状态的线性组合。如果更新门
									<span class = "math inline">\(Z_t\)</span>接近1，那么<span class = "math inline">\(H_t\)</span>将接近 <span class = "math inline">\(H_t-1\)</span>，
									即前一时刻的隐藏状态将几乎不变地传递到当前时刻，完全忽略<span class = "math inline">\(X_t\)</span>的影响。如果更新门 
									<span class = "math inline">\(Z_t\)</span>接近0，那么 <span class = "math inline">\(H_t\)</span>将接近 
									<span class = "math inline">\(\tilde{\mathcal{H}}_t\)</span>，即当前时刻的隐藏状态将主要由候选隐状态决定。</p>
									<image class="educationImage" src="resources/educationimages/newRNN_1.png"  alt="newRNN_1" ></image>

						</ol>
						<div class="educationSection">（三）长短期记忆网络（LSTM）</div>
						<p>GRU（门控循环单元）和LSTM（长短期记忆网络）是两种广泛应用的改进型RNN架构。它们通过引入“门”结构，允许模型更有效地控制信息流，选择性地保留或忘记特定信息。GRU和LSTM能缓解梯度消失问题，更好地捕捉长程依赖。GRU结构相对简单，计算速度快；而LSTM在处理更复杂的依赖关系时更具优势，因此这两种结构都成为序列建模中的重要工具，本节中讲介绍LSTM网络。</p>
						<p>LSTM（长短期记忆网络）单元的工作方式与GRU类似，但实现方式是通过三个关键的门机制：遗忘门（forget gate）、输入门（input gate）和输出门（output gate），并引入记忆元（memory cell）来实现信息的流动和记忆管理，其中门机制都是带有激活函数的全连接层。</p>
						<ol>
							<li><strong>门机制</strong></li>
							<p>遗忘门决定了哪些旧的记忆信息可以被“遗忘”。它会根据当前输入和上一个时刻的隐藏状态来选择性地保留或忘记部分记忆。比如，当遗忘门检测到某些过时的背景信息时，就会让这些信息在记忆中被“清除”，这样可以减轻旧信息的干扰。</p>
							<p>输入门控制哪些新信息可以被“储存”到记忆中，并决定当前时刻的输入内容将如何影响记忆单元。输入门的输出会与候选记忆相乘，从而筛选出当前要存入的有效信息。输出门则会在最终决定使用多少更新的记忆元信息。</p>
							<span class= "math inline">\[F_t=\sigma(X_tW_{xf}+H_{t-1}W_{hf}+b_f)\]</span>
							<span class= "math inline">\[I_t=\sigma(X_tW_{xi}+H_{t-1}W_{hi}+b_i)\]</span>
							<span class= "math inline">\[O_t=\sigma(X_tW_{xo}+H_{t-1}W_{ho}+b_o)\]</span>

							<li><strong>候选记忆内容<span class = "math inline">\(\tilde{\mathcal{C}}_t\)</span></strong></li>
							<p>候选记忆元<span class = "math inline">\(\tilde{\mathcal{C}}_t\)</span>用于生成潜在的新的记忆内容，根据输入数据
								<span class = "math inline">\(X_t\)</span>和前一隐藏状态
								<span class = "math inline">\(\tilde{\mathcal{H}}_t-1\)</span>生成候选记忆元，然后通过输入门将有效信息添加到记忆单元中。它决定了新的信息进入记忆的程度，其本质等同于RNN中的隐状态
								<span class = "math inline">\(\tilde{\mathcal{H}}_t\)</span>。</p>
								<span class = "math inline">\[\tilde{C}_t=\tanh(X_tW_{xc}+H_{t-1}W_{hc}+b_c)\]</span>

							<li><strong>记忆元</strong></li>
							<p>记忆元<span class = "math inline">\(\tilde{\mathcal{C}}_t\)</span>是LSTM的核心，结合了前一时刻的记忆（经过遗忘门筛选）和当前时刻的候选记忆（经过输入门筛选），从而产生更新后的记忆。对比GRU的隐状态不同的是：1.
								<span class = "math inline">\(\tilde{\mathcal{C}}_t\)</span>的值没有范围限制；2.控制过去记忆的参数与候选记忆的参数相互独立，而GRU的是此消彼长的关系；3.
								<span class = "math inline">\(\tilde{\mathcal{C}}_t\)</span>独立于
								<span class = "math inline">\(\tilde{\mathcal{H}}_t\)</span>进行传递，即LSTM有
								<span class = "math inline">\(\tilde{\mathcal{C}}_t\)</span>作为长期记忆不断累积，
								<span class = "math inline">\(\tilde{\mathcal{H}}_t\)</span>负责短期更新，而GRU只不断选择性更新
								<span class = "math inline">\(\tilde{\mathcal{H}}_t\)</span>。</p>
								<span class = "math inline">\[C_t=F_t\odot C_{t-1}+I_t\odot\tilde{C}_t\]</span>

							<li><strong>隐藏状态<span class = "math inline">\(H_t\)</span></strong></li>
							<p>最后，输出门决定当前时刻要输出多少隐藏状态。输出门的值会控制隐藏状态的输出比例，让模型可以更灵活地选择输出多少信息。同时，当前隐藏状态
								<span class = "math inline">\(\tilde{\mathcal{H}}_t\)</span>是经过过滤的记忆内容，用于传递给下一个单元。</p>
							<span class = "math inline">\[H_t=0_t\odot\tanh(C_t)\]</span>
							<p>公式中的tanh函数确保<span class = "math inline">\(H_t\)</span>始终在（-1,1）之间。</p>
							<p>过这些机制，LSTM可以对每一时刻的输入进行灵活处理，决定哪些信息该记住、哪些该忘记，并将重要信息传递到后续的时刻，因而能够更有效地捕捉长时依赖关系，其图形化演示如下：</p>
							<image class="educationImage" src="resources/educationimages/newRNN_2.png"  alt="newRNN_2" ></image>
							<image class="educationImage" src="resources/educationimages/newRNN_3.png"  alt="newRNN_3" ></image>
						</ol>

						<div class="educationSection">（四）深度循环神经网络</div>
						<p>单层RNN在建模复杂的序列关系时可能不够灵活。通过增加RNN的层数，我们可以构建“深度”RNN，逐层捕捉数据中不同层次的模式。这种深层架构可以提取出更复杂的序列特征，显著提升模型的表达能力，适应更多类型的任务需求，但由于深度结构引入了更多的参数，训练过程更容易出现梯度消失或梯度爆炸问题，因此通常需要配合梯度裁剪、正则化等技术。</p>
						<p>深度RNN包含多层RNN单元，每一层的输出作为下一层的输入，从而形成更深的网络结构。随着层数的增加，模型的容量也会增加，使其能够更好地表示复杂的时序模式和长时依赖关系。底层RNN层通常捕捉较低级的特征（例如基本的时序模式），而随着网络层次的加深，逐层的高层RNN可以学习更加复杂的模式或长时依赖关系，其图形化显示如下：</p>
						<image class="educationImage" src="resources/educationimages/newRNN_4.png"  alt="newRNN_4" ></image>
						<p>形象地说，<span class = "math inline">\(H_t^{(l)}\)</span>同时受到<span class = "math inline">\(H_t-1^{(l)}\)</span>和<span class = "math inline">\(H_t^{(l-1)}\)</span>的影响。</p>
						<p>在深度RNN中，每一时刻t的输入不仅通过第一层RNN进行处理，得到的隐藏状态还会传递到下一层。假设深度RNN包含L层，记第l层的计算公式为：</p>
						<span class = "math inline">\[H_t^{(1)}=\sigma\left(W_{xh}^{(1)}\cdot H_t^{(1-1)}+W\ln h^{(1)}\cdot H_{t-1}^{(1)}+b^{(1)}\right)\]</span>

						<div class="educationSection">（五）双向循环神经网络</div>
						<p>在很多序列任务中，不仅需要考虑过去的信息，有时还需考虑未来的上下文。例如，在命名实体识别等任务中，一个词的含义可能受到前后文的影响。双向RNN通过正向和反向传播同时处理序列，可以更全面地捕捉上下文信息，提高模型的整体表现。</p>
						<p>在双向RNN中，每一时刻的隐藏状态包括前向隐藏状态和后向隐藏状态，两者的输出会被合并，作为该时刻的输出，其图形化显示如下：</p>
						<image class="educationImage" src="resources/educationimages/newRNN_5.png"  alt="newRNN_5" ></image>
						<p>假设输入序列为<span class = "math inline">\(\mathbf{X=\{x1,x2,\cdots,xT\}}\)</span>，则双向RNN的计算过程如下：</p>
						<ol>
							<li><strong>前向隐藏状态：</strong>沿正序方向，从前到后计算隐藏状态。</li>
							<span class = "math inline">\[\overrightarrow{\mathrm{h}_t}=f\left(W^{(f)}x_t+U^{(f)}\overrightarrow{\mathrm{h}_{t-1}}+b^{(f)}\right)\]</span>
							<li><strong>后向隐藏状态：</strong>沿反向方向，从后到前计算隐藏状态。</li>
							<span class = "math inline">\[\overset{\leftarrow}{\operatorname*{h}_{t}}=f\left(W^{(b)}x_{t}+U^{(b)}\overset{\leftarrow}{\operatorname*{h}_{t+1}}+b^{(b)}\right)\]</span>

							<li><strong>合并前向和后向隐藏状态：</strong>将前向和后向的隐藏状态合并，通常是将它们连接起来作为最终的输出。</li>
							<span class = "math inline">\[\mathbf{h}_t=\begin{bmatrix}\to\leftarrow\\\mathbf{h}_t;\mathbf{h}_t\end{bmatrix}\]</span>

							<li><strong>其中：</strong></li>
							<p><span class = "math inline">\(\mathrm{W(f)}\)</span>和<span class = "math inline">\(\mathrm{W(b)}\)</span>分别是前向和后向的输入权重矩阵。</p>

							<p><span class = "math inline">\(\mathrm{U(f)}\)</span>和<span class = "math inline">\(\mathrm{U(b)}\)</span>分别是前向和后向的递归权重矩阵。</p>
							<p><span class = "math inline">\(\mathrm{b(f)}\)</span>和<span class = "math inline">\(\mathrm{b(b)}\)</span>是偏置项。</p>
							<p>f是激活函数，通常为tanh或ReLU。</p>

						</ol>
					

					</div>
				</div>

			<div id="educationTransformer">
				    <div class="educationTitle">注意力机制（Transformer）</div>
					<div class="educationContent">
						<p>在过去几年中，Transformer 模型已经成为高级深度学习和深度神经网络领域的热门话题。自从其在 2017 年被引入以来，Transformer 深度学习模型架构已经在几乎所有可能的领域中得到了广泛应用和演进。该模型不仅在自然语言处理任务中表现出色，还对于其他领域，尤其是时间序列预测方面，也具有巨大的帮助和潜力。</p>
						<p>Transformer 模型是一种深度学习架构，自 2017 年推出以来，彻底改变了自然语言处理 (NLP) 领域。该模型由 Vaswani 等人提出，并已成为 NLP 界最具影响力的模型之一。</p>
						<p>通常而言，传统的顺序模型（例如循环神经网络 (RNN)）在捕获远程依赖性和实现并行计算方面存在局限性。为了解决这些问题，Transformer 模型引入了自注意力机制，通过广泛使用该机制，模型能够在生成输出时权衡输入序列中不同位置的重要性。</p>
			            <p>Transformer 模型通过自注意力机制和并行计算的优势，能够更好地处理长距离依赖关系，提高了模型的训练和推理效率。它在机器翻译、文本摘要、问答系统等多个 NLP 任务中取得了显著的性能提升。</p>
						<p>除此之外，Transformer 模型的突破性表现使得它成为现代 NLP 研究和应用中的重要组成部分。它能够捕捉复杂的语义关系和上下文信息，极大地推动了自然语言处理的发展。</p>
						<div class="educationSection">Transformer 模型历史发展</div>
						<p>Transformer 在神经网络中的历史可以追溯到20世纪90年代初，当时 Jürgen Schmidhuber 提出了第一个 Transformer 模型的概念。这个模型被称为"快速权重控制器"，它采用了自注意力机制来学习句子中单词之间的关系。然而，尽管这个早期的 Transformer 模型在概念上是先进的，但由于其效率较低，它并未得到广泛的应用。</p>
						<p>随着时间的推移和深度学习技术的发展，Transformer 在2017年的一篇开创性论文中被正式引入，并取得了巨大的成功。通过引入自注意力机制和位置编码层，有效地捕捉输入序列中的长距离依赖关系，并且在处理长序列时表现出色。此外，Transformer 模型的并行化计算能力也使得训练速度更快，推动了深度学习在自然语言处理领域的重大突破，如机器翻译任务中的BERT（Bidirectional Encoder Representations from Transformers）模型等。</p>
						<p>因此，尽管早期的"快速权重控制器"并未受到广泛应用，但通过 Vaswani 等人的论文，Transformer 模型得到了重新定义和改进，成为现代深度学习的前沿技术之一，并在自然语言处理等领域取得了令人瞩目的成就。</p>
			    	    <p> Transformer 之所以如此成功，是因为它能够学习句子中单词之间的长距离依赖关系，这对于许多自然语言处理（NLP）任务至关重要，因为它允许模型理解单词在句子中的上下文。Transformer 利用自注意力机制来实现这一点，该机制使得模型在解码输出标记时能够聚焦于句子中最相关的单词。</p>
						<p>Transformer 对 NLP 领域产生了重大影响。它现在被广泛应用于许多 NLP 任务，并且不断进行改进。未来，Transformer 很可能被用于解决更广泛的 NLP 任务，并且它们将变得更加高效和强大。</p>
						<p>有关神经网络 Transformer 历史上的一些关键发展事件，我们可参考如下所示：</p>
						<li>1、1990年：Jürgen Schmidhuber 提出了第一个 Transformer 模型，即"快速权重控制器"。</li>
						<li>2、2017年：Vaswani 等人发表了论文《Attention is All You Need》，介绍了 Transformer 模型的核心思想。</li>
						<li>3、2018年：Transformer 模型在各种 NLP 任务中取得了最先进的结果，包括机器翻译、文本摘要和问答等。</li>
						<li>4、2019年：Transformer 被用于创建大型语言模型（LLM），例如 BERT 和 GPT-2，这些模型在各种 NLP 任务中取得了重要突破。</li>
						<li> 5、2020年：Transformer 继续被用于创建更强大的模型，例如 GPT-3，它在自然语言生成和理解方面取得了惊人的成果。</li>
						<p>总的来说，Transformer 模型的引入对于 NLP 领域产生了革命性的影响。它的能力在于学习长距离依赖关系并理解上下文，使得它成为众多 NLP 任务的首选方法，并为未来的发展提供了广阔的可能性。</p>
						<div class="educationSection">Transformer 模型通用架构设计</div>
						<p>Transformer 架构是从 RNN（循环神经网络）的编码器-解码器架构中汲取灵感而来，其引入了注意力机制。它被广泛应用于序列到序列（seq2seq）任务，并且相比于 RNN， Transformer 摒弃了顺序处理的方式。</p>
						<p>不同于 RNN，Transformer 以并行化的方式处理数据，从而实现更大规模的并行计算和更快速的训练。这得益于 Transformer 架构中的自注意力机制，它使得模型能够同时考虑输入序列中的所有位置，而无需按顺序逐步处理。自注意力机制允许模型根据输入序列中的不同位置之间的关系，对每个位置进行加权处理，从而捕捉全局上下文信息。</p>
						<image class="educationImage" src="resources/educationimages/transformer_1.png"  alt="transformer_1" ></image>
						<p>基于如上的 Transformer 深度学习模型的整体架构参考模型图，我们可以看到：它由两个主要组件组成：</p>
						<li><strong> 1、编码器堆栈 </strong></li>
						<p>这是由 Nx 个相同的编码器层组成的堆栈（在原始论文中，Nx=6）。每个编码器层都由两个子层组成：多头自注意力机制和前馈神经网络。多头自注意力机制用于对输入序列中的不同位置之间的关系进行建模，而前馈神经网络则用于对每个位置进行非线性转换。编码器堆栈的作用是将输入序列转换为一系列高级特征表示。</p>
						<li><strong> 2、解码器堆栈 </strong></li>
						<p>这也是由 Nx 个相同的解码器层组成的堆栈（在原始论文中，Nx=6）。每个解码器层除了包含编码器层的两个子层外，还包含一个额外的多头自注意力机制子层。这个额外的自注意力机制用于对编码器堆栈的输出进行关注，并帮助解码器对输入序列中的信息进行解码和生成输出序列。</p>
						<p>在编码器和解码器堆栈之间，还有一个位置编码层。这个位置编码层的作用是利用序列的顺序信息，为输入序列中的每个位置提供一个固定的编码表示。这样，模型可以在没有递归或卷积操作的情况下，利用位置编码层来处理序列的顺序信息。</p>
						<div class="educationSection">什么是 Transformer 神经网络？</div>
						<p>众所周知，Transformer 在处理文本序列、基因组序列、声音和时间序列数据等神经网络设计中起着关键作用。其中，自然语言处理是 Transformer 神经网络最常见的应用领域。</p>
						<p>当给定一个向量序列时，Transformer 神经网络会对这些向量进行编码，并将其解码回原始形式。而 Transformer 的注意力机制则是其不可或缺的核心组成部分。注意力机制表明了在输入序列中，对于给定标记的编码，其周围其他标记的上下文信息的重要性。</p>
					    <P>打个比方，在机器翻译模型中，注意力机制使得 Transformer 能够根据所有相关单词的上下文，将英语中的"it"正确翻译为法语或西班牙语中的性别对应的词汇。 Transformers 能够利用注意力机制来确定如何翻译当前单词，同时考虑其周围单词的影响。</P>
						<p>然而，需要注意的是，Transformer 神经网络取代了早期的循环神经网络（RNN）、长短期记忆（LSTM）和门控循环单元（GRU）等模型，成为了更为先进和有效的选择。</p>
						<p>通常而言，Transformer 神经网络接受输入句子并将其编码为两个不同的序列：</p>
						<li><strong>1、词向量嵌入序列</strong></li>
						<p>词向量嵌入是文本的数字表示形式。在这种情况下，神经网络只能处理转换为嵌入表示的单词。字典中的单词在嵌入表示中表示为向量。</p>
						<li><strong>2、位置编码器序列</strong></li>
						<p>位置编码器将原始文本中单词的位置表示为向量。Transformer 将词向量嵌入和位置编码结合起来。然后，它将组合结果发送到各个编码器，然后是解码器。</p>
						<p>与 RNN 和 LSTM 按顺序提供输入不同，Transformer 同时提供输入。每个编码器将其输入转换为另一个向量序列，称为编码。</p>
						<p>解码器以相反的顺序工作。它将编码转换回概率，并根据概率生成输出单词。通过使用 softmax 函数，Transformer 可以根据输出概率生成句子。</p>
						<p>每个解码器和编码器中都有一个称为注意力机制的组件。它允许一个输入单词使用其他单词的相关信息进行处理，同时屏蔽不包含相关信息的单词。</p>
						<p>为了充分利用 GPU 提供的并行计算能力，Transformer 使用多头注意力机制进行并行实现。多头注意力机制允许同时处理多个注意力机制，从而提高计算效率。</p>
						<p>相比于 LSTM 和 RNN，Transformer 深度学习模型的优势之一是能够同时处理多个单词。这得益于 Transformer 的并行计算能力，使得它能够更高效地处理序列数据。</p>
			</div>
			</div>

			<div class="educationTitle">激活函数</div>
                        <p>激活函数为神经网络引入了非线性，使其能够学习并模拟复杂的数据模式。没有它，神经网络就只是一堆线性回归的堆叠。</p>
			<div id="educationReLU">
				 <div class="educationTitle">ReLU-线性整流函数</div>
				 <div class="educationContent">
					    <p><strong>公式：</strong>f(x) = max(0, x)</p>
					    <p><strong>特点：</strong>目前最常用的激活函数。计算简单，能有效缓解梯度消失问题。</p>
						<p><strong>缺点：</strong>在输入为负数时，梯度为零，可能导致“神经元死亡”。</p>
				 </div>
			</div>

			<div id="educationSigmoid">
				 <div class="educationTitle">Sigmoid</div>
				 <div class="educationContent">
					<p><strong>公式：</strong>f(x) = 1 / (1 + e^(-x))</p>
					<p><strong>特点：</strong>将输入压缩到(0, 1) 区间。输出可以解释为概率。</p>
					<p><strong>用途：</strong>常用于二分类问题的输出层。</p>
					<p><strong>缺点：</strong>两端饱和区容易导致梯度消失；输出不是零中心的。</p>
				 </div>
			</div>

			<div id="educationTanh">
				 <div class="educationTitle">Tanh-双曲正切函数</div>
				 <div class="educationContent">
					<p><strong>公式：</strong>f(x) = (e^x - e^(-x)) / (e^x + e^(-x))</p>
					<p><strong>特点：</strong>将输入压缩到(-1, 1) 区间。是零中心的。</p>
					<p><strong>用途：</strong>常通常比Sigmoid表现更好，尤其在隐藏层中。</p>
					<p><strong>缺点：</strong>两端饱和区容易导致梯度消失；输出不是零中心的。</p>
				 </div>
			</div>
			<div id="educationSoftmax">
				 <div class="educationTitle">Softmax</div>
				 <div class="educationContent">
					<p><strong>核心作用:</strong></p>
					<li>1.将任意实数值的分数转换为概率分布。</li>
					<li>2.放大分数间的差异，使得最大值在概率上更加突出。</li>
					<p><strong>公式：</strong></p>
					<p>对于一个包含K个类别的向量<span class="math inline">\(Z=[z_1,z_2,...,z_K]\)</span>Softmax的计算如下：</p>
					<p><span class="math inline">\[\sigma(z_i)=e^{z_i}/{\sum_{j=1}^{K}e^{z_{j}}}\]</span></p>
					<p><strong>结果解释：</strong></p>
					<li>每个经过 Softmax处理后的输出<span class="math inline">\(\sigma(z_i)\)</span>都是一个介于0和1之间的值。</li>
					<li>所有输出的总和为1</li>
					<li>因此，输出可以被解释为每个类别的概率。</li>
				 </div>
			</div>



			<div id="educationempty">
				 <div class="educationTitle">       </div>
				 <div class="educationContent">
					<p>    </p>
					<p>    </p>
					<p>    </p>
					<p>    </p>
					<p>    </p>
				 </div>
			</div>

		</div>

	<!-- 笔记栏 -->
	<div id="educationNoteSidebar" class="education-note-sidebar hidden" style="display: none;">
			<div class="note-sidebar-header">
				<span class="note-sidebar-title">笔记</span>
				<button id="noteSidebarToggle" class="note-sidebar-toggle" title="隐藏笔记栏">−</button>
			</div>
			<div id="educationNoteList" class="note-sidebar-content">
				<!-- 笔记列表将在这里动态生成 -->
			</div>
		</div>

		<!-- 选中文本操作按钮 -->
		<div id="educationSelectionToolbar" class="education-selection-toolbar" style="display: none;">
			<button id="highlightBtn" class="selection-btn highlight-btn" title="高亮">高亮</button>
			<button id="noteBtn" class="selection-btn note-btn" title="做笔记">做笔记</button>
		</div>

		<!-- 笔记编辑对话框 -->
		<div id="noteEditDialog" class="note-edit-dialog" style="display: none;">
			<div class="note-edit-content">
				<div class="note-edit-header">
					<span>添加笔记</span>
					<button id="noteEditClose" class="note-edit-close">×</button>
				</div>
				<div class="note-edit-body">
					<textarea id="noteEditTextarea" class="note-edit-textarea" placeholder="输入你的笔记..."></textarea>
				</div>
				<div class="note-edit-footer">
					<button id="noteEditCancel" class="note-edit-btn cancel-btn">取消</button>
					<button id="noteEditSave" class="note-edit-btn save-btn">保存</button>
				</div>
			</div>
		</div>

		<!-- 隐藏笔记栏的按钮（当笔记栏隐藏时显示） -->
		<button id="noteSidebarShowBtn" class="note-sidebar-show-btn" style="display: none;" title="显示笔记栏">笔记</button>

		<div id = 'exerciseTab' style="display: none">
			<div class="exercise-layout">
				<!-- 左侧导航栏 -->
				<div class="exercise-left-nav">
					<div class="nav-header">
						<h3>练习模块</h3>
					</div>
					<div class="nav-content">
						<!-- 基础组件模块 -->
						<div class="nav-category">
							<div class="nav-category-title" data-category="basic">
								<span class="category-number">1</span>
								<span class="category-name">基础组件模块</span>
								<span class="category-count">(50题)</span>
							</div>
							<div class="nav-subcategory" data-subcategory="neural-layers">
								<span class="subcategory-name">神经网络层</span>
							</div>
							<div class="nav-subcategory" data-subcategory="activations">
								<span class="subcategory-name">激活函数</span>
							</div>
						</div>

						<!-- 核心架构模块 -->
						<div class="nav-category">
							<div class="nav-category-title" data-category="core">
								<span class="category-number">2</span>
								<span class="category-name">核心架构模块</span>
								<span class="category-count">(50题)</span>
							</div>
							<div class="nav-subcategory" data-subcategory="perceptron">
								<span class="subcategory-name">感知机</span>
							</div>
							<div class="nav-subcategory" data-subcategory="cnn">
								<span class="subcategory-name">CNN</span>
							</div>
							<div class="nav-subcategory" data-subcategory="rnn">
								<span class="subcategory-name">RNN</span>
							</div>
							<div class="nav-subcategory" data-subcategory="transformer">
								<span class="subcategory-name">Transformer</span>
							</div>
						</div>

						<!-- 综合应用模块 -->
						<div class="nav-category">
							<div class="nav-category-title" data-category="comprehensive">
								<span class="category-number">3</span>
								<span class="category-name">综合应用模块</span>
								<span class="category-count">(55题)</span>
							</div>
							<div class="nav-subcategory" data-subcategory="arch-selection">
								<span class="subcategory-name">架构选择与对比</span>
							</div>
							<div class="nav-subcategory" data-subcategory="problem-diagnosis">
								<span class="subcategory-name">问题诊断与调优</span>
							</div>
							<div class="nav-subcategory" data-subcategory="advanced">
								<span class="subcategory-name">前沿与交叉领域</span>
							</div>
						</div>
					</div>
				</div>

				<!-- 中间题目区域 -->
				<div class="exercise-center-content" style="overflow-y: auto; max-height: 100vh;">
					<div class="content-header">
						<h2 class="exercise-main-title">深度学习综合练习</h2>
						<p class="exercise-main-desc">
							点击左侧导航栏的题目类型开始练习。每道题目都有详细解答，帮助您巩固深度学习知识。
						</p>
					</div>
					
					<!-- 题目内容区域，仿照教学页面格式 -->
					<div id="exerciseQuestionsTab" class="exercise-questions-tab">
						<!-- 基础组件：神经网络层（1-30题） -->
						<div id="exerciseNeuralLayers" style="display: block;">
							<div class="exercise-section-title">
								<div class="title-header">
									<span class="title-tag">基础组件模块</span>
									<span class="title-icon">🧩</span>
								</div>
								<h2 class="title-name">神经网络层</h2>
								<div class="title-info">
									<span class="info-item">
										<i class="icon-question"></i>
										30道题目
									</span>
									<span class="info-item">
										<i class="icon-clock"></i>
										题号：1-30
									</span>
								</div>
							</div>
							<div class="exercise-section-content">
								<div class="exercise-card" data-exercise="neural_layers_quiz">
									<div class="card-content">
										<p>检测你对神经网络层的理解，包括全连接层、卷积层、池化层等基础组件。</p>
										<button class="start-exercise-btn">开始练习</button>
									</div>
									<div class="card-questions" style="display:none;">
										<!-- 题目 1 -->
										<div class="question">
											<p><strong>1. 全连接层（Dense）的主要功能是什么？</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q1" value="A"> A. 提取图像局部特征</label>
												<label><input type="radio" name="neural_layers_q1" value="B"> B. 学习输入特征之间的全局模式</label>
												<label><input type="radio" name="neural_layers_q1" value="C"> C. 降低数据维度防止过拟合</label>
												<label><input type="radio" name="neural_layers_q1" value="D"> D. 标准化输入数据</label>
											</div>
										</div>
										<!-- 题目 2 -->
										<div class="question">
											<p><strong>2. 卷积层（Convolution）最擅长处理哪种类型的数据？</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q2" value="A"> A. 时间序列数据</label>
												<label><input type="radio" name="neural_layers_q2" value="B"> B. 结构化表格数据</label>
												<label><input type="radio" name="neural_layers_q2" value="C"> C. 网格状数据（如图像、语音频谱图）</label>
												<label><input type="radio" name="neural_layers_q2" value="D"> D. 文本数据</label>
											</div>
										</div>
										<!-- 题目 3 -->
										<div class="question">
											<p><strong>3. 最大池化层（Max Pooling）的主要作用是？</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q3" value="A"> A. 增加特征图的通道数</label>
												<label><input type="radio" name="neural_layers_q3" value="B"> B. 提取更精细的局部特征</label>
												<label><input type="radio" name="neural_layers_q3" value="C"> C. 下采样并保持特征的平移不变性</label>
												<label><input type="radio" name="neural_layers_q3" value="D"> D. 引入非线性变换</label>
											</div>
										</div>
										<!-- 题目 4 -->
										<div class="question">
											<p><strong>4. Batch Normalization通常被放置在什么位置？</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q4" value="A"> A. 激活函数之后，卷积层之前</label>
												<label><input type="radio" name="neural_layers_q4" value="B"> B. 卷积层或全连接层之后，激活函数之前</label>
												<label><input type="radio" name="neural_layers_q4" value="C"> C. 只在网络的最开始</label>
												<label><input type="radio" name="neural_layers_q4" value="D"> D. 只在网络的最后输出层</label>
											</div>
										</div>
										<!-- 题目 5 -->
										<div class="question">
											<p><strong>5. Dropout层在训练和测试阶段的行为分别是？</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q5" value="A"> A. 训练时丢弃部分神经元，测试时也丢弃</label>
												<label><input type="radio" name="neural_layers_q5" value="B"> B. 训练时不丢弃，测试时丢弃</label>
												<label><input type="radio" name="neural_layers_q5" value="C"> C. 训练时丢弃部分神经元，测试时使用全部神经元</label>
												<label><input type="radio" name="neural_layers_q5" value="D"> D. 训练和测试都使用全部神经元</label>
											</div>
										</div>
										<!-- 题目 6 -->
										<div class="question">
											<p><strong>6. 当需要将卷积神经网络的特征图转换为一维向量以输入全连接层时，应使用：</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q6" value="A"> A. Concatenate层</label>
												<label><input type="radio" name="neural_layers_q6" value="B"> B. Add层</label>
												<label><input type="radio" name="neural_layers_q6" value="C"> C. Flatten层</label>
												<label><input type="radio" name="neural_layers_q6" value="D"> D. Dropout层</label>
											</div>
										</div>
										<!-- 题目 7 -->
										<div class="question">
											<p><strong>7. Concatenate层的主要功能是？</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q7" value="A"> A. 将多个张量逐元素相加</label>
												<label><input type="radio" name="neural_layers_q7" value="B"> B. 将多个张量沿指定轴连接合并</label>
												<label><input type="radio" name="neural_layers_q7" value="C"> C. 将张量从多维压平到一维</label>
												<label><input type="radio" name="neural_layers_q7" value="D"> D. 标准化张量的分布</label>
											</div>
										</div>
										<!-- 题目 8 -->
										<div class="question">
											<p><strong>8. Add层要求输入的多个张量必须：</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q8" value="A"> A. 形状完全相同</label>
												<label><input type="radio" name="neural_layers_q8" value="B"> B. 通道数相同即可</label>
												<label><input type="radio" name="neural_layers_q8" value="C"> C. 批次大小相同即可</label>
												<label><input type="radio" name="neural_layers_q8" value="D"> D. 没有任何限制</label>
											</div>
										</div>
										<!-- 题目 9 -->
										<div class="question">
											<p><strong>9. 残差网络（ResNet）中的跳跃连接（Skip Connection）主要是通过什么层实现的？</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q9" value="A"> A. Concatenate层</label>
												<label><input type="radio" name="neural_layers_q9" value="B"> B. Add层</label>
												<label><input type="radio" name="neural_layers_q9" value="C"> C. Multiply层</label>
												<label><input type="radio" name="neural_layers_q9" value="D"> D. Flatten层</label>
											</div>
										</div>
										<!-- 题目 10 -->
										<div class="question">
											<p><strong>10. 以下哪个层的主要目的是减少过拟合？</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q10" value="A"> A. Convolution和Max Pooling</label>
												<label><input type="radio" name="neural_layers_q10" value="B"> B. Dense和Flatten</label>
												<label><input type="radio" name="neural_layers_q10" value="C"> C. Dropout和Batch Normalization</label>
												<label><input type="radio" name="neural_layers_q10" value="D"> D. Add和Concatenate</label>
											</div>
										</div>
										<!-- 题目 11 -->
										<div class="question">
											<p><strong>11. 在CNN中，通常紧跟在卷积层之后的是？</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q11" value="A"> A. 另一个卷积层</label>
												<label><input type="radio" name="neural_layers_q11" value="B"> B. 池化层（Pooling）</label>
												<label><input type="radio" name="neural_layers_q11" value="C"> C. 全连接层</label>
												<label><input type="radio" name="neural_layers_q11" value="D"> D. Flatten层</label>
											</div>
										</div>
										<!-- 题目 12 -->
										<div class="question">
											<p><strong>12. 哪种层可以帮助解决深层神经网络中的"内部协变量偏移"问题？</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q12" value="A"> A. Dropout</label>
												<label><input type="radio" name="neural_layers_q12" value="B"> B. Batch Normalization</label>
												<label><input type="radio" name="neural_layers_q12" value="C"> C. Max Pooling</label>
												<label><input type="radio" name="neural_layers_q12" value="D"> D. Convolution</label>
											</div>
										</div>
										<!-- 题目 13 -->
										<div class="question">
											<p><strong>13. 如果想要构建一个多分支的网络结构（如Inception模块），将不同分支的特征融合，最应该使用：</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q13" value="A"> A. Add层</label>
												<label><input type="radio" name="neural_layers_q13" value="B"> B. Multiply层</label>
												<label><input type="radio" name="neural_layers_q13" value="C"> C. Concatenate层</label>
												<label><input type="radio" name="neural_layers_q13" value="D"> D. Flatten层</label>
											</div>
										</div>
										<!-- 题目 14 -->
										<div class="question">
											<p><strong>14. 对于一个二分类任务，最后一层应该使用什么层？</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q14" value="A"> A. 一个神经元的Dense层 + Sigmoid激活</label>
												<label><input type="radio" name="neural_layers_q14" value="B"> B. 两个神经元的Dense层 + Softmax激活</label>
												<label><input type="radio" name="neural_layers_q14" value="C"> C. 卷积层 + ReLU激活</label>
												<label><input type="radio" name="neural_layers_q14" value="D"> D. Dropout层</label>
											</div>
										</div>
										<!-- 题目 15 -->
										<div class="question">
											<p><strong>15. Flatten层会改变以下哪个维度？</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q15" value="A"> A. 批次大小（Batch Size）</label>
												<label><input type="radio" name="neural_layers_q15" value="B"> B. 通道数（Channels）</label>
												<label><input type="radio" name="neural_layers_q15" value="C"> C. 空间维度（Height, Width）</label>
												<label><input type="radio" name="neural_layers_q15" value="D"> D. 不会改变任何维度，只是重新排列</label>
											</div>
										</div>
										<!-- 题目 16 -->
										<div class="question">
											<p><strong>16. 在训练初期，Batch Normalization允许使用更大的学习率，主要是因为：</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q16" value="A"> A. 它增加了网络的复杂度</label>
												<label><input type="radio" name="neural_layers_q16" value="B"> B. 它减少了梯度消失的风险</label>
												<label><input type="radio" name="neural_layers_q16" value="C"> C. 它标准化了层输入，使优化地形更平滑</label>
												<label><input type="radio" name="neural_layers_q16" value="D"> D. 它引入了随机性</label>
											</div>
										</div>
										<!-- 题目 17 -->
										<div class="question">
											<p><strong>17. Dropout的保留概率（如0.5）表示：</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q17" value="A"> A. 训练时有50%的神经元被永久删除</label>
												<label><input type="radio" name="neural_layers_q17" value="B"> B. 训练时每个神经元有50%的概率被临时置零</label>
												<label><input type="radio" name="neural_layers_q17" value="C"> C. 测试时有50%的神经元被使用</label>
												<label><input type="radio" name="neural_layers_q17" value="D"> D. 学习率降低50%</label>
											</div>
										</div>
										<!-- 题目 18 -->
										<div class="question">
											<p><strong>18. 哪个层没有需要训练的参数？</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q18" value="A"> A. Dense层</label>
												<label><input type="radio" name="neural_layers_q18" value="B"> B. Convolution层</label>
												<label><input type="radio" name="neural_layers_q18" value="C"> C. Max Pooling层</label>
												<label><input type="radio" name="neural_layers_q18" value="D"> D. Batch Normalization层</label>
											</div>
										</div>
										<!-- 题目 19 -->
										<div class="question">
											<p><strong>19. 如果你想减少特征图的空间尺寸但保留最显著的特征，应该使用：</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q19" value="A"> A. 卷积层（stride=2）</label>
												<label><input type="radio" name="neural_layers_q19" value="B"> B. 最大池化层</label>
												<label><input type="radio" name="neural_layers_q19" value="C"> C. Dropout层</label>
												<label><input type="radio" name="neural_layers_q19" value="D"> D. Flatten层</label>
											</div>
										</div>
										<!-- 题目 20 -->
										<div class="question">
											<p><strong>20. Concatenate和Add层的主要区别在于：</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q20" value="A"> A. Concatenate增加输出维度，Add不增加</label>
												<label><input type="radio" name="neural_layers_q20" value="B"> B. Add增加输出维度，Concatenate不增加</label>
												<label><input type="radio" name="neural_layers_q20" value="C"> C. 两者都增加输出维度</label>
												<label><input type="radio" name="neural_layers_q20" value="D"> D. 两者都不增加输出维度</label>
											</div>
										</div>
										<!-- 题目 21 -->
										<div class="question">
											<p><strong>21. 在图像分割任务中，经常使用"编码器-解码器"结构，解码器中常用什么层来增大特征图尺寸？</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q21" value="A"> A. 最大池化层（Max Pooling）</label>
												<label><input type="radio" name="neural_layers_q21" value="B"> B. 转置卷积层（Transposed Convolution）</label>
												<label><input type="radio" name="neural_layers_q21" value="C"> C. Dropout层</label>
												<label><input type="radio" name="neural_layers_q21" value="D"> D. Flatten层</label>
											</div>
										</div>
										<!-- 题目 22 -->
										<div class="question">
											<p><strong>22. 如果一个网络的训练误差很低但验证误差很高，首先应该考虑增加什么层？</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q22" value="A"> A. 更多的Dense层</label>
												<label><input type="radio" name="neural_layers_q22" value="B"> B. Dropout层</label>
												<label><input type="radio" name="neural_layers_q22" value="C"> C. Batch Normalization层</label>
												<label><input type="radio" name="neural_layers_q22" value="D"> D. Convolution层</label>
											</div>
										</div>
										<!-- 题目 23 -->
										<div class="question">
											<p><strong>23. Batch Normalization在测试阶段使用的均值和方差是：</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q23" value="A"> A. 当前测试批次的均值和方差</label>
												<label><input type="radio" name="neural_layers_q23" value="B"> B. 训练阶段通过移动平均计算得到的全局均值和方差</label>
												<label><input type="radio" name="neural_layers_q23" value="C"> C. 固定的0和1</label>
												<label><input type="radio" name="neural_layers_q23" value="D"> D. 随机初始化的值</label>
											</div>
										</div>
										<!-- 题目 24 -->
										<div class="question">
											<p><strong>24. 哪一对层通常被一起使用来构建CNN的基础特征提取模块？</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q24" value="A"> A. Dense和Dropout</label>
												<label><input type="radio" name="neural_layers_q24" value="B"> B. Convolution和Max Pooling</label>
												<label><input type="radio" name="neural_layers_q24" value="C"> C. Flatten和Dense</label>
												<label><input type="radio" name="neural_layers_q24" value="D"> D. Add和Concatenate</label>
											</div>
										</div>
										<!-- 题目 25 -->
										<div class="question">
											<p><strong>25. 在一维卷积中，通常用于处理什么类型的数据？</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q25" value="A"> A. 图像</label>
												<label><input type="radio" name="neural_layers_q25" value="B"> B. 时间序列或文本</label>
												<label><input type="radio" name="neural_layers_q25" value="C"> C. 视频</label>
												<label><input type="radio" name="neural_layers_q25" value="D"> D. 3D物体</label>
											</div>
										</div>
										<!-- 题目 26 -->
										<div class="question">
											<p><strong>26. 全局平均池化（Global Average Pooling）层有时可以替代什么层？</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q26" value="A"> A. 卷积层</label>
												<label><input type="radio" name="neural_layers_q26" value="B"> B. 全连接层</label>
												<label><input type="radio" name="neural_layers_q26" value="C"> C. Dropout层</label>
												<label><input type="radio" name="neural_layers_q26" value="D"> D. Flatten层</label>
											</div>
										</div>
										<!-- 题目 27 -->
										<div class="question">
											<p><strong>27. 哪个操作有助于增加模型的非线性表达能力？</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q27" value="A"> A. 使用多个线性层堆叠</label>
												<label><input type="radio" name="neural_layers_q27" value="B"> B. 在层之间添加激活函数</label>
												<label><input type="radio" name="neural_layers_q27" value="C"> C. 使用更大的批次大小</label>
												<label><input type="radio" name="neural_layers_q27" value="D"> D. 增加学习率</label>
											</div>
										</div>
										<!-- 题目 28 -->
										<div class="question">
											<p><strong>28. 如果输入图像尺寸为224x224x3，经过一个滤波器大小为3x3、步长为1、填充为"相同"的卷积层后，输出特征图尺寸为：</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q28" value="A"> A. 222x222</label>
												<label><input type="radio" name="neural_layers_q28" value="B"> B. 224x224</label>
												<label><input type="radio" name="neural_layers_q28" value="C"> C. 226x226</label>
												<label><input type="radio" name="neural_layers_q28" value="D"> D. 112x112</label>
											</div>
										</div>
										<!-- 题目 29 -->
										<div class="question">
											<p><strong>29. 深度可分离卷积（Depthwise Separable Convolution）与标准卷积相比，主要优点是：</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q29" value="A"> A. 提取更丰富的特征</label>
												<label><input type="radio" name="neural_layers_q29" value="B"> B. 极大地减少计算量和参数量</label>
												<label><input type="radio" name="neural_layers_q29" value="C"> C. 避免梯度消失</label>
												<label><input type="radio" name="neural_layers_q29" value="D"> D. 防止过拟合</label>
											</div>
										</div>
										<!-- 题目 30 -->
										<div class="question">
											<p><strong>30. 在网络中，早期的卷积层通常学习到的是：</strong></p>
											<div class="options">
												<label><input type="radio" name="neural_layers_q30" value="A"> A. 高级语义特征（如"狗头"、"车轮"）</label>
												<label><input type="radio" name="neural_layers_q30" value="B"> B. 低级视觉特征（如边缘、颜色、纹理）</label>
												<label><input type="radio" name="neural_layers_q30" value="C"> C. 全局上下文信息</label>
												<label><input type="radio" name="neural_layers_q30" value="D"> D. 与任务无关的噪声</label>
											</div>
										</div>
										<!-- 得分显示和提交按钮 -->
										<div class="score-display"></div>
										<button class="submit-quiz-btn" data-category="neural_layers_quiz">提交答案</button>
										<button class="close-exercise-btn">收起练习</button>
									</div>
								</div>
							</div>
						</div>
					
						<!-- 基础组件：激活函数（31-50题） -->
						<div id="exerciseActivations" style="display: block;">
							<div class="exercise-section-title">
								<div class="title-header">
									<span class="title-tag">基础组件模块</span>
									<span class="title-icon">⚡</span>
								</div>
								<h2 class="title-name">激活函数</h2>
								<div class="title-info">
									<span class="info-item">
										<i class="icon-question"></i>
										20道题目
									</span>
									<span class="info-item">
										<i class="icon-clock"></i>
										题号：31-50
									</span>
								</div>
							</div>
							<div class="exercise-section-content">
								<div class="exercise-card" data-exercise="activations_quiz">
									<div class="card-content">
										<p>基于"激活函数"章节，检查你对不同激活函数特性的理解。</p>
										<button class="start-exercise-btn">开始练习</button>
									</div>
									<div class="card-questions" style="display:none;">
										<!-- 题目 31 -->
										<div class="question">
											<p><strong>31. ReLU激活函数的数学表达式是？</strong></p>
											<div class="options">
												<label><input type="radio" name="activations_q31" value="A"> A. f(x) = 1 / (1 + e^(-x))</label>
												<label><input type="radio" name="activations_q31" value="B"> B. f(x) = max(0, x)</label>
												<label><input type="radio" name="activations_q31" value="C"> C. f(x) = (e^x - e^(-x)) / (e^x + e^(-x))</label>
												<label><input type="radio" name="activations_q31" value="D"> D. f(x) = x</label>
											</div>
										</div>
										<!-- 题目 32 -->
										<div class="question">
											<p><strong>32. 当输入为很大的负数时，ReLU的梯度是？</strong></p>
											<div class="options">
												<label><input type="radio" name="activations_q32" value="A"> A. 1</label>
												<label><input type="radio" name="activations_q32" value="B"> B. 0</label>
												<label><input type="radio" name="activations_q32" value="C"> C. -1</label>
												<label><input type="radio" name="activations_q32" value="D"> D. 无穷大</label>
											</div>
										</div>
										<!-- 题目 33 -->
										<div class="question">
											<p><strong>33. Sigmoid函数将输入映射到哪个区间？</strong></p>
											<div class="options">
												<label><input type="radio" name="activations_q33" value="A"> A. (0, 1)</label>
												<label><input type="radio" name="activations_q33" value="B"> B. (-1, 1)</label>
												<label><input type="radio" name="activations_q33" value="C"> C. (0, +∞)</label>
												<label><input type="radio" name="activations_q33" value="D"> D. (-∞, +∞)</label>
											</div>
										</div>
										<!-- 题目 34 -->
										<div class="question">
											<p><strong>34. 以下哪个激活函数是"零中心"的（输出均值为0）？</strong></p>
											<div class="options">
												<label><input type="radio" name="activations_q34" value="A"> A. ReLU</label>
												<label><input type="radio" name="activations_q34" value="B"> B. Sigmoid</label>
												<label><input type="radio" name="activations_q34" value="C"> C. Tanh</label>
												<label><input type="radio" name="activations_q34" value="D"> D. 都不是</label>
											</div>
										</div>
										<!-- 题目 35 -->
										<div class="question">
											<p><strong>35. 在二分类任务的输出层，最常用的激活函数是？</strong></p>
											<div class="options">
												<label><input type="radio" name="activations_q35" value="A"> A. ReLU</label>
												<label><input type="radio" name="activations_q35" value="B"> B. Tanh</label>
												<label><input type="radio" name="activations_q35" value="C"> C. Sigmoid</label>
												<label><input type="radio" name="activations_q35" value="D"> D. Softmax</label>
											</div>
										</div>
										<!-- 题目 36 -->
										<div class="question">
											<p><strong>36. 在多分类任务（如10个类别）的输出层，应该使用什么激活函数？</strong></p>
											<div class="options">
												<label><input type="radio" name="activations_q36" value="A"> A. ReLU</label>
												<label><input type="radio" name="activations_q36" value="B"> B. Sigmoid</label>
												<label><input type="radio" name="activations_q36" value="C"> C. Tanh</label>
												<label><input type="radio" name="activations_q36" value="D"> D. Softmax</label>
											</div>
										</div>
										<!-- 题目 37 -->
										<div class="question">
											<p><strong>37. 哪个激活函数存在"梯度饱和"问题，当输入绝对值很大时梯度接近于零？</strong></p>
											<div class="options">
												<label><input type="radio" name="activations_q37" value="A"> A. 仅Sigmoid</label>
												<label><input type="radio" name="activations_q37" value="B"> B. 仅Tanh</label>
												<label><input type="radio" name="activations_q37" value="C"> C. Sigmoid和Tanh</label>
												<label><input type="radio" name="activations_q37" value="D"> D. ReLU</label>
											</div>
										</div>
										<!-- 题目 38 -->
										<div class="question">
											<p><strong>38. ReLU激活函数相比Sigmoid和Tanh的一个主要优势是？</strong></p>
											<div class="options">
												<label><input type="radio" name="activations_q38" value="A"> A. 计算更复杂</label>
												<label><input type="radio" name="activations_q38" value="B"> B. 缓解了梯度消失问题（在正区间）</label>
												<label><input type="radio" name="activations_q38" value="C"> C. 输出范围更小</label>
												<label><input type="radio" name="activations_q38" value="D"> D. 是零中心的</label>
											</div>
										</div>
										<!-- 题目 39 -->
										<div class="question">
											<p><strong>39. Leaky ReLU是对标准ReLU的改进，主要解决了什么问题？</strong></p>
											<div class="options">
												<label><input type="radio" name="activations_q39" value="A"> A. 计算速度慢</label>
												<label><input type="radio" name="activations_q39" value="B"> B. 正区间的梯度爆炸</label>
												<label><input type="radio" name="activations_q39" value="C"> C. 负区间的神经元死亡</label>
												<label><input type="radio" name="activations_q39" value="D"> D. 输出范围太大</label>
											</div>
										</div>
										<!-- 题目 40 -->
										<div class="question">
											<p><strong>40. Softmax函数的输出之和总是等于：</strong></p>
											<div class="options">
												<label><input type="radio" name="activations_q40" value="A"> A. 0</label>
												<label><input type="radio" name="activations_q40" value="B"> B. 0.5</label>
												<label><input type="radio" name="activations_q40" value="C"> C. 1</label>
												<label><input type="radio" name="activations_q40" value="D"> D. 取决于输入</label>
											</div>
										</div>
										<!-- 题目 41 -->
										<div class="question">
											<p><strong>41. 在隐藏层中，目前最常用的激活函数是？</strong></p>
											<div class="options">
												<label><input type="radio" name="activations_q41" value="A"> A. Sigmoid</label>
												<label><input type="radio" name="activations_q41" value="B"> B. Tanh</label>
												<label><input type="radio" name="activations_q41" value="C"> C. ReLU及其变体</label>
												<label><input type="radio" name="activations_q41" value="D"> D. Softmax</label>
											</div>
										</div>
										<!-- 题目 42 -->
										<div class="question">
											<p><strong>42. 对于一个回归任务（预测一个连续值），输出层通常使用什么激活函数？</strong></p>
											<div class="options">
												<label><input type="radio" name="activations_q42" value="A"> A. Sigmoid</label>
												<label><input type="radio" name="activations_q42" value="B"> B. Tanh</label>
												<label><input type="radio" name="activations_q42" value="C"> C. ReLU</label>
												<label><input type="radio" name="activations_q42" value="D"> D. 线性激活（或无激活函数）</label>
											</div>
										</div>
										<!-- 题目 43 -->
										<div class="question">
											<p><strong>43. Tanh激活函数可以由Sigmoid函数变换得到，公式是？</strong></p>
											<div class="options">
												<label><input type="radio" name="activations_q43" value="A"> A. tanh(x) = sigmoid(x)</label>
												<label><input type="radio" name="activations_q43" value="B"> B. tanh(x) = 2 * sigmoid(x) - 1</label>
												<label><input type="radio" name="activations_q43" value="C"> C. tanh(x) = sigmoid(2x)</label>
												<label><input type="radio" name="activations_q43" value="D"> D. tanh(x) = sigmoid(x) / 2</label>
											</div>
										</div>
										<!-- 题目 44 -->
										<div class="question">
											<p><strong>44. 哪个激活函数对输入的缩放不敏感？即，输入乘以一个常数，输出分布形状不变？</strong></p>
											<div class="options">
												<label><input type="radio" name="activations_q44" value="A"> A. Sigmoid</label>
												<label><input type="radio" name="activations_q44" value="B"> B. Tanh</label>
												<label><input type="radio" name="activations_q44" value="C"> C. ReLU</label>
												<label><input type="radio" name="activations_q44" value="D"> D. 都不敏感</label>
											</div>
										</div>
										<!-- 题目 45 -->
										<div class="question">
											<p><strong>45. 在训练深度网络时，使用ReLU可能导致什么问题？</strong></p>
											<div class="options">
												<label><input type="radio" name="activations_q45" value="A"> A. 梯度爆炸</label>
												<label><input type="radio" name="activations_q45" value="B"> B. 神经元死亡（Dead Neurons）</label>
												<label><input type="radio" name="activations_q45" value="C"> C. 输出不是零中心</label>
												<label><input type="radio" name="activations_q45" value="D"> D. B和C</label>
											</div>
										</div>
										<!-- 题目 46 -->
										<div class="question">
											<p><strong>46. Sigmoid函数的导数最大值是多少？</strong></p>
											<div class="options">
												<label><input type="radio" name="activations_q46" value="A"> A. 0</label>
												<label><input type="radio" name="activations_q46" value="B"> B. 0.25</label>
												<label><input type="radio" name="activations_q46" value="C"> C. 0.5</label>
												<label><input type="radio" name="activations_q46" value="D"> D. 1</label>
											</div>
										</div>
										<!-- 题目 47 -->
										<div class="question">
											<p><strong>47. 当我们需要神经网络的输出表示一个概率分布（多标签分类）时，可以使用？</strong></p>
											<div class="options">
												<label><input type="radio" name="activations_q47" value="A"> A. 多个Sigmoid</label>
												<label><input type="radio" name="activations_q47" value="B"> B. 一个Softmax</label>
												<label><input type="radio" name="activations_q47" value="C"> C. Tanh</label>
												<label><input type="radio" name="activations_q47" value="D"> D. ReLU</label>
											</div>
										</div>
										<!-- 题目 48 -->
										<div class="question">
											<p><strong>48. ELU（指数线性单元）激活函数相比ReLU的主要改进是？</strong></p>
											<div class="options">
												<label><input type="radio" name="activations_q48" value="A"> A. 计算更快</label>
												<label><input type="radio" name="activations_q48" value="B"> B. 对负输入有非零输出，且平滑渐近</label>
												<label><input type="radio" name="activations_q48" value="C"> C. 输出范围更大</label>
												<label><input type="radio" name="activations_q48" value="D"> D. 解决了梯度爆炸</label>
											</div>
										</div>
										<!-- 题目 49 -->
										<div class="question">
											<p><strong>49. Softmax函数常用于什么类型的层之后？</strong></p>
											<div class="options">
												<label><input type="radio" name="activations_q49" value="A"> A. 卷积层</label>
												<label><input type="radio" name="activations_q49" value="B"> B. 池化层</label>
												<label><input type="radio" name="activations_q49" value="C"> C. 全连接层</label>
												<label><input type="radio" name="activations_q49" value="D"> D. Dropout层</label>
											</div>
										</div>
										<!-- 题目 50 -->
										<div class="question">
											<p><strong>50. 以下关于激活函数的说法，哪一项是错误的？</strong></p>
											<div class="options">
												<label><input type="radio" name="activations_q50" value="A"> A. 激活函数为神经网络引入了非线性</label>
												<label><input type="radio" name="activations_q50" value="B"> B. 没有激活函数，多层网络等价于单层线性模型</label>
												<label><input type="radio" name="activations_q50" value="C"> C. 所有的激活函数都是可微的</label>
												<label><input type="radio" name="activations_q50" value="D"> D. 激活函数的选择会影响训练的难易和速度</label>
											</div>
										</div>
										<!-- 得分显示和提交按钮 -->
										<div class="score-display"></div>
										<button class="submit-quiz-btn" data-category="activations_quiz">提交答案</button>
										<button class="close-exercise-btn">收起练习</button>
									</div>
								</div>
							</div>
						</div>
					
						<!-- 核心架构：感知机（51-60题） -->
						<div id="exercisePerceptron" style="display: block;">
							<div class="exercise-section-title">
								<div class="title-header">
									<span class="title-tag">核心架构模块</span>
									<span class="title-icon">🧠</span>
								</div>
								<h2 class="title-name">感知机</h2>
								<div class="title-info">
									<span class="info-item">
										<i class="icon-question"></i>
										10道题目
									</span>
									<span class="info-item">
										<i class="icon-clock"></i>
										题号：51-60
									</span>
								</div>
							</div>
							<div class="exercise-section-content">
								<div class="exercise-card" data-exercise="perceptron_quiz">
									<div class="card-content">
										<p>检测你对感知机基础概念和多层感知机工作原理的理解。</p>
										<button class="start-exercise-btn">开始练习</button>
									</div>
									<div class="card-questions" style="display:none;">
										<!-- 题目 51 -->
										<div class="question">
											<p><strong>51. 感知机（Perceptron）是哪种模型的雏形？</strong></p>
											<div class="options">
												<label><input type="radio" name="perceptron_q51" value="A"> A. 卷积神经网络</label>
												<label><input type="radio" name="perceptron_q51" value="B"> B. 循环神经网络</label>
												<label><input type="radio" name="perceptron_q51" value="C"> C. Transformer</label>
												<label><input type="radio" name="perceptron_q51" value="D"> D. 人工神经网络</label>
											</div>
										</div>
										<!-- 题目 52 -->
										<div class="question">
											<p><strong>52. 单层感知机无法解决什么问题？</strong></p>
											<div class="options">
												<label><input type="radio" name="perceptron_q52" value="A"> A. 线性回归</label>
												<label><input type="radio" name="perceptron_q52" value="B"> B. 线性分类</label>
												<label><input type="radio" name="perceptron_q52" value="C"> C. 逻辑与（AND）</label>
												<label><input type="radio" name="perceptron_q52" value="D"> D. 异或（XOR）</label>
											</div>
										</div>
										<!-- 题目 53 -->
										<div class="question">
											<p><strong>53. 多层感知机（MLP）克服单层感知机局限的关键是：</strong></p>
											<div class="options">
												<label><input type="radio" name="perceptron_q53" value="A"> A. 使用更多的输入特征</label>
												<label><input type="radio" name="perceptron_q53" value="B"> B. 引入隐藏层和非线性激活函数</label>
												<label><input type="radio" name="perceptron_q53" value="C"> C. 使用更小的学习率</label>
												<label><input type="radio" name="perceptron_q53" value="D"> D. 增加更多的输出神经元</label>
											</div>
										</div>
										<!-- 题目 54 -->
										<div class="question">
											<p><strong>54. 神经网络中的"前向传播"是指：</strong></p>
											<div class="options">
												<label><input type="radio" name="perceptron_q54" value="A"> A. 从输出层向输入层传递梯度</label>
												<label><input type="radio" name="perceptron_q54" value="B"> B. 从输入层向输出层计算预测值</label>
												<label><input type="radio" name="perceptron_q54" value="C"> C. 更新网络权重</label>
												<label><input type="radio" name="perceptron_q54" value="D"> D. 计算损失函数</label>
											</div>
										</div>
										<!-- 题目 55 -->
										<div class="question">
											<p><strong>55. 反向传播（Backpropagation）算法主要用于：</strong></p>
											<div class="options">
												<label><input type="radio" name="perceptron_q55" value="A"> A. 计算神经网络的预测输出</label>
												<label><input type="radio" name="perceptron_q55" value="B"> B. 计算损失函数相对于网络权重的梯度</label>
												<label><input type="radio" name="perceptron_q55" value="C"> C. 初始化网络权重</label>
												<label><input type="radio" name="perceptron_q55" value="D"> D. 选择最优的激活函数</label>
											</div>
										</div>
										<!-- 题目 56 -->
										<div class="question">
											<p><strong>56. 以下哪个是典型的监督学习任务？</strong></p>
											<div class="options">
												<label><input type="radio" name="perceptron_q56" value="A"> A. 聚类</label>
												<label><input type="radio" name="perceptron_q56" value="B"> B. 降维</label>
												<label><input type="radio" name="perceptron_q56" value="C"> C. 图像分类</label>
												<label><input type="radio" name="perceptron_q56" value="D"> D. 关联规则挖掘</label>
											</div>
										</div>
										<!-- 题目 57 -->
										<div class="question">
											<p><strong>57. 损失函数（Loss Function）的主要作用是：</strong></p>
											<div class="options">
												<label><input type="radio" name="perceptron_q57" value="A"> A. 衡量模型预测值与真实值之间的差异</label>
												<label><input type="radio" name="perceptron_q57" value="B"> B. 决定网络的结构</label>
												<label><input type="radio" name="perceptron_q57" value="C"> C. 选择优化算法</label>
												<label><input type="radio" name="perceptron_q57" value="D"> D. 初始化网络参数</label>
											</div>
										</div>
										<!-- 题目 58 -->
										<div class="question">
											<p><strong>58. 梯度下降（Gradient Descent）优化算法的核心思想是：</strong></p>
											<div class="options">
												<label><input type="radio" name="perceptron_q58" value="A"> A. 沿着损失函数梯度的反方向更新参数，以减小损失</label>
												<label><input type="radio" name="perceptron_q58" value="B"> B. 沿着损失函数梯度的正方向更新参数，以增大损失</label>
												<label><input type="radio" name="perceptron_q58" value="C"> C. 随机更新参数</label>
												<label><input type="radio" name="perceptron_q58" value="D"> D. 保持参数不变</label>
											</div>
										</div>
										<!-- 题目 59 -->
										<div class="question">
											<p><strong>59. 过拟合（Overfitting）是指模型：</strong></p>
											<div class="options">
												<label><input type="radio" name="perceptron_q59" value="A"> A. 在训练集和测试集上都表现很差</label>
												<label><input type="radio" name="perceptron_q59" value="B"> B. 在训练集上表现很好，但在未见过的测试集上表现很差</label>
												<label><input type="radio" name="perceptron_q59" value="C"> C. 在训练集上表现很差，但在测试集上表现很好</label>
												<label><input type="radio" name="perceptron_q59" value="D"> D. 无法收敛</label>
											</div>
										</div>
										<!-- 题目 60 -->
										<div class="question">
											<p><strong>60. 以下哪种方法不能用于防止过拟合？</strong></p>
											<div class="options">
												<label><input type="radio" name="perceptron_q60" value="A"> A. 获取更多训练数据</label>
												<label><input type="radio" name="perceptron_q60" value="B"> B. 使用Dropout</label>
												<label><input type="radio" name="perceptron_q60" value="C"> C. 减少网络层数（减小模型容量）</label>
												<label><input type="radio" name="perceptron_q60" value="D"> D. 增加网络层数（增大模型容量）</label>
											</div>
										</div>
										<!-- 得分显示和提交按钮 -->
										<div class="score-display"></div>
										<button class="submit-quiz-btn" data-category="perceptron_quiz">提交答案</button>
										<button class="close-exercise-btn">收起练习</button>
									</div>
								</div>
							</div>
						</div>
					
						<!-- 核心架构：CNN（61-75题） -->
						<div id="exerciseCNN" style="display: block;">
							<div class="exercise-section-title">
								<div class="title-header">
									<span class="title-tag">核心架构模块</span>
									<span class="title-icon">🔍</span>
								</div>
								<h2 class="title-name">卷积神经网络 (CNN)</h2>
								<div class="title-info">
									<span class="info-item">
										<i class="icon-question"></i>
										15道题目
									</span>
									<span class="info-item">
										<i class="icon-clock"></i>
										题号：61-75
									</span>
								</div>
							</div>
							<div class="exercise-section-content">
								<div class="exercise-card" data-exercise="cnn_quiz">
									<div class="card-content">
										<p>检测你对卷积神经网络结构和原理的理解。</p>
										<button class="start-exercise-btn">开始练习</button>
									</div>
									<div class="card-questions" style="display:none;">
										<!-- 题目 61 -->
										<div class="question">
											<p><strong>61. 卷积神经网络（CNN）的核心思想是：</strong></p>
											<div class="options">
												<label><input type="radio" name="cnn_q61" value="A"> A. 参数共享和局部连接</label>
												<label><input type="radio" name="cnn_q61" value="B"> B. 全局连接和循环反馈</label>
												<label><input type="radio" name="cnn_q61" value="C"> C. 自注意力机制</label>
												<label><input type="radio" name="cnn_q61" value="D"> D. 门控机制</label>
											</div>
										</div>
										<!-- 题目 62 -->
										<div class="question">
											<p><strong>62. 在CNN中，一个滤波器（或卷积核）的作用类似于：</strong></p>
											<div class="options">
												<label><input type="radio" name="cnn_q62" value="A"> A. 一个全连接层</label>
												<label><input type="radio" name="cnn_q62" value="B"> B. 一个特征检测器</label>
												<label><input type="radio" name="cnn_q62" value="C"> C. 一个池化操作</label>
												<label><input type="radio" name="cnn_q62" value="D"> D. 一个激活函数</label>
											</div>
										</div>
										<!-- 题目 63 -->
										<div class="question">
											<p><strong>63. LeNet-5是早期成功的CNN，主要应用于：</strong></p>
											<div class="options">
												<label><input type="radio" name="cnn_q63" value="A"> A. ImageNet图像分类</label>
												<label><input type="radio" name="cnn_q63" value="B"> B. 人脸识别</label>
												<label><input type="radio" name="cnn_q63" value="C"> C. 手写数字识别</label>
												<label><input type="radio" name="cnn_q63" value="D"> D. 目标检测</label>
											</div>
										</div>
										<!-- 题目 64 -->
										<div class="question">
											<p><strong>64. AlexNet在2012年ImageNet竞赛中获胜的关键因素不包括：</strong></p>
											<div class="options">
												<label><input type="radio" name="cnn_q64" value="A"> A. 使用更深的网络结构</label>
												<label><input type="radio" name="cnn_q64" value="B"> B. 使用ReLU激活函数缓解梯度消失</label>
												<label><input type="radio" name="cnn_q64" value="C"> C. 使用Dropout防止过拟合</label>
												<label><input type="radio" name="cnn_q64" value="D"> D. 使用Transformer模块</label>
											</div>
										</div>
										<!-- 题目 65 -->
										<div class="question">
											<p><strong>65. 填充（Padding）在卷积操作中的主要作用是：</strong></p>
											<div class="options">
												<label><input type="radio" name="cnn_q65" value="A"> A. 增加特征图的通道数</label>
												<label><input type="radio" name="cnn_q65" value="B"> B. 控制输出特征图的空间尺寸，防止过快缩小</label>
												<label><input type="radio" name="cnn_q65" value="C"> C. 引入非线性</label>
												<label><input type="radio" name="cnn_q65" value="D"> D. 减少计算量</label>
											</div>
										</div>
										<!-- 题目 66 -->
										<div class="question">
											<p><strong>66. 步长（Stride）在卷积操作中是指：</strong></p>
											<div class="options">
												<label><input type="radio" name="cnn_q66" value="A"> A. 滤波器的大小</label>
												<label><input type="radio" name="cnn_q66" value="B"> B. 滤波器每次移动的像素数</label>
												<label><input type="radio" name="cnn_q66" value="C"> C. 输入图像的通道数</label>
												<label><input type="radio" name="cnn_q66" value="D"> D. 激活函数的类型</label>
											</div>
										</div>
										<!-- 题目 67 -->
										<div class="question">
											<p><strong>67. VGGNet的主要贡献是证明了：</strong></p>
											<div class="options">
												<label><input type="radio" name="cnn_q67" value="A"> A. 网络的深度是提升性能的关键</label>
												<label><input type="radio" name="cnn_q67" value="B"> B. 使用大尺寸卷积核更好</label>
												<label><input type="radio" name="cnn_q67" value="C"> C. 残差连接至关重要</label>
												<label><input type="radio" name="cnn_q67" value="D"> D. 注意力机制最有效</label>
											</div>
										</div>
										<!-- 题目 68 -->
										<div class="question">
											<p><strong>68. GoogLeNet（Inception v1）的核心模块Inception的主要设计理念是：</strong></p>
											<div class="options">
												<label><input type="radio" name="cnn_q68" value="A"> A. 网络越深越好</label>
												<label><input type="radio" name="cnn_q68" value="B"> B. 在同一层使用不同尺寸的卷积核来捕获多尺度信息</label>
												<label><input type="radio" name="cnn_q68" value="C"> C. 使用跳跃连接</label>
												<label><input type="radio" name="cnn_q68" value="D"> D. 仅使用1x1卷积</label>
											</div>
										</div>
										<!-- 题目 69 -->
										<div class="question">
											<p><strong>69. 在Inception模块中，1x1卷积的主要作用是：</strong></p>
											<div class="options">
												<label><input type="radio" name="cnn_q69" value="A"> A. 增加空间维度</label>
												<label><input type="radio" name="cnn_q69" value="B"> B. 降维，减少计算量和参数量</label>
												<label><input type="radio" name="cnn_q69" value="C"> C. 提取大范围特征</label>
												<label><input type="radio" name="cnn_q69" value="D"> D. 代替池化层</label>
											</div>
										</div>
										<!-- 题目 70 -->
										<div class="question">
											<p><strong>70. 残差网络（ResNet）解决了深度CNN中的什么问题？</strong></p>
											<div class="options">
												<label><input type="radio" name="cnn_q70" value="A"> A. 计算量过大</label>
												<label><input type="radio" name="cnn_q70" value="B"> B. 梯度消失和网络退化（性能饱和甚至下降）</label>
												<label><input type="radio" name="cnn_q70" value="C"> C. 过拟合</label>
												<label><input type="radio" name="cnn_q70" value="D"> D. 特征提取能力不足</label>
											</div>
										</div>
										<!-- 题目 71 -->
										<div class="question">
											<p><strong>71. ResNet中的"残差块"学习的是：</strong></p>
											<div class="options">
												<label><input type="radio" name="cnn_q71" value="A"> A. 期望的输出H(x)</label>
												<label><input type="radio" name="cnn_q71" value="B"> B. 期望的输出与输入之间的残差F(x) = H(x) - x</label>
												<label><input type="radio" name="cnn_q71" value="C"> C. 输入的恒等映射x</label>
												<label><input type="radio" name="cnn_q71" value="D"> D. 随机噪声</label>
											</div>
										</div>
										<!-- 题目 72 -->
										<div class="question">
											<p><strong>72. DenseNet的核心思想是：</strong></p>
											<div class="options">
												<label><input type="radio" name="cnn_q72" value="A"> A. 每一层都直接连接到输入层</label>
												<label><input type="radio" name="cnn_q72" value="B"> B. 每一层都接受前面所有层的输出作为输入</label>
												<label><input type="radio" name="cnn_q72" value="C"> C. 只有相邻层之间有连接</label>
												<label><input type="radio" name="cnn_q72" value="D"> D. 使用非常大的卷积核</label>
											</div>
										</div>
										<!-- 题目 73 -->
										<div class="question">
											<p><strong>73. MobileNet的主要目标是：</strong></p>
											<div class="options">
												<label><input type="radio" name="cnn_q73" value="A"> A. 获得最高的分类准确率</label>
												<label><input type="radio" name="cnn_q73" value="B"> B. 构建最深的网络</label>
												<label><input type="radio" name="cnn_q73" value="C"> C. 设计轻量级模型，适用于移动和嵌入式设备</label>
												<label><input type="radio" name="cnn_q73" value="D"> D. 证明注意力机制的有效性</label>
											</div>
										</div>
										<!-- 题目 74 -->
										<div class="question">
											<p><strong>74. 在目标检测任务中，R-CNN、Fast R-CNN、Faster R-CNN等算法属于：</strong></p>
											<div class="options">
												<label><input type="radio" name="cnn_q74" value="A"> A. 单阶段检测器</label>
												<label><input type="radio" name="cnn_q74" value="B"> B. 两阶段检测器</label>
												<label><input type="radio" name="cnn_q74" value="C"> C. 纯CNN分类器</label>
												<label><input type="radio" name="cnn_q74" value="D"> D. 生成对抗网络</label>
											</div>
										</div>
										<!-- 题目 75 -->
										<div class="question">
											<p><strong>75. YOLO（You Only Look Once）和SSD（Single Shot MultiBox Detector）属于：</strong></p>
											<div class="options">
												<label><input type="radio" name="cnn_q75" value="A"> A. 两阶段检测器</label>
												<label><input type="radio" name="cnn_q75" value="B"> B. 单阶段检测器</label>
												<label><input type="radio" name="cnn_q75" value="C"> C. 语义分割模型</label>
												<label><input type="radio" name="cnn_q75" value="D"> D. 实例分割模型</label>
											</div>
										</div>
										<!-- 得分显示和提交按钮 -->
										<div class="score-display"></div>
										<button class="submit-quiz-btn" data-category="cnn_quiz">提交答案</button>
										<button class="close-exercise-btn">收起练习</button>
									</div>
								</div>
							</div>
						</div>
					
						<!-- 核心架构：RNN（76-90题） -->
						<div id="exerciseRNN" style="display: block;">
							<div class="exercise-section-title">
								<div class="title-header">
									<span class="title-tag">核心架构模块</span>
									<span class="title-icon">🔄</span>
								</div>
								<h2 class="title-name">循环神经网络 (RNN)</h2>
								<div class="title-info">
									<span class="info-item">
										<i class="icon-question"></i>
										15道题目
									</span>
									<span class="info-item">
										<i class="icon-clock"></i>
										题号：76-90
									</span>
								</div>
							</div>
							<div class="exercise-section-content">
								<div class="exercise-card" data-exercise="rnn_quiz">
									<div class="card-content">
										<p>检测你对循环神经网络及其变体的理解。</p>
										<button class="start-exercise-btn">开始练习</button>
									</div>
									<div class="card-questions" style="display:none;">
										<!-- 题目 76 -->
										<div class="question">
											<p><strong>76. 循环神经网络（RNN）最适用于处理什么类型的数据？</strong></p>
											<div class="options">
												<label><input type="radio" name="rnn_q76" value="A"> A. 独立的图像数据</label>
												<label><input type="radio" name="rnn_q76" value="B"> B. 具有序列依赖关系的数据（如文本、时间序列）</label>
												<label><input type="radio" name="rnn_q76" value="C"> C. 高维表格数据</label>
												<label><input type="radio" name="rnn_q76" value="D"> D. 3D点云数据</label>
											</div>
										</div>
										<!-- 题目 77 -->
										<div class="question">
											<p><strong>77. 在RNN中，隐藏状态（Hidden State）的作用是：</strong></p>
											<div class="options">
												<label><input type="radio" name="rnn_q77" value="A"> A. 存储当前输入的信息</label>
												<label><input type="radio" name="rnn_q77" value="B"> B. 存储从序列开始到当前时刻的累积信息</label>
												<label><input type="radio" name="rnn_q77" value="C"> C. 仅存储上一时刻的输出</label>
												<label><input type="radio" name="rnn_q77" value="D"> D. 存储网络参数</label>
											</div>
										</div>
										<!-- 题目 78 -->
										<div class="question">
											<p><strong>78. 传统RNN在训练长序列时面临的主要问题是：</strong></p>
											<div class="options">
												<label><input type="radio" name="rnn_q78" value="A"> A. 计算速度慢</label>
												<label><input type="radio" name="rnn_q78" value="B"> B. 梯度消失或梯度爆炸</label>
												<label><input type="radio" name="rnn_q78" value="C"> C. 参数过少</label>
												<label><input type="radio" name="rnn_q78" value="D"> D. 无法处理变长序列</label>
											</div>
										</div>
										<!-- 题目 79 -->
										<div class="question">
											<p><strong>79. 长短期记忆网络（LSTM）引入什么机制来解决梯度问题？</strong></p>
											<div class="options">
												<label><input type="radio" name="rnn_q79" value="A"> A. 跳跃连接</label>
												<label><input type="radio" name="rnn_q79" value="B"> B. 门控机制（输入门、遗忘门、输出门）</label>
												<label><input type="radio" name="rnn_q79" value="C"> C. 注意力机制</label>
												<label><input type="radio" name="rnn_q79" value="D"> D. 卷积操作</label>
											</div>
										</div>
										<!-- 题目 80 -->
										<div class="question">
											<p><strong>80. 在LSTM中，遗忘门（Forget Gate）决定：</strong></p>
											<div class="options">
												<label><input type="radio" name="rnn_q80" value="A"> A. 哪些新信息要存入细胞状态</label>
												<label><input type="radio" name="rnn_q80" value="B"> B. 从细胞状态中丢弃哪些旧信息</label>
												<label><input type="radio" name="rnn_q80" value="C"> C. 基于细胞状态输出什么信息</label>
												<label><input type="radio" name="rnn_q80" value="D"> D. 当前输入的重要性</label>
											</div>
										</div>
										<!-- 题目 81 -->
										<div class="question">
											<p><strong>81. 门控循环单元（GRU）与LSTM的主要区别是：</strong></p>
											<div class="options">
												<label><input type="radio" name="rnn_q81" value="A"> A. GRU没有门控机制</label>
												<label><input type="radio" name="rnn_q81" value="B"> B. GRU将LSTM的输入门和遗忘门合并为更新门，并简化了结构</label>
												<label><input type="radio" name="rnn_q81" value="C"> C. GRU比LSTM计算更复杂</label>
												<label><input type="radio" name="rnn_q81" value="D"> D. GRU不能缓解梯度消失</label>
											</div>
										</div>
										<!-- 题目 82 -->
										<div class="question">
											<p><strong>82. 双向RNN（Bi-RNN）的特点是：</strong></p>
											<div class="options">
												<label><input type="radio" name="rnn_q82" value="A"> A. 只能从左到右处理序列</label>
												<label><input type="radio" name="rnn_q82" value="B"> B. 同时从左到右和从右到左处理序列，融合两个方向的信息</label>
												<label><input type="radio" name="rnn_q82" value="C"> C. 随机处理序列</label>
												<label><input type="radio" name="rnn_q82" value="D"> D. 跳跃处理序列</label>
											</div>
										</div>
										<!-- 题目 83 -->
										<div class="question">
											<p><strong>83. Seq2Seq（序列到序列）模型通常用于什么任务？</strong></p>
											<div class="options">
												<label><input type="radio" name="rnn_q83" value="A"> A. 图像分类</label>
												<label><input type="radio" name="rnn_q83" value="B"> B. 机器翻译、文本摘要</label>
												<label><input type="radio" name="rnn_q83" value="C"> C. 目标检测</label>
												<label><input type="radio" name="rnn_q83" value="D"> D. 聚类分析</label>
											</div>
										</div>
										<!-- 题目 84 -->
										<div class="question">
											<p><strong>84. 在传统的Seq2Seq模型中，编码器将整个输入序列编码为：</strong></p>
											<div class="options">
												<label><input type="radio" name="rnn_q84" value="A"> A. 一个词向量序列</label>
												<label><input type="radio" name="rnn_q84" value="B"> B. 一个固定长度的上下文向量</label>
												<label><input type="radio" name="rnn_q84" value="C"> C. 一个注意力权重矩阵</label>
												<label><input type="radio" name="rnn_q84" value="D"> D. 一个分类概率</label>
											</div>
										</div>
										<!-- 题目 85 -->
										<div class="question">
											<p><strong>85. 注意力机制（Attention）被引入Seq2Seq模型，主要是为了解决什么问题？</strong></p>
											<div class="options">
												<label><input type="radio" name="rnn_q85" value="A"> A. 编码器和解码器训练不同步</label>
												<label><input type="radio" name="rnn_q85" value="B"> B. 固定长度上下文向量的信息瓶颈问题</label>
												<label><input type="radio" name="rnn_q85" value="C"> C. RNN计算速度慢</label>
												<label><input type="radio" name="rnn_q85" value="D"> D. 梯度消失</label>
											</div>
										</div>
										<!-- 题目 86 -->
										<div class="question">
											<p><strong>86. Transformer模型完全基于以下哪种机制？</strong></p>
											<div class="options">
												<label><input type="radio" name="rnn_q86" value="A"> A. 卷积</label>
												<label><input type="radio" name="rnn_q86" value="B"> B. 循环</label>
												<label><input type="radio" name="rnn_q86" value="C"> C. 自注意力（Self-Attention）</label>
												<label><input type="radio" name="rnn_q86" value="D"> D. 门控</label>
											</div>
										</div>
										<!-- 题目 87 -->
										<div class="question">
											<p><strong>87. 在Transformer的自注意力机制中，一个序列中的每个词会生成三个向量，它们是：</strong></p>
											<div class="options">
												<label><input type="radio" name="rnn_q87" value="A"> A. Query, Key, Value</label>
												<label><input type="radio" name="rnn_q87" value="B"> B. Input, Output, State</label>
												<label><input type="radio" name="rnn_q87" value="C"> C. Source, Target, Context</label>
												<label><input type="radio" name="rnn_q87" value="D"> D. Word, Position, Embedding</label>
											</div>
										</div>
										<!-- 题目 88 -->
										<div class="question">
											<p><strong>88. 自注意力机制中，计算第i个词与第j个词之间的相关性（注意力分数）是通过：</strong></p>
											<div class="options">
												<label><input type="radio" name="rnn_q88" value="A"> A. 计算Query_i与Key_j的点积</label>
												<label><input type="radio" name="rnn_q88" value="B"> B. 计算Value_i与Value_j的点积</label>
												<label><input type="radio" name="rnn_q88" value="C"> C. 计算Query_i与Query_j的余弦相似度</label>
												<label><input type="radio" name="rnn_q88" value="D"> D. 直接使用一个可学习的参数</label>
											</div>
										</div>
										<!-- 题目 89 -->
										<div class="question">
											<p><strong>89. Transformer为什么要使用多头注意力（Multi-Head Attention）？</strong></p>
											<div class="options">
												<label><input type="radio" name="rnn_q89" value="A"> A. 为了增加模型深度</label>
												<label><input type="radio" name="rnn_q89" value="B"> B. 为了让模型同时关注来自不同表示子空间的信息</label>
												<label><input type="radio" name="rnn_q89" value="C"> C. 为了减少计算量</label>
												<label><input type="radio" name="rnn_q89" value="D"> D. 为了引入循环连接</label>
											</div>
										</div>
										<!-- 题目 90 -->
										<div class="question">
											<p><strong>90. Transformer中的位置编码（Positional Encoding）是必需的，因为：</strong></p>
											<div class="options">
												<label><input type="radio" name="rnn_q90" value="A"> A. 自注意力机制本身具有位置感知能力</label>
												<label><input type="radio" name="rnn_q90" value="B"> B. 自注意力机制是排列不变的，需要额外注入序列的顺序信息</label>
												<label><input type="radio" name="rnn_q90" value="C"> C. 它可以帮助防止过拟合</label>
												<label><input type="radio" name="rnn_q90" value="D"> D. 它用于替代词嵌入（Word Embedding）</label>
											</div>
										</div>
										<!-- 得分显示和提交按钮 -->
										<div class="score-display"></div>
										<button class="submit-quiz-btn" data-category="rnn_quiz">提交答案</button>
										<button class="close-exercise-btn">收起练习</button>
									</div>
								</div>
							</div>
						</div>
					
						<!-- 核心架构：Transformer（91-100题） -->
						<div id="exerciseTransformer" style="display: block;">
							<div class="exercise-section-title">
								<div class="title-header">
									<span class="title-tag">核心架构模块</span>
									<span class="title-icon">🎯</span>
								</div>
								<h2 class="title-name">Transformer</h2>
								<div class="title-info">
									<span class="info-item">
										<i class="icon-question"></i>
										10道题目
									</span>
									<span class="info-item">
										<i class="icon-clock"></i>
										题号：91-100
									</span>
								</div>
							</div>
							<div class="exercise-section-content">
								<div class="exercise-card" data-exercise="transformer_quiz">
									<div class="card-content">
										<p>检测你对Transformer架构和注意力机制的理解。</p>
										<button class="start-exercise-btn">开始练习</button>
									</div>
									<div class="card-questions" style="display:none;">
										<!-- 题目 91 -->
										<div class="question">
											<p><strong>91. Transformer模型完全基于以下哪种机制？</strong></p>
											<div class="options">
												<label><input type="radio" name="transformer_q91" value="A"> A. 卷积</label>
												<label><input type="radio" name="transformer_q91" value="B"> B. 循环</label>
												<label><input type="radio" name="transformer_q91" value="C"> C. 自注意力（Self-Attention）</label>
												<label><input type="radio" name="transformer_q91" value="D"> D. 门控</label>
											</div>
										</div>
										<!-- 题目 92 -->
										<div class="question">
											<p><strong>92. 在Transformer的自注意力机制中，一个序列中的每个词会生成三个向量，它们是：</strong></p>
											<div class="options">
												<label><input type="radio" name="transformer_q92" value="A"> A. Query, Key, Value</label>
												<label><input type="radio" name="transformer_q92" value="B"> B. Input, Output, State</label>
												<label><input type="radio" name="transformer_q92" value="C"> C. Source, Target, Context</label>
												<label><input type="radio" name="transformer_q92" value="D"> D. Word, Position, Embedding</label>
											</div>
										</div>
										<!-- 题目 93 -->
										<div class="question">
											<p><strong>93. 自注意力机制中，计算第i个词与第j个词之间的相关性（注意力分数）是通过：</strong></p>
											<div class="options">
												<label><input type="radio" name="transformer_q93" value="A"> A. 计算Query_i与Key_j的点积</label>
												<label><input type="radio" name="transformer_q93" value="B"> B. 计算Value_i与Value_j的点积</label>
												<label><input type="radio" name="transformer_q93" value="C"> C. 计算Query_i与Query_j的余弦相似度</label>
												<label><input type="radio" name="transformer_q93" value="D"> D. 直接使用一个可学习的参数</label>
											</div>
										</div>
										<!-- 题目 94 -->
										<div class="question">
											<p><strong>94. Transformer为什么要使用多头注意力（Multi-Head Attention）？</strong></p>
											<div class="options">
												<label><input type="radio" name="transformer_q94" value="A"> A. 为了增加模型深度</label>
												<label><input type="radio" name="transformer_q94" value="B"> B. 为了让模型同时关注来自不同表示子空间的信息</label>
												<label><input type="radio" name="transformer_q94" value="C"> C. 为了减少计算量</label>
												<label><input type="radio" name="transformer_q94" value="D"> D. 为了引入循环连接</label>
											</div>
										</div>
										<!-- 题目 95 -->
										<div class="question">
											<p><strong>95. Transformer中的位置编码（Positional Encoding）是必需的，因为：</strong></p>
											<div class="options">
												<label><input type="radio" name="transformer_q95" value="A"> A. 自注意力机制本身具有位置感知能力</label>
												<label><input type="radio" name="transformer_q95" value="B"> B. 自注意力机制是排列不变的，需要额外注入序列的顺序信息</label>
												<label><input type="radio" name="transformer_q95" value="C"> C. 它可以帮助防止过拟合</label>
												<label><input type="radio" name="transformer_q95" value="D"> D. 它用于替代词嵌入（Word Embedding）</label>
											</div>
										</div>
										<!-- 题目 96 -->
										<div class="question">
											<p><strong>96. Transformer编码器层主要由哪两个子层组成？</strong></p>
											<div class="options">
												<label><input type="radio" name="transformer_q96" value="A"> A. 自注意力子层和前馈神经网络子层</label>
												<label><input type="radio" name="transformer_q96" value="B"> B. 卷积子层和池化子层</label>
												<label><input type="radio" name="transformer_q96" value="C"> C. LSTM子层和GRU子层</label>
												<label><input type="radio" name="transformer_q96" value="D"> D. 编码子层和解码子层</label>
											</div>
										</div>
										<!-- 题目 97 -->
										<div class="question">
											<p><strong>97. Transformer模型在训练时相比RNN的一个巨大优势是：</strong></p>
											<div class="options">
												<label><input type="radio" name="transformer_q97" value="A"> A. 参数更少</label>
												<label><input type="radio" name="transformer_q97" value="B"> B. 可以更好地处理长程依赖</label>
												<label><input type="radio" name="transformer_q97" value="C"> C. 具有高度的并行计算能力</label>
												<label><input type="radio" name="transformer_q97" value="D"> D. B和C</label>
											</div>
										</div>
										<!-- 题目 98 -->
										<div class="question">
											<p><strong>98. BERT（Bidirectional Encoder Representations from Transformers）模型是基于Transformer的：</strong></p>
											<div class="options">
												<label><input type="radio" name="transformer_q98" value="A"> A. 仅编码器结构</label>
												<label><input type="radio" name="transformer_q98" value="B"> B. 仅解码器结构</label>
												<label><input type="radio" name="transformer_q98" value="C"> C. 编码器-解码器结构</label>
												<label><input type="radio" name="transformer_q98" value="D"> D. 卷积-Transformer混合结构</label>
											</div>
										</div>
										<!-- 题目 99 -->
										<div class="question">
											<p><strong>99. GPT（Generative Pre-trained Transformer）系列模型是基于Transformer的：</strong></p>
											<div class="options">
												<label><input type="radio" name="transformer_q99" value="A"> A. 仅编码器结构</label>
												<label><input type="radio" name="transformer_q99" value="B"> B. 仅解码器结构</label>
												<label><input type="radio" name="transformer_q99" value="C"> C. 编码器-解码器结构</label>
												<label><input type="radio" name="transformer_q99" value="D"> D. 注意力-卷积结构</label>
											</div>
										</div>
										<!-- 题目 100 -->
										<div class="question">
											<p><strong>100. Vision Transformer（ViT）成功地将Transformer应用于计算机视觉，其关键步骤是：</strong></p>
											<div class="options">
												<label><input type="radio" name="transformer_q100" value="A"> A. 直接将像素输入Transformer</label>
												<label><input type="radio" name="transformer_q100" value="B"> B. 将图像分割成小块（Patches），并将其视为一个序列</label>
												<label><input type="radio" name="transformer_q100" value="C"> C. 先用CNN提取特征，再输入Transformer</label>
												<label><input type="radio" name="transformer_q100" value="D"> D. 在图像上应用循环注意力</label>
											</div>
										</div>
										<!-- 得分显示和提交按钮 -->
										<div class="score-display"></div>
										<button class="submit-quiz-btn" data-category="transformer_quiz">提交答案</button>
										<button class="close-exercise-btn">收起练习</button>
									</div>
								</div>
							</div>
						</div>
					
						<!-- 综合应用：架构选择与对比（101-120题） -->
						<div id="exerciseArchSelection" style="display: block;">
							<div class="exercise-section-title">
								<div class="title-header">
									<span class="title-tag">综合应用模块</span>
									<span class="title-icon">📊</span>
								</div>
								<h2 class="title-name">架构选择与对比</h2>
								<div class="title-info">
									<span class="info-item">
										<i class="icon-question"></i>
										20道题目
									</span>
									<span class="info-item">
										<i class="icon-clock"></i>
										题号：101-120
									</span>
								</div>
							</div>
							<div class="exercise-section-content">
								<div class="exercise-card" data-exercise="arch_selection_quiz">
									<div class="card-content">
										<p>检测你根据任务需求选择合适神经网络架构的能力。</p>
										<button class="start-exercise-btn">开始练习</button>
									</div>
									<div class="card-questions" style="display:none;">
										<!-- 题目 101 -->
										<div class="question">
											<p><strong>101. 对于图像分类任务（如ImageNet），以下哪种架构在传统上被认为是最佳选择？</strong></p>
											<div class="options">
												<label><input type="radio" name="arch_selection_q101" value="A"> A. 纯RNN</label>
												<label><input type="radio" name="arch_selection_q101" value="B"> B. Transformer</label>
												<label><input type="radio" name="arch_selection_q101" value="C"> C. 卷积神经网络（如ResNet, EfficientNet）</label>
												<label><input type="radio" name="arch_selection_q101" value="D"> D. 多层感知机（MLP）</label>
											</div>
										</div>
										<!-- 题目 102 -->
										<div class="question">
											<p><strong>102. 当你需要处理一个长文档并理解其整体情感时，哪种架构可能更合适？</strong></p>
											<div class="options">
												<label><input type="radio" name="arch_selection_q102" value="A"> A. CNN</label>
												<label><input type="radio" name="arch_selection_q102" value="B"> B. RNN/LSTM</label>
												<label><input type="radio" name="arch_selection_q102" value="C"> C. Transformer（如BERT）</label>
												<label><input type="radio" name="arch_selection_q102" value="D"> D. 单层感知机</label>
											</div>
										</div>
										<!-- 题目 103 -->
										<div class="question">
											<p><strong>103. 在资源受限的移动设备上进行实时图像分类，应优先考虑哪种模型？</strong></p>
											<div class="options">
												<label><input type="radio" name="arch_selection_q103" value="A"> A. ResNet-152</label>
												<label><input type="radio" name="arch_selection_q103" value="B"> B. VGG-19</label>
												<label><input type="radio" name="arch_selection_q103" value="C"> C. MobileNet 或 ShuffleNet</label>
												<label><input type="radio" name="arch_selection_q103" value="D"> D. DenseNet-201</label>
											</div>
										</div>
										<!-- 题目 104 -->
										<div class="question">
											<p><strong>104. 对于机器翻译任务，以下哪种架构组合是当前最主流的？</strong></p>
											<div class="options">
												<label><input type="radio" name="arch_selection_q104" value="A"> A. 编码器用CNN，解码器用RNN</label>
												<label><input type="radio" name="arch_selection_q104" value="B"> B. 编码器和解码器都用RNN（如LSTM）</label>
												<label><input type="radio" name="arch_selection_q104" value="C"> C. 编码器和解码器都用Transformer</label>
												<label><input type="radio" name="arch_selection_q104" value="D"> D. 编码器用RNN，解码器用MLP</label>
											</div>
										</div>
										<!-- 题目 105 -->
										<div class="question">
											<p><strong>105. 哪种模型架构天生适合处理视频数据（时空序列）？</strong></p>
											<div class="options">
												<label><input type="radio" name="arch_selection_q105" value="A"> A. 2D CNN</label>
												<label><input type="radio" name="arch_selection_q105" value="B"> B. 3D CNN</label>
												<label><input type="radio" name="arch_selection_q105" value="C"> C. 纯LSTM</label>
												<label><input type="radio" name="arch_selection_q105" value="D"> D. 纯Transformer</label>
											</div>
										</div>
										<!-- 题目 106 -->
										<div class="question">
											<p><strong>106. 哪个任务不适合用标准的卷积神经网络（CNN）直接处理？</strong></p>
											<div class="options">
												<label><input type="radio" name="arch_selection_q106" value="A"> A. 图像风格迁移</label>
												<label><input type="radio" name="arch_selection_q106" value="B"> B. 股票价格时间序列预测</label>
												<label><input type="radio" name="arch_selection_q106" value="C"> C. 人脸识别</label>
												<label><input type="radio" name="arch_selection_q106" value="D"> D. 目标检测</label>
											</div>
										</div>
										<!-- 题目 107 -->
										<div class="question">
											<p><strong>107. 如果你想建立一个聊天机器人，需要模型能根据对话历史生成连贯的回复，最好使用：</strong></p>
											<div class="options">
												<label><input type="radio" name="arch_selection_q107" value="A"> A. 一个图像分类模型</label>
												<label><input type="radio" name="arch_selection_q107" value="B"> B. 一个基于Transformer的解码器模型（如GPT）</label>
												<label><input type="radio" name="arch_selection_q107" value="C"> C. 一个简单的线性回归模型</label>
												<label><input type="radio" name="arch_selection_q107" value="D"> D. 一个无监督聚类模型</label>
											</div>
										</div>
										<!-- 题目 108 -->
										<div class="question">
											<p><strong>108. 对比CNN和RNN，以下说法正确的是：</strong></p>
											<div class="options">
												<label><input type="radio" name="arch_selection_q108" value="A"> A. CNN只能处理固定长度的输入，RNN可以处理变长输入</label>
												<label><input type="radio" name="arch_selection_q108" value="B"> B. RNN的并行计算能力优于CNN</label>
												<label><input type="radio" name="arch_selection_q108" value="C"> C. CNN天然适合处理序列数据，RNN天然适合处理图像数据</label>
												<label><input type="radio" name="arch_selection_q108" value="D"> D. 两者都不能处理非线性问题</label>
											</div>
										</div>
										<!-- 题目 109 -->
										<div class="question">
											<p><strong>109. Transformer相比LSTM，在训练速度上的主要优势来自于：</strong></p>
											<div class="options">
												<label><input type="radio" name="arch_selection_q109" value="A"> A. 参数更少</label>
												<label><input type="radio" name="arch_selection_q109" value="B"> B. 不需要梯度下降</label>
												<label><input type="radio" name="arch_selection_q109" value="C"> C. 序列计算可以完全并行化</label>
												<label><input type="radio" name="arch_selection_q109" value="D"> D. 使用了更简单的激活函数</label>
											</div>
										</div>
										<!-- 题目 110 -->
										<div class="question">
											<p><strong>110. 将CNN和RNN结合使用的常见场景是：</strong></p>
											<div class="options">
												<label><input type="radio" name="arch_selection_q110" value="A"> A. 图像描述生成（Image Captioning）</label>
												<label><input type="radio" name="arch_selection_q110" value="B"> B. 文本分类</label>
												<label><input type="radio" name="arch_selection_q110" value="C"> C. 语音识别</label>
												<label><input type="radio" name="arch_selection_q110" value="D"> D. A和C</label>
											</div>
										</div>
										<!-- 题目 111 -->
										<div class="question">
											<p><strong>111. 残差连接（Residual Connection）最初在哪个架构中被普及？</strong></p>
											<div class="options">
												<label><input type="radio" name="arch_selection_q111" value="A"> A. VGG</label>
												<label><input type="radio" name="arch_selection_q111" value="B"> B. Inception</label>
												<label><input type="radio" name="arch_selection_q111" value="C"> C. ResNet</label>
												<label><input type="radio" name="arch_selection_q111" value="D"> D. Transformer</label>
											</div>
										</div>
										<!-- 题目 112 -->
										<div class="question">
											<p><strong>112. 以下哪种技术主要是为了减少模型的内存占用和计算量？</strong></p>
											<div class="options">
												<label><input type="radio" name="arch_selection_q112" value="A"> A. 使用更大的批量大小（Batch Size）</label>
												<label><input type="radio" name="arch_selection_q112" value="B"> B. 使用深度可分离卷积</label>
												<label><input type="radio" name="arch_selection_q112" value="C"> C. 使用更复杂的激活函数</label>
												<label><input type="radio" name="arch_selection_q112" value="D"> D. 使用更深的网络</label>
											</div>
										</div>
										<!-- 题目 113 -->
										<div class="question">
											<p><strong>113. "迁移学习"在深度学习中的常见做法是：</strong></p>
											<div class="options">
												<label><input type="radio" name="arch_selection_q113" value="A"> A. 在一个大数据集上预训练模型，然后在小数据集上微调</label>
												<label><input type="radio" name="arch_selection_q113" value="B"> B. 随机初始化权重，直接在小数据集上训练</label>
												<label><input type="radio" name="arch_selection_q113" value="C"> C. 只使用无标签数据训练</label>
												<label><input type="radio" name="arch_selection_q113" value="D"> D. 不断改变网络结构</label>
											</div>
										</div>
										<!-- 题目 114 -->
										<div class="question">
											<p><strong>114. 当处理的数据集非常小（如几百张图片）时，以下哪种策略最可能有效？</strong></p>
											<div class="options">
												<label><input type="radio" name="arch_selection_q114" value="A"> A. 从头开始训练一个很深的ResNet</label>
												<label><input type="radio" name="arch_selection_q114" value="B"> B. 使用迁移学习，并可能进行数据增强</label>
												<label><input type="radio" name="arch_selection_q114" value="C"> C. 使用更复杂的模型</label>
												<label><input type="radio" name="arch_selection_q114" value="D"> D. 增加更多的全连接层</label>
											</div>
										</div>
										<!-- 题目 115 -->
										<div class="question">
											<p><strong>115. 模型集成（Ensemble）通常能提升性能，因为它：</strong></p>
											<div class="options">
												<label><input type="radio" name="arch_selection_q115" value="A"> A. 减少了单个模型的偏差（Bias）</label>
												<label><input type="radio" name="arch_selection_q115" value="B"> B. 减少了单个模型的方差（Variance）</label>
												<label><input type="radio" name="arch_selection_q115" value="C"> C. 同时减少了偏差和方差</label>
												<label><input type="radio" name="arch_selection_q115" value="D"> D. 只是让模型变得更大</label>
											</div>
										</div>
										<!-- 题目 116 -->
										<div class="question">
											<p><strong>116. 以下哪种现象可能表明学习率设置得太高？</strong></p>
											<div class="options">
												<label><input type="radio" name="arch_selection_q116" value="A"> A. 训练损失缓慢下降</label>
												<label><input type="radio" name="arch_selection_q116" value="B"> B. 训练损失震荡剧烈，甚至发散（NaN）</label>
												<label><input type="radio" name="arch_selection_q116" value="C"> C. 验证准确率稳步提升</label>
												<label><input type="radio" name="arch_selection_q116" value="D"> D. 模型很快收敛到一个很差的局部最优解</label>
											</div>
										</div>
										<!-- 题目 117 -->
										<div class="question">
											<p><strong>117. 早停法（Early Stopping）是一种正则化技术，它通过什么来防止过拟合？</strong></p>
											<div class="options">
												<label><input type="radio" name="arch_selection_q117" value="A"> A. 在损失函数中添加惩罚项</label>
												<label><input type="radio" name="arch_selection_q117" value="B"> B. 随机丢弃神经元</label>
												<label><input type="radio" name="arch_selection_q117" value="C"> C. 在验证集性能开始下降时停止训练</label>
												<label><input type="radio" name="arch_selection_q117" value="D"> D. 对输入数据进行标准化</label>
											</div>
										</div>
										<!-- 题目 118 -->
										<div class="question">
											<p><strong>118. 批标准化（BatchNorm）和丢弃法（Dropout）一起使用时，通常的顺序是？</strong></p>
											<div class="options">
												<label><input type="radio" name="arch_selection_q118" value="A"> A. Dropout -> BatchNorm -> 激活函数</label>
												<label><input type="radio" name="arch_selection_q118" value="B"> B. 卷积层 -> BatchNorm -> 激活函数 -> Dropout</label>
												<label><input type="radio" name="arch_selection_q118" value="C"> C. 卷积层 -> Dropout -> BatchNorm -> 激活函数</label>
												<label><input type="radio" name="arch_selection_q118" value="D"> D. 顺序无关紧要</label>
											</div>
										</div>
										<!-- 题目 119 -->
										<div class="question">
											<p><strong>119. 对于自然语言处理任务，词嵌入（Word Embedding）层的作用是：</strong></p>
											<div class="options">
												<label><input type="radio" name="arch_selection_q119" value="A"> A. 将高维的one-hot词向量映射到低维的稠密向量</label>
												<label><input type="radio" name="arch_selection_q119" value="B"> B. 提取图像的边缘特征</label>
												<label><input type="radio" name="arch_selection_q119" value="C"> C. 进行池化操作</label>
												<label><input type="radio" name="arch_selection_q119" value="D"> D. 计算注意力分数</label>
											</div>
										</div>
										<!-- 题目 120 -->
										<div class="question">
											<p><strong>120. 胶囊网络（Capsule Network）试图解决传统CNN的什么局限性？</strong></p>
											<div class="options">
												<label><input type="radio" name="arch_selection_q120" value="A"> A. 计算速度慢</label>
												<label><input type="radio" name="arch_selection_q120" value="B"> B. 对物体的空间层次关系（如部件之间的相对位置和姿态）建模能力弱</label>
												<label><input type="radio" name="arch_selection_q120" value="C"> C. 无法处理序列数据</label>
												<label><input type="radio" name="arch_selection_q120" value="D"> D. 参数过多</label>
											</div>
										</div>
										<!-- 得分显示和提交按钮 -->
										<div class="score-display"></div>
										<button class="submit-quiz-btn" data-category="arch_selection_quiz">提交答案</button>
										<button class="close-exercise-btn">收起练习</button>
									</div>
								</div>
							</div>
						</div>
					
						<!-- 综合应用：问题诊断与调优（121-135题） -->
						<div id="exerciseProblemDiagnosis" style="display: block;">
							<div class="exercise-section-title">
								<div class="title-header">
									<span class="title-tag">综合应用模块</span>
									<span class="title-icon">🔧</span>
								</div>
								<h2 class="title-name">问题诊断与调优</h2>
								<div class="title-info">
									<span class="info-item">
										<i class="icon-question"></i>
										15道题目
									</span>
									<span class="info-item">
										<i class="icon-clock"></i>
										题号：121-135
									</span>
								</div>
							</div>
							<div class="exercise-section-content">
								<div class="exercise-card" data-exercise="problem_diagnosis_quiz">
									<div class="card-content">
										<p>检测你对深度学习模型问题诊断和调优的理解。</p>
										<button class="start-exercise-btn">开始练习</button>
									</div>
									<div class="card-questions" style="display:none;">
										<!-- 题目 121 -->
										<div class="question">
											<p><strong>121. 训练一个CNN时，发现训练准确率很高（99%），但验证准确率很低（60%）。最可能的原因是？</strong></p>
											<div class="options">
												<label><input type="radio" name="problem_diagnosis_q121" value="A"> A. 欠拟合</label>
												<label><input type="radio" name="problem_diagnosis_q121" value="B"> B. 过拟合</label>
												<label><input type="radio" name="problem_diagnosis_q121" value="C"> C. 学习率太低</label>
												<label><input type="radio" name="problem_diagnosis_q121" value="D"> D. 批次大小太小</label>
											</div>
										</div>
										<!-- 题目 122 -->
										<div class="question">
											<p><strong>122. 如果模型的训练损失和验证损失都很高且很接近，这通常意味着？</strong></p>
											<div class="options">
												<label><input type="radio" name="problem_diagnosis_q122" value="A"> A. 过拟合</label>
												<label><input type="radio" name="problem_diagnosis_q122" value="B"> B. 欠拟合</label>
												<label><input type="radio" name="problem_diagnosis_q122" value="C"> C. 训练正好</label>
												<label><input type="radio" name="problem_diagnosis_q122" value="D"> D. 梯度爆炸</label>
											</div>
										</div>
										<!-- 题目 123 -->
										<div class="question">
											<p><strong>123. 训练过程中损失值突然变成NaN，首先应该检查？</strong></p>
											<div class="options">
												<label><input type="radio" name="problem_diagnosis_q123" value="A"> A. 学习率是否太高</label>
												<label><input type="radio" name="problem_diagnosis_q123" value="B"> B. 数据中是否存在异常值或未归一化</label>
												<label><input type="radio" name="problem_diagnosis_q123" value="C"> C. 网络结构是否太深</label>
												<label><input type="radio" name="problem_diagnosis_q123" value="D"> D. A和B</label>
											</div>
										</div>
										<!-- 题目 124 -->
										<div class="question">
											<p><strong>124. 使用ReLU激活函数的深层网络，如果很多神经元的输出持续为0，这种现象称为？</strong></p>
											<div class="options">
												<label><input type="radio" name="problem_diagnosis_q124" value="A"> A. 梯度爆炸</label>
												<label><input type="radio" name="problem_diagnosis_q124" value="B"> B. 神经元死亡</label>
												<label><input type="radio" name="problem_diagnosis_q124" value="C"> C. 过拟合</label>
												<label><input type="radio" name="problem_diagnosis_q124" value="D"> D. 欠拟合</label>
											</div>
										</div>
										<!-- 题目 125 -->
										<div class="question">
											<p><strong>125. 为了缓解梯度消失问题，可以采取的措施不包括：</strong></p>
											<div class="options">
												<label><input type="radio" name="problem_diagnosis_q125" value="A"> A. 使用ReLU等非饱和激活函数</label>
												<label><input type="radio" name="problem_diagnosis_q125" value="B"> B. 使用残差连接</label>
												<label><input type="radio" name="problem_diagnosis_q125" value="C"> C. 使用梯度裁剪（Gradient Clipping）</label>
												<label><input type="radio" name="problem_diagnosis_q125" value="D"> D. 使用Batch Normalization</label>
											</div>
										</div>
										<!-- 题目 126 -->
										<div class="question">
											<p><strong>126. 当增加网络深度时，如果性能不再提升甚至下降，除了梯度问题，还可能是因为？</strong></p>
											<div class="options">
												<label><input type="radio" name="problem_diagnosis_q126" value="A"> A. 模型容量过大</label>
												<label><input type="radio" name="problem_diagnosis_q126" value="B"> B. 恒等映射变得难以学习</label>
												<label><input type="radio" name="problem_diagnosis_q126" value="C"> C. 学习率太小</label>
												<label><input type="radio" name="problem_diagnosis_q126" value="D"> D. 数据太少</label>
											</div>
										</div>
										<!-- 题目 127 -->
										<div class="question">
											<p><strong>127. 在训练Transformer时，通常使用哪种优化器？</strong></p>
											<div class="options">
												<label><input type="radio" name="problem_diagnosis_q127" value="A"> A. SGD（随机梯度下降）</label>
												<label><input type="radio" name="problem_diagnosis_q127" value="B"> B. SGD with Momentum</label>
												<label><input type="radio" name="problem_diagnosis_q127" value="C"> C. Adam 或 AdamW</label>
												<label><input type="radio" name="problem_diagnosis_q127" value="D"> D. Adagrad</label>
											</div>
										</div>
										<!-- 题目 128 -->
										<div class="question">
											<p><strong>128. 训练时使用更大的批量大小（Batch Size）可能导致：</strong></p>
											<div class="options">
												<label><input type="radio" name="problem_diagnosis_q128" value="A"> A. 更稳定的梯度估计，可能允许更大的学习率</label>
												<label><input type="radio" name="problem_diagnosis_q128" value="B"> B. 更快的模型收敛</label>
												<label><input type="radio" name="problem_diagnosis_q128" value="C"> C. 需要更多的GPU内存</label>
												<label><input type="radio" name="problem_diagnosis_q128" value="D"> D. 以上都是</label>
											</div>
										</div>
										<!-- 题目 129 -->
										<div class="question">
											<p><strong>129. 权重衰减（Weight Decay）是一种正则化技术，它通过在损失函数中添加什么来实现？</strong></p>
											<div class="options">
												<label><input type="radio" name="problem_diagnosis_q129" value="A"> A. 权重的L1范数</label>
												<label><input type="radio" name="problem_diagnosis_q129" value="B"> B. 权重的L2范数</label>
												<label><input type="radio" name="problem_diagnosis_q129" value="C"> C. 激活值的范数</label>
												<label><input type="radio" name="problem_diagnosis_q129" value="D"> D. 输入的范数</label>
											</div>
										</div>
										<!-- 题目 130 -->
										<div class="question">
											<p><strong>130. 当使用预训练语言模型（如BERT）进行文本分类微调时，通常的做法是：</strong></p>
											<div class="options">
												<label><input type="radio" name="problem_diagnosis_q130" value="A"> A. 冻结所有预训练层，只训练新添加的分类头</label>
												<label><input type="radio" name="problem_diagnosis_q130" value="B"> B. 解冻所有层，全部重新训练</label>
												<label><input type="radio" name="problem_diagnosis_q130" value="C"> C. 解冻最后几层和分类头进行训练，保持底层冻结</label>
												<label><input type="radio" name="problem_diagnosis_q130" value="D"> D. B或C都是常见做法</label>
											</div>
										</div>
										<!-- 题目 131 -->
										<div class="question">
											<p><strong>131. 知识蒸馏（Knowledge Distillation）的主要思想是：</strong></p>
											<div class="options">
												<label><input type="radio" name="problem_diagnosis_q131" value="A"> A. 用一个大的"教师模型"来指导一个小的"学生模型"的训练</label>
												<label><input type="radio" name="problem_diagnosis_q131" value="B"> B. 让多个模型投票决定预测结果</label>
												<label><input type="radio" name="problem_diagnosis_q131" value="C"> C. 将模型的知识存储到数据库中</label>
												<label><input type="radio" name="problem_diagnosis_q131" value="D"> D. 增加模型的深度和宽度</label>
											</div>
										</div>
										<!-- 题目 132 -->
										<div class="question">
											<p><strong>132. 神经架构搜索（Neural Architecture Search, NAS）的目标是：</strong></p>
											<div class="options">
												<label><input type="radio" name="problem_diagnosis_q132" value="A"> A. 自动寻找最优的网络结构</label>
												<label><input type="radio" name="problem_diagnosis_q132" value="B"> B. 自动调整超参数（如学习率）</label>
												<label><input type="radio" name="problem_diagnosis_q132" value="C"> C. 自动进行数据增强</label>
												<label><input type="radio" name="problem_diagnosis_q132" value="D"> D. 自动标注数据</label>
											</div>
										</div>
										<!-- 题目 133 -->
										<div class="question">
											<p><strong>133. 混合精度训练（Mixed Precision Training）的主要好处是：</strong></p>
											<div class="options">
												<label><input type="radio" name="problem_diagnosis_q133" value="A"> A. 减少模型大小</label>
												<label><input type="radio" name="problem_diagnosis_q133" value="B"> B. 减少GPU内存占用并加快训练速度</label>
												<label><input type="radio" name="problem_diagnosis_q133" value="C"> C. 提高模型精度</label>
												<label><input type="radio" name="problem_diagnosis_q133" value="D"> D. 让模型更容易收敛</label>
											</div>
										</div>
										<!-- 题目 134 -->
										<div class="question">
											<p><strong>134. 标签平滑（Label Smoothing）被用来缓解什么问题？</strong></p>
											<div class="options">
												<label><input type="radio" name="problem_diagnosis_q134" value="A"> A. 类别不平衡</label>
												<label><input type="radio" name="problem_diagnosis_q134" value="B"> B. 模型对预测结果过于自信（过度拟合硬标签）</label>
												<label><input type="radio" name="problem_diagnosis_q134" value="C"> C. 梯度消失</label>
												<label><input type="radio" name="problem_diagnosis_q134" value="D"> D. 训练速度慢</label>
											</div>
										</div>
										<!-- 题目 135 -->
										<div class="question">
											<p><strong>135. 数据集存在严重的类别不平衡时，以下哪种方法可能无效？</strong></p>
											<div class="options">
												<label><input type="radio" name="problem_diagnosis_q135" value="A"> A. 对少数类进行过采样（如SMOTE）</label>
												<label><input type="radio" name="problem_diagnosis_q135" value="B"> B. 对多数类进行欠采样</label>
												<label><input type="radio" name="problem_diagnosis_q135" value="C"> C. 在损失函数中使用类别权重</label>
												<label><input type="radio" name="problem_diagnosis_q135" value="D"> D. 简单地收集更多数据但不平衡类别比例</label>
											</div>
										</div>
										<!-- 得分显示和提交按钮 -->
										<div class="score-display"></div>
										<button class="submit-quiz-btn" data-category="problem_diagnosis_quiz">提交答案</button>
										<button class="close-exercise-btn">收起练习</button>
									</div>
								</div>
							</div>
						</div>
					
						<!-- 综合应用：前沿与交叉领域（136-155题） -->
						<div id="exerciseAdvanced" style="display: block;">
							<div class="exercise-section-title">
								<div class="title-header">
									<span class="title-tag">综合应用模块</span>
									<span class="title-icon">📚</span>
								</div>
								<h2 class="title-name">前沿与交叉领域</h2>
								<div class="title-info">
									<span class="info-item">
										<i class="icon-question"></i>
										20道题目
									</span>
									<span class="info-item">
										<i class="icon-clock"></i>
										题号：136-155
									</span>
								</div>
							</div>
							<div class="exercise-section-content">
								<div class="exercise-card" data-exercise="advanced_quiz">
									<div class="card-content">
										<p>检测你对深度学习前沿与交叉领域的理解。</p>
										<button class="start-exercise-btn">开始练习</button>
									</div>
									<div class="card-questions" style="display:none;">
										<!-- 题目 136 -->
										<div class="question">
											<p><strong>136. 生成对抗网络（GAN）由哪两部分组成？</strong></p>
											<div class="options">
												<label><input type="radio" name="advanced_q136" value="A"> A. 编码器和解码器</label>
												<label><input type="radio" name="advanced_q136" value="B"> B. 生成器和判别器</label>
												<label><input type="radio" name="advanced_q136" value="C"> C. 教师和学生</label>
												<label><input type="radio" name="advanced_q136" value="D"> D. 主角和配角</label>
											</div>
										</div>
										<!-- 题目 137 -->
										<div class="question">
											<p><strong>137. 自监督学习（Self-Supervised Learning）的主要特点是：</strong></p>
											<div class="options">
												<label><input type="radio" name="advanced_q137" value="A"> A. 完全不需要数据</label>
												<label><input type="radio" name="advanced_q137" value="B"> B. 从无标签数据中自动构造监督信号进行学习</label>
												<label><input type="radio" name="advanced_q137" value="C"> C. 只需要少量标签</label>
												<label><input type="radio" name="advanced_q137" value="D"> D. 只使用强化学习信号</label>
											</div>
										</div>
										<!-- 题目 138 -->
										<div class="question">
											<p><strong>138. 对比学习（Contrastive Learning）的核心思想是：</strong></p>
											<div class="options">
												<label><input type="radio" name="advanced_q138" value="A"> A. 拉近相似样本的表示，推远不相似样本的表示</label>
												<label><input type="radio" name="advanced_q138" value="B"> B. 最大化所有样本表示之间的相似度</label>
												<label><input type="radio" name="advanced_q138" value="C"> C. 最小化预测误差</label>
												<label><input type="radio" name="advanced_q138" value="D"> D. 生成新的数据样本</label>
											</div>
										</div>
										<!-- 题目 139 -->
										<div class="question">
											<p><strong>139. 图神经网络（GNN）专门用于处理什么类型的数据？</strong></p>
											<div class="options">
												<label><input type="radio" name="advanced_q139" value="A"> A. 规则网格数据（如图像）</label>
												<label><input type="radio" name="advanced_q139" value="B"> B. 序列数据</label>
												<label><input type="radio" name="advanced_q139" value="C"> C. 图结构数据（社交网络、分子结构）</label>
												<label><input type="radio" name="advanced_q139" value="D"> D. 文本数据</label>
											</div>
										</div>
										<!-- 题目 140 -->
										<div class="question">
											<p><strong>140. 神经辐射场（NeRF）主要用于哪个领域？</strong></p>
											<div class="options">
												<label><input type="radio" name="advanced_q140" value="A"> A. 自然语言处理</label>
												<label><input type="radio" name="advanced_q140" value="B"> B. 三维场景重建与新视角合成</label>
												<label><input type="radio" name="advanced_q140" value="C"> C. 时间序列预测</label>
												<label><input type="radio" name="advanced_q140" value="D"> D. 语音合成</label>
											</div>
										</div>
										<!-- 题目 141 -->
										<div class="question">
											<p><strong>141. 扩散模型（Diffusion Models）的图像生成过程可以描述为：</strong></p>
											<div class="options">
												<label><input type="radio" name="advanced_q141" value="A"> A. 一步到位地从噪声生成图像</label>
												<label><input type="radio" name="advanced_q141" value="B"> B. 一个逐步去噪的过程，从随机噪声逐步重建出清晰图像</label>
												<label><input type="radio" name="advanced_q141" value="C"> C. 一个逐步加噪的过程，从清晰图像变为噪声</label>
												<label><input type="radio" name="advanced_q141" value="D"> D. 一个对抗博弈的过程</label>
											</div>
										</div>
										<!-- 题目 142 -->
										<div class="question">
											<p><strong>142. 模型量化（Model Quantization）的主要目的是？</strong></p>
											<div class="options">
												<label><input type="radio" name="advanced_q142" value="A"> A. 提高模型精度</label>
												<label><input type="radio" name="advanced_q142" value="B"> B. 减少模型存储空间和加速推理</label>
												<label><input type="radio" name="advanced_q142" value="C"> C. 让模型更容易训练</label>
												<label><input type="radio" name="advanced_q142" value="D"> D. 增加模型非线性</label>
											</div>
										</div>
										<!-- 题目 143 -->
										<div class="question">
											<p><strong>143. 终身学习（Lifelong Learning）或持续学习，试图解决的主要挑战是：</strong></p>
											<div class="options">
												<label><input type="radio" name="advanced_q143" value="A"> A. 模型在学会新任务时，会灾难性地忘记旧任务</label>
												<label><input type="radio" name="advanced_q143" value="B"> B. 模型训练速度太慢</label>
												<label><input type="radio" name="advanced_q143" value="C"> C. 模型无法处理小数据</label>
												<label><input type="radio" name="advanced_q143" value="D"> D. 模型解释性差</label>
											</div>
										</div>
										<!-- 题目 144 -->
										<div class="question">
											<p><strong>144. 可解释人工智能（XAI）的目标是：</strong></p>
											<div class="options">
												<label><input type="radio" name="advanced_q144" value="A"> A. 让模型跑得更快</label>
												<label><input type="radio" name="advanced_q144" value="B"> B. 让人类理解模型的决策依据</label>
												<label><input type="radio" name="advanced_q144" value="C"> C. 让模型变得更复杂</label>
												<label><input type="radio" name="advanced_q144" value="D"> D. 自动设计模型架构</label>
											</div>
										</div>
										<!-- 题目 145 -->
										<div class="question">
											<p><strong>145. 联邦学习（Federated Learning）的主要优势在于：</strong></p>
											<div class="options">
												<label><input type="radio" name="advanced_q145" value="A"> A. 集中所有数据进行训练，获得最佳性能</label>
												<label><input type="radio" name="advanced_q145" value="B"> B. 在保护数据隐私的前提下，实现协同模型训练</label>
												<label><input type="radio" name="advanced_q145" value="C"> C. 训练速度最快</label>
												<label><input type="radio" name="advanced_q145" value="D"> D. 模型结构最简单</label>
											</div>
										</div>
										<!-- 题目 146 -->
										<div class="question">
											<p><strong>146. 元学习（Meta-Learning）或"学会学习"，其目标是：</strong></p>
											<div class="options">
												<label><input type="radio" name="advanced_q146" value="A"> A. 快速适应新任务，只需少量样本</label>
												<label><input type="radio" name="advanced_q146" value="B"> B. 一次性记住所有任务</label>
												<label><input type="radio" name="advanced_q146" value="C"> C. 训练一个通用的、不做任何调整的模型</label>
												<label><input type="radio" name="advanced_q146" value="D"> D. 避免使用梯度下降</label>
											</div>
										</div>
										<!-- 题目 147 -->
										<div class="question">
											<p><strong>147. Transformer的注意力机制计算复杂度相对于序列长度n是？</strong></p>
											<div class="options">
												<label><input type="radio" name="advanced_q147" value="A"> A. O(n)</label>
												<label><input type="radio" name="advanced_q147" value="B"> B. O(n log n)</label>
												<label><input type="radio" name="advanced_q147" value="C"> C. O(n²)</label>
												<label><input type="radio" name="advanced_q147" value="D"> D. O(1)</label>
											</div>
										</div>
										<!-- 题目 148 -->
										<div class="question">
											<p><strong>148. 为了降低Transformer处理长序列的计算开销，以下哪种不是有效方法？</strong></p>
											<div class="options">
												<label><input type="radio" name="advanced_q148" value="A"> A. 使用局部窗口注意力</label>
												<label><input type="radio" name="advanced_q148" value="B"> B. 使用线性注意力（Linear Attention）</label>
												<label><input type="radio" name="advanced_q148" value="C"> C. 将序列长度加倍</label>
												<label><input type="radio" name="advanced_q148" value="D"> D. 使用轴向注意力（Axial Attention）</label>
											</div>
										</div>
										<!-- 题目 149 -->
										<div class="question">
											<p><strong>149. 多模态学习（Multimodal Learning）是指：</strong></p>
											<div class="options">
												<label><input type="radio" name="advanced_q149" value="A"> A. 只使用一种类型的数据</label>
												<label><input type="radio" name="advanced_q149" value="B"> B. 联合利用多种类型的数据（如图像、文本、音频）进行学习</label>
												<label><input type="radio" name="advanced_q149" value="C"> C. 使用多个相同的模型</label>
												<label><input type="radio" name="advanced_q149" value="D"> D. 在不同的模式下训练模型</label>
											</div>
										</div>
										<!-- 题目 150 -->
										<div class="question">
											<p><strong>150. 强化学习（Reinforcement Learning）中，智能体通过什么来学习？</strong></p>
											<div class="options">
												<label><input type="radio" name="advanced_q150" value="A"> A. 带标签的训练数据</label>
												<label><input type="radio" name="advanced_q150" value="B"> B. 与环境的交互获得的奖励信号</label>
												<label><input type="radio" name="advanced_q150" value="C"> C. 无标签的数据聚类</label>
												<label><input type="radio" name="advanced_q150" value="D"> D. 教师的示范</label>
											</div>
										</div>
										<!-- 题目 151 -->
										<div class="question">
											<p><strong>151. 在深度学习与强化学习的结合中，深度Q网络（DQN）使用什么来近似Q值函数？</strong></p>
											<div class="options">
												<label><input type="radio" name="advanced_q151" value="A"> A. 线性函数</label>
												<label><input type="radio" name="advanced_q151" value="B"> B. 决策树</label>
												<label><input type="radio" name="advanced_q151" value="C"> C. 深度神经网络</label>
												<label><input type="radio" name="advanced_q151" value="D"> D. 支持向量机</label>
											</div>
										</div>
										<!-- 题目 152 -->
										<div class="question">
											<p><strong>152. 稀疏注意力（Sparse Attention）和门控机制的结合，在哪个知名模型中得到了体现？</strong></p>
											<div class="options">
												<label><input type="radio" name="advanced_q152" value="A"> A. ResNet</label>
												<label><input type="radio" name="advanced_q152" value="B"> B. LSTM</label>
												<label><input type="radio" name="advanced_q152" value="C"> C. GPT-3</label>
												<label><input type="radio" name="advanced_q152" value="D"> D. Switch Transformer</label>
											</div>
										</div>
										<!-- 题目 153 -->
										<div class="question">
											<p><strong>153. 自回归模型（如GPT）和自编码模型（如BERT）在预训练目标上的主要区别是？</strong></p>
											<div class="options">
												<label><input type="radio" name="advanced_q153" value="A"> A. 自回归模型预测未来token，自编码模型重建被掩码的token</label>
												<label><input type="radio" name="advanced_q153" value="B"> B. 自回归模型使用卷积，自编码模型使用注意力</label>
												<label><input type="radio" name="advanced_q153" value="C"> C. 自回归模型更小，自编码模型更大</label>
												<label><input type="radio" name="advanced_q153" value="D"> D. 没有区别</label>
											</div>
										</div>
										<!-- 题目 154 -->
										<div class="question">
											<p><strong>154. 大语言模型（LLM）出现"幻觉"（Hallucination）问题是指？</strong></p>
											<div class="options">
												<label><input type="radio" name="advanced_q154" value="A"> A. 模型产生与输入无关或事实错误的文本</label>
												<label><input type="radio" name="advanced_q154" value="B"> B. 模型运行速度变慢</label>
												<label><input type="radio" name="advanced_q154" value="C"> C. 模型无法理解指令</label>
												<label><input type="radio" name="advanced_q154" value="D"> D. 模型参数量太大</label>
											</div>
										</div>
										<!-- 题目 155 -->
										<div class="question">
											<p><strong>155. MoE（Mixture of Experts）架构的核心优势是？</strong></p>
											<div class="options">
												<label><input type="radio" name="advanced_q155" value="A"> A. 用极少的计算量激活全部参数</label>
												<label><input type="radio" name="advanced_q155" value="B"> B. 用少量计算激活大量参数，实现模型容量与计算效率的分离</label>
												<label><input type="radio" name="advanced_q155" value="C"> C. 让模型完全串行计算</label>
												<label><input type="radio" name="advanced_q155" value="D"> D. 消除了所有参数</label>
											</div>
										</div>
										<!-- 得分显示和提交按钮 -->
										<div class="score-display"></div>
										<button class="submit-quiz-btn" data-category="advanced_quiz">提交答案</button>
										<button class="close-exercise-btn">收起练习</button>
									</div>
								</div>
							</div>
						</div>
					
					</div>
				</div>

					
				<div class="exercise-right-status">
					<div class="status-header">
					<h3>📊 答题进度</h3>
					<div class="progress-indicator">
						<div class="progress-text">当前进度</div>
						<div class="progress-number">0/155</div>
					</div>
					</div>
					
					<div class="status-content">
					<!-- 基础组件模块进度 -->
					<div class="status-category">
						<h4>🧩 基础组件模块 (50题)</h4>
						
						<div class="status-subcategory-container">
						<!-- 神经网络层：1-30题 -->
						<div class="status-subcategory">
							<h5>神经网络层 (30题)</h5>
							<div class="status-grid">
							<div class="status-item unanswered" data-question="1" data-tooltip="神经网络层 - 题目1"></div>
							<div class="status-item unanswered" data-question="2" data-tooltip="神经网络层 - 题目2"></div>
							<div class="status-item unanswered" data-question="3" data-tooltip="神经网络层 - 题目3"></div>
							<div class="status-item unanswered" data-question="4" data-tooltip="神经网络层 - 题目4"></div>
							<div class="status-item unanswered" data-question="5" data-tooltip="神经网络层 - 题目5"></div>
							<div class="status-item unanswered" data-question="6" data-tooltip="神经网络层 - 题目6"></div>
							<div class="status-item unanswered" data-question="7" data-tooltip="神经网络层 - 题目7"></div>
							<div class="status-item unanswered" data-question="8" data-tooltip="神经网络层 - 题目8"></div>
							<div class="status-item unanswered" data-question="9" data-tooltip="神经网络层 - 题目9"></div>
							<div class="status-item unanswered" data-question="10" data-tooltip="神经网络层 - 题目10"></div>
							<div class="status-item unanswered" data-question="11" data-tooltip="神经网络层 - 题目11"></div>
							<div class="status-item unanswered" data-question="12" data-tooltip="神经网络层 - 题目12"></div>
							<div class="status-item unanswered" data-question="13" data-tooltip="神经网络层 - 题目13"></div>
							<div class="status-item unanswered" data-question="14" data-tooltip="神经网络层 - 题目14"></div>
							<div class="status-item unanswered" data-question="15" data-tooltip="神经网络层 - 题目15"></div>
							<div class="status-item unanswered" data-question="16" data-tooltip="神经网络层 - 题目16"></div>
							<div class="status-item unanswered" data-question="17" data-tooltip="神经网络层 - 题目17"></div>
							<div class="status-item unanswered" data-question="18" data-tooltip="神经网络层 - 题目18"></div>
							<div class="status-item unanswered" data-question="19" data-tooltip="神经网络层 - 题目19"></div>
							<div class="status-item unanswered" data-question="20" data-tooltip="神经网络层 - 题目20"></div>
							<div class="status-item unanswered" data-question="21" data-tooltip="神经网络层 - 题目21"></div>
							<div class="status-item unanswered" data-question="22" data-tooltip="神经网络层 - 题目22"></div>
							<div class="status-item unanswered" data-question="23" data-tooltip="神经网络层 - 题目23"></div>
							<div class="status-item unanswered" data-question="24" data-tooltip="神经网络层 - 题目24"></div>
							<div class="status-item unanswered" data-question="25" data-tooltip="神经网络层 - 题目25"></div>
							<div class="status-item unanswered" data-question="26" data-tooltip="神经网络层 - 题目26"></div>
							<div class="status-item unanswered" data-question="27" data-tooltip="神经网络层 - 题目27"></div>
							<div class="status-item unanswered" data-question="28" data-tooltip="神经网络层 - 题目28"></div>
							<div class="status-item unanswered" data-question="29" data-tooltip="神经网络层 - 题目29"></div>
							<div class="status-item unanswered" data-question="30" data-tooltip="神经网络层 - 题目30"></div>
							</div>
							<div class="module-stats">
							<span class="unanswered-count">未答: 30</span>
							<span class="correct-count">正确: 0</span>
							<span class="incorrect-count">错误: 0</span>
							</div>
						</div>
						
						<!-- 激活函数：31-50题 -->
						<div class="status-subcategory">
							<h5>⚡ 激活函数 (20题)</h5>
							<div class="status-grid">
							<div class="status-item unanswered" data-question="31" data-tooltip="激活函数 - 题目31"></div>
							<div class="status-item unanswered" data-question="32" data-tooltip="激活函数 - 题目32"></div>
							<div class="status-item unanswered" data-question="33" data-tooltip="激活函数 - 题目33"></div>
							<div class="status-item unanswered" data-question="34" data-tooltip="激活函数 - 题目34"></div>
							<div class="status-item unanswered" data-question="35" data-tooltip="激活函数 - 题目35"></div>
							<div class="status-item unanswered" data-question="36" data-tooltip="激活函数 - 题目36"></div>
							<div class="status-item unanswered" data-question="37" data-tooltip="激活函数 - 题目37"></div>
							<div class="status-item unanswered" data-question="38" data-tooltip="激活函数 - 题目38"></div>
							<div class="status-item unanswered" data-question="39" data-tooltip="激活函数 - 题目39"></div>
							<div class="status-item unanswered" data-question="40" data-tooltip="激活函数 - 题目40"></div>
							<div class="status-item unanswered" data-question="41" data-tooltip="激活函数 - 题目41"></div>
							<div class="status-item unanswered" data-question="42" data-tooltip="激活函数 - 题目42"></div>
							<div class="status-item unanswered" data-question="43" data-tooltip="激活函数 - 题目43"></div>
							<div class="status-item unanswered" data-question="44" data-tooltip="激活函数 - 题目44"></div>
							<div class="status-item unanswered" data-question="45" data-tooltip="激活函数 - 题目45"></div>
							<div class="status-item unanswered" data-question="46" data-tooltip="激活函数 - 题目46"></div>
							<div class="status-item unanswered" data-question="47" data-tooltip="激活函数 - 题目47"></div>
							<div class="status-item unanswered" data-question="48" data-tooltip="激活函数 - 题目48"></div>
							<div class="status-item unanswered" data-question="49" data-tooltip="激活函数 - 题目49"></div>
							<div class="status-item unanswered" data-question="50" data-tooltip="激活函数 - 题目50"></div>
							</div>
							<div class="module-stats">
							<span class="unanswered-count">未答: 20</span>
							<span class="correct-count">正确: 0</span>
							<span class="incorrect-count">错误: 0</span>
							</div>
						</div>
						</div>
					</div>
				
					<!-- 核心架构模块进度 -->
					<div class="status-category">
						<h4>🏗️ 核心架构模块 (50题)</h4>
						
						<div class="status-subcategory-container">
						<!-- 感知机：51-60题 -->
						<div class="status-subcategory">
							<h5>🧠 感知机 (10题)</h5>
							<div class="status-grid">
							<div class="status-item unanswered" data-question="51" data-tooltip="感知机 - 题目51"></div>
							<div class="status-item unanswered" data-question="52" data-tooltip="感知机 - 题目52"></div>
							<div class="status-item unanswered" data-question="53" data-tooltip="感知机 - 题目53"></div>
							<div class="status-item unanswered" data-question="54" data-tooltip="感知机 - 题目54"></div>
							<div class="status-item unanswered" data-question="55" data-tooltip="感知机 - 题目55"></div>
							<div class="status-item unanswered" data-question="56" data-tooltip="感知机 - 题目56"></div>
							<div class="status-item unanswered" data-question="57" data-tooltip="感知机 - 题目57"></div>
							<div class="status-item unanswered" data-question="58" data-tooltip="感知机 - 题目58"></div>
							<div class="status-item unanswered" data-question="59" data-tooltip="感知机 - 题目59"></div>
							<div class="status-item unanswered" data-question="60" data-tooltip="感知机 - 题目60"></div>
							</div>
							<div class="module-stats">
							<span class="unanswered-count">未答: 10</span>
							<span class="correct-count">正确: 0</span>
							<span class="incorrect-count">错误: 0</span>
							</div>
						</div>
						
						<!-- CNN：61-75题 -->
						<div class="status-subcategory">
							<h5>🔍 CNN (15题)</h5>
							<div class="status-grid">
							<div class="status-item unanswered" data-question="61" data-tooltip="CNN - 题目61"></div>
							<div class="status-item unanswered" data-question="62" data-tooltip="CNN - 题目62"></div>
							<div class="status-item unanswered" data-question="63" data-tooltip="CNN - 题目63"></div>
							<div class="status-item unanswered" data-question="64" data-tooltip="CNN - 题目64"></div>
							<div class="status-item unanswered" data-question="65" data-tooltip="CNN - 题目65"></div>
							<div class="status-item unanswered" data-question="66" data-tooltip="CNN - 题目66"></div>
							<div class="status-item unanswered" data-question="67" data-tooltip="CNN - 题目67"></div>
							<div class="status-item unanswered" data-question="68" data-tooltip="CNN - 题目68"></div>
							<div class="status-item unanswered" data-question="69" data-tooltip="CNN - 题目69"></div>
							<div class="status-item unanswered" data-question="70" data-tooltip="CNN - 题目70"></div>
							<div class="status-item unanswered" data-question="71" data-tooltip="CNN - 题目71"></div>
							<div class="status-item unanswered" data-question="72" data-tooltip="CNN - 题目72"></div>
							<div class="status-item unanswered" data-question="73" data-tooltip="CNN - 题目73"></div>
							<div class="status-item unanswered" data-question="74" data-tooltip="CNN - 题目74"></div>
							<div class="status-item unanswered" data-question="75" data-tooltip="CNN - 题目75"></div>
							</div>
							<div class="module-stats">
							<span class="unanswered-count">未答: 15</span>
							<span class="correct-count">正确: 0</span>
							<span class="incorrect-count">错误: 0</span>
							</div>
						</div>
						
						<!-- RNN：76-90题 -->
						<div class="status-subcategory">
							<h5>🔄 RNN (15题)</h5>
							<div class="status-grid">
							<div class="status-item unanswered" data-question="76" data-tooltip="RNN - 题目76"></div>
							<div class="status-item unanswered" data-question="77" data-tooltip="RNN - 题目77"></div>
							<div class="status-item unanswered" data-question="78" data-tooltip="RNN - 题目78"></div>
							<div class="status-item unanswered" data-question="79" data-tooltip="RNN - 题目79"></div>
							<div class="status-item unanswered" data-question="80" data-tooltip="RNN - 题目80"></div>
							<div class="status-item unanswered" data-question="81" data-tooltip="RNN - 题目81"></div>
							<div class="status-item unanswered" data-question="82" data-tooltip="RNN - 题目82"></div>
							<div class="status-item unanswered" data-question="83" data-tooltip="RNN - 题目83"></div>
							<div class="status-item unanswered" data-question="84" data-tooltip="RNN - 题目84"></div>
							<div class="status-item unanswered" data-question="85" data-tooltip="RNN - 题目85"></div>
							<div class="status-item unanswered" data-question="86" data-tooltip="RNN - 题目86"></div>
							<div class="status-item unanswered" data-question="87" data-tooltip="RNN - 题目87"></div>
							<div class="status-item unanswered" data-question="88" data-tooltip="RNN - 题目88"></div>
							<div class="status-item unanswered" data-question="89" data-tooltip="RNN - 题目89"></div>
							<div class="status-item unanswered" data-question="90" data-tooltip="RNN - 题目90"></div>
							</div>
							<div class="module-stats">
							<span class="unanswered-count">未答: 15</span>
							<span class="correct-count">正确: 0</span>
							<span class="incorrect-count">错误: 0</span>
							</div>
						</div>
						
						<!-- Transformer：91-100题 -->
						<div class="status-subcategory">
							<h5>🎯 Transformer (10题)</h5>
							<div class="status-grid">
							<div class="status-item unanswered" data-question="91" data-tooltip="Transformer - 题目91"></div>
							<div class="status-item unanswered" data-question="92" data-tooltip="Transformer - 题目92"></div>
							<div class="status-item unanswered" data-question="93" data-tooltip="Transformer - 题目93"></div>
							<div class="status-item unanswered" data-question="94" data-tooltip="Transformer - 题目94"></div>
							<div class="status-item unanswered" data-question="95" data-tooltip="Transformer - 题目95"></div>
							<div class="status-item unanswered" data-question="96" data-tooltip="Transformer - 题目96"></div>
							<div class="status-item unanswered" data-question="97" data-tooltip="Transformer - 题目97"></div>
							<div class="status-item unanswered" data-question="98" data-tooltip="Transformer - 题目98"></div>
							<div class="status-item unanswered" data-question="99" data-tooltip="Transformer - 题目99"></div>
							<div class="status-item unanswered" data-question="100" data-tooltip="Transformer - 题目100"></div>
							</div>
							<div class="module-stats">
							<span class="unanswered-count">未答: 10</span>
							<span class="correct-count">正确: 0</span>
							<span class="incorrect-count">错误: 0</span>
							</div>
						</div>
						</div>
					</div>
				
					<!-- 综合应用模块进度 -->
					<div class="status-category">
						<h4>🚀 综合应用模块 (55题)</h4>
						
						<div class="status-subcategory-container">
						<!-- 架构选择与对比：101-120题 -->
						<div class="status-subcategory">
							<h5>📊 架构选择与对比 (20题)</h5>
							<div class="status-grid">
							<div class="status-item unanswered" data-question="101" data-tooltip="架构选择与对比 - 题目101"></div>
							<div class="status-item unanswered" data-question="102" data-tooltip="架构选择与对比 - 题目102"></div>
							<div class="status-item unanswered" data-question="103" data-tooltip="架构选择与对比 - 题目103"></div>
							<div class="status-item unanswered" data-question="104" data-tooltip="架构选择与对比 - 题目104"></div>
							<div class="status-item unanswered" data-question="105" data-tooltip="架构选择与对比 - 题目105"></div>
							<div class="status-item unanswered" data-question="106" data-tooltip="架构选择与对比 - 题目106"></div>
							<div class="status-item unanswered" data-question="107" data-tooltip="架构选择与对比 - 题目107"></div>
							<div class="status-item unanswered" data-question="108" data-tooltip="架构选择与对比 - 题目108"></div>
							<div class="status-item unanswered" data-question="109" data-tooltip="架构选择与对比 - 题目109"></div>
							<div class="status-item unanswered" data-question="110" data-tooltip="架构选择与对比 - 题目110"></div>
							<div class="status-item unanswered" data-question="111" data-tooltip="架构选择与对比 - 题目111"></div>
							<div class="status-item unanswered" data-question="112" data-tooltip="架构选择与对比 - 题目112"></div>
							<div class="status-item unanswered" data-question="113" data-tooltip="架构选择与对比 - 题目113"></div>
							<div class="status-item unanswered" data-question="114" data-tooltip="架构选择与对比 - 题目114"></div>
							<div class="status-item unanswered" data-question="115" data-tooltip="架构选择与对比 - 题目115"></div>
							<div class="status-item unanswered" data-question="116" data-tooltip="架构选择与对比 - 题目116"></div>
							<div class="status-item unanswered" data-question="117" data-tooltip="架构选择与对比 - 题目117"></div>
							<div class="status-item unanswered" data-question="118" data-tooltip="架构选择与对比 - 题目118"></div>
							<div class="status-item unanswered" data-question="119" data-tooltip="架构选择与对比 - 题目119"></div>
							<div class="status-item unanswered" data-question="120" data-tooltip="架构选择与对比 - 题目120"></div>
							</div>
							<div class="module-stats">
							<span class="unanswered-count">未答: 20</span>
							<span class="correct-count">正确: 0</span>
							<span class="incorrect-count">错误: 0</span>
							</div>
						</div>
						
						<!-- 问题诊断与调优：121-135题 -->
						<div class="status-subcategory">
							<h5>🔧 问题诊断与调优 (15题)</h5>
							<div class="status-grid">
							<div class="status-item unanswered" data-question="121" data-tooltip="问题诊断与调优 - 题目121"></div>
							<div class="status-item unanswered" data-question="122" data-tooltip="问题诊断与调优 - 题目122"></div>
							<div class="status-item unanswered" data-question="123" data-tooltip="问题诊断与调优 - 题目123"></div>
							<div class="status-item unanswered" data-question="124" data-tooltip="问题诊断与调优 - 题目124"></div>
							<div class="status-item unanswered" data-question="125" data-tooltip="问题诊断与调优 - 题目125"></div>
							<div class="status-item unanswered" data-question="126" data-tooltip="问题诊断与调优 - 题目126"></div>
							<div class="status-item unanswered" data-question="127" data-tooltip="问题诊断与调优 - 题目127"></div>
							<div class="status-item unanswered" data-question="128" data-tooltip="问题诊断与调优 - 题目128"></div>
							<div class="status-item unanswered" data-question="129" data-tooltip="问题诊断与调优 - 题目129"></div>
							<div class="status-item unanswered" data-question="130" data-tooltip="问题诊断与调优 - 题目130"></div>
							<div class="status-item unanswered" data-question="131" data-tooltip="问题诊断与调优 - 题目131"></div>
							<div class="status-item unanswered" data-question="132" data-tooltip="问题诊断与调优 - 题目132"></div>
							<div class="status-item unanswered" data-question="133" data-tooltip="问题诊断与调优 - 题目133"></div>
							<div class="status-item unanswered" data-question="134" data-tooltip="问题诊断与调优 - 题目134"></div>
							<div class="status-item unanswered" data-question="135" data-tooltip="问题诊断与调优 - 题目135"></div>
							</div>
							<div class="module-stats">
							<span class="unanswered-count">未答: 15</span>
							<span class="correct-count">正确: 0</span>
							<span class="incorrect-count">错误: 0</span>
							</div>
						</div>
						
						<!-- 前沿与交叉领域：136-155题 -->
						<div class="status-subcategory">
							<h5>📚 前沿与交叉领域 (20题)</h5>
							<div class="status-grid">
							<div class="status-item unanswered" data-question="136" data-tooltip="前沿与交叉领域 - 题目136"></div>
							<div class="status-item unanswered" data-question="137" data-tooltip="前沿与交叉领域 - 题目137"></div>
							<div class="status-item unanswered" data-question="138" data-tooltip="前沿与交叉领域 - 题目138"></div>
							<div class="status-item unanswered" data-question="139" data-tooltip="前沿与交叉领域 - 题目139"></div>
							<div class="status-item unanswered" data-question="140" data-tooltip="前沿与交叉领域 - 题目140"></div>
							<div class="status-item unanswered" data-question="141" data-tooltip="前沿与交叉领域 - 题目141"></div>
							<div class="status-item unanswered" data-question="142" data-tooltip="前沿与交叉领域 - 题目142"></div>
							<div class="status-item unanswered" data-question="143" data-tooltip="前沿与交叉领域 - 题目143"></div>
							<div class="status-item unanswered" data-question="144" data-tooltip="前沿与交叉领域 - 题目144"></div>
							<div class="status-item unanswered" data-question="145" data-tooltip="前沿与交叉领域 - 题目145"></div>
							<div class="status-item unanswered" data-question="146" data-tooltip="前沿与交叉领域 - 题目146"></div>
							<div class="status-item unanswered" data-question="147" data-tooltip="前沿与交叉领域 - 题目147"></div>
							<div class="status-item unanswered" data-question="148" data-tooltip="前沿与交叉领域 - 题目148"></div>
							<div class="status-item unanswered" data-question="149" data-tooltip="前沿与交叉领域 - 题目149"></div>
							<div class="status-item unanswered" data-question="150" data-tooltip="前沿与交叉领域 - 题目150"></div>
							<div class="status-item unanswered" data-question="151" data-tooltip="前沿与交叉领域 - 题目151"></div>
							<div class="status-item unanswered" data-question="152" data-tooltip="前沿与交叉领域 - 题目152"></div>
							<div class="status-item unanswered" data-question="153" data-tooltip="前沿与交叉领域 - 题目153"></div>
							<div class="status-item unanswered" data-question="154" data-tooltip="前沿与交叉领域 - 题目154"></div>
							<div class="status-item unanswered" data-question="155" data-tooltip="前沿与交叉领域 - 题目155"></div>
							</div>
							<div class="module-stats">
							<span class="unanswered-count">未答: 20</span>
							<span class="correct-count">正确: 0</span>
							<span class="incorrect-count">错误: 0</span>
							</div>
						</div>
						</div>
					</div>
					</div>
				</div>
				<!-- 右侧答题状态区域 -->

			</div>
		</div>

		<div id = 'loadingDataTab' style="display: none;">
			<div id='loadingMNIST'>
				加载中 <span id="datasetLoadingName">MNIST</span> 数据集
			</div>
		</div>

		<!-- Error popup -->
		<div id = 'error' style="display: none">
			<svg id = 'x' xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path fill="none" d="M0 0h24v24H0V0z"/><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
			<div id = 'errorMessage'> </div>
		</div>

	</div>


	<!-- The right panel -->
	<div id = 'paramshell'>
		<div class = 'trainbox' id = 'trainbox'>
			<div id = 'train' class = 'train' data-actionType = 'json'> 训练 </div>
		</div>

		<div class = 'category' id = 'kerasinfo'>
			<div class = 'categoryTitle' data-expanded = 'true'>
				<div class='expander'>
					<svg height="24px" width="24px">
						<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
					</svg>
				</div>
				<div class='categoryTitleText'>
					模型状态
				</div>
			</div>
			<div class = 'parambox'>
				<div id = 'ti_training' class = 'paramline'>
					<div class = 'paramname'>训练中</div>
					<div class = 'paramvalue'>否</div>
				</div>
				<div id = 'ti_acc' class = 'paramline'>
					<div class = 'paramname'>准确率：</div>
					<div class = 'paramvalue'>N/A</div>
				</div>
				<div id = 'ti_loss' class = 'paramline'>
					<div class = 'paramname'>损失率:</div>
					<div class = 'paramvalue'>N/A</div>
				</div>
				<div id = 'ti_vacc' class = 'paramline'>
					<div class = 'paramname'>验证集准确率:</div>
					<div class = 'paramvalue'>N/A</div>
				</div>
				<div id = 'ti_vloss' class = 'paramline'>
					<div class = 'paramname'>验证集损失率:</div>
					<div class = 'paramvalue'>N/A</div>
				</div>
			</div>
		</div>
		<div class="category">
			<div class = 'categoryTitle' data-expanded = 'true'>
				<div class='expander'>
					<svg height="24px" width="24px">
						<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
					</svg>
				</div>
				<div class='categoryTitleText'>
					导出代码
				</div>
			</div>
			<div id="exportPython" class="select-option right-option">导出 Python</div>
			<div id="exportJulia" class="select-option right-option">导出 Julia</div>
			<div id="copyModel" class="select-option right-option">模型链接</div>
		</div>
		<div id = 'networkParamshell'>

			<div class = 'category' id='paramtruck'>
				<div class = 'categoryTitle' data-expanded = 'true'>
					<div class='expander'>
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class='categoryTitleText'>
						参数
					</div>
				</div>
				<div id='defaultparambox' class = 'parambox'>点击一个层以查看和更改其参数。</div>
			</div>
		</div>

		<div id = 'progressParamshell' style="display: none">
			<div class = 'category' id='paramtruck'>
				<div class = 'categoryTitle' data-expanded = 'true'>
					<div class='expander'>
						<svg height="24px" width="24px">
							<path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z" style="fill:#FFFFFF;"></path>
						</svg>
					</div>
					<div class='categoryTitleText'>
						超参数
					</div>
				</div>
				<div class = 'parambox'>
					<div class = 'paramline'>
						<div class="paramname" data-name="lr">Learning Rate: </div>
						<input id="learningRate" class="paramvalue hyperparamvalue" value="0.01">
					</div>
					<div class = 'paramline'>
						<div class="paramname" data-name="epochs">Epochs: </div>
						<input id="epochs" class="paramvalue hyperparamvalue" value="6">
					</div>
					<div class = 'paramline'>
						<div class="paramname" data-name="lr">Batch Size: </div>
						<input id="batchSize" class="paramvalue hyperparamvalue" value="64">
					</div>
				</div>
			</div>
		</div>

		<div id = 'visualizationParamshell' style="display: none">
		</div>


		<div id = 'educationParamshell' style="display: none">
		</div>
	</div>
	<!-- End of paramshell -->

	<!-- Exercise menu - moved outside paramshell to avoid being hidden when paramshell is hidden -->
	<div id = 'exerciseMenu' style="display: none">
			<div class="exercise-right-status">
				<div class="status-header">
					<h3>答题进度</h3>
				</div>
				<div class="status-content">
					<!-- 基础组件模块进度 -->
					<div class="status-category">
						<h4>基础组件模块 (50题)</h4>
						<div class="status-grid">
							<!-- 神经网络层：1-30题 -->
							<div class="status-subcategory">
								<h5>神经网络层</h5>
								<div class="status-grid">
									<!-- 1-30题的小方块 -->
									<div class="status-item unanswered" data-question="1" data-tooltip="神经网络层 - 题目1"></div>
									<div class="status-item unanswered" data-question="2" data-tooltip="神经网络层 - 题目2"></div>
									<div class="status-item unanswered" data-question="3" data-tooltip="神经网络层 - 题目3"></div>
									<div class="status-item unanswered" data-question="4" data-tooltip="神经网络层 - 题目4"></div>
									<div class="status-item unanswered" data-question="5" data-tooltip="神经网络层 - 题目5"></div>
									<div class="status-item unanswered" data-question="6" data-tooltip="神经网络层 - 题目6"></div>
									<div class="status-item unanswered" data-question="7" data-tooltip="神经网络层 - 题目7"></div>
									<div class="status-item unanswered" data-question="8" data-tooltip="神经网络层 - 题目8"></div>
									<div class="status-item unanswered" data-question="9" data-tooltip="神经网络层 - 题目9"></div>
									<div class="status-item unanswered" data-question="10" data-tooltip="神经网络层 - 题目10"></div>
									<div class="status-item unanswered" data-question="11" data-tooltip="神经网络层 - 题目11"></div>
									<div class="status-item unanswered" data-question="12" data-tooltip="神经网络层 - 题目12"></div>
									<div class="status-item unanswered" data-question="13" data-tooltip="神经网络层 - 题目13"></div>
									<div class="status-item unanswered" data-question="14" data-tooltip="神经网络层 - 题目14"></div>
									<div class="status-item unanswered" data-question="15" data-tooltip="神经网络层 - 题目15"></div>
									<div class="status-item unanswered" data-question="16" data-tooltip="神经网络层 - 题目16"></div>
									<div class="status-item unanswered" data-question="17" data-tooltip="神经网络层 - 题目17"></div>
									<div class="status-item unanswered" data-question="18" data-tooltip="神经网络层 - 题目18"></div>
									<div class="status-item unanswered" data-question="19" data-tooltip="神经网络层 - 题目19"></div>
									<div class="status-item unanswered" data-question="20" data-tooltip="神经网络层 - 题目20"></div>
									<div class="status-item unanswered" data-question="21" data-tooltip="神经网络层 - 题目21"></div>
									<div class="status-item unanswered" data-question="22" data-tooltip="神经网络层 - 题目22"></div>
									<div class="status-item unanswered" data-question="23" data-tooltip="神经网络层 - 题目23"></div>
									<div class="status-item unanswered" data-question="24" data-tooltip="神经网络层 - 题目24"></div>
									<div class="status-item unanswered" data-question="25" data-tooltip="神经网络层 - 题目25"></div>
									<div class="status-item unanswered" data-question="26" data-tooltip="神经网络层 - 题目26"></div>
									<div class="status-item unanswered" data-question="27" data-tooltip="神经网络层 - 题目27"></div>
									<div class="status-item unanswered" data-question="28" data-tooltip="神经网络层 - 题目28"></div>
									<div class="status-item unanswered" data-question="29" data-tooltip="神经网络层 - 题目29"></div>
									<div class="status-item unanswered" data-question="30" data-tooltip="神经网络层 - 题目30"></div>
								</div>
							</div>
							
							<!-- 激活函数：31-50题 -->
							<div class="status-subcategory">
								<h5>激活函数</h5>
								<div class="status-grid">
									<!-- 31-50题的小方块 -->
									<div class="status-item unanswered" data-question="31" data-tooltip="激活函数 - 题目31"></div>
									<div class="status-item unanswered" data-question="32" data-tooltip="激活函数 - 题目32"></div>
									<div class="status-item unanswered" data-question="33" data-tooltip="激活函数 - 题目33"></div>
									<div class="status-item unanswered" data-question="34" data-tooltip="激活函数 - 题目34"></div>
									<div class="status-item unanswered" data-question="35" data-tooltip="激活函数 - 题目35"></div>
									<div class="status-item unanswered" data-question="36" data-tooltip="激活函数 - 题目36"></div>
									<div class="status-item unanswered" data-question="37" data-tooltip="激活函数 - 题目37"></div>
									<div class="status-item unanswered" data-question="38" data-tooltip="激活函数 - 题目38"></div>
									<div class="status-item unanswered" data-question="39" data-tooltip="激活函数 - 题目39"></div>
									<div class="status-item unanswered" data-question="40" data-tooltip="激活函数 - 题目40"></div>
									<div class="status-item unanswered" data-question="41" data-tooltip="激活函数 - 题目41"></div>
									<div class="status-item unanswered" data-question="42" data-tooltip="激活函数 - 题目42"></div>
									<div class="status-item unanswered" data-question="43" data-tooltip="激活函数 - 题目43"></div>
									<div class="status-item unanswered" data-question="44" data-tooltip="激活函数 - 题目44"></div>
									<div class="status-item unanswered" data-question="45" data-tooltip="激活函数 - 题目45"></div>
									<div class="status-item unanswered" data-question="46" data-tooltip="激活函数 - 题目46"></div>
									<div class="status-item unanswered" data-question="47" data-tooltip="激活函数 - 题目47"></div>
									<div class="status-item unanswered" data-question="48" data-tooltip="激活函数 - 题目48"></div>
									<div class="status-item unanswered" data-question="49" data-tooltip="激活函数 - 题目49"></div>
									<div class="status-item unanswered" data-question="50" data-tooltip="激活函数 - 题目50"></div>
								</div>
							</div>
						</div>
					</div>

					<!-- 核心架构模块进度 -->
					<div class="status-category">
						<h4>核心架构模块 (50题)</h4>
						<div class="status-grid">
							<!-- 感知机：51-60题 -->
							<div class="status-subcategory">
								<h5>感知机</h5>
								<div class="status-grid">
									<div class="status-item unanswered" data-question="51" data-tooltip="感知机 - 题目51"></div>
									<div class="status-item unanswered" data-question="52" data-tooltip="感知机 - 题目52"></div>
									<div class="status-item unanswered" data-question="53" data-tooltip="感知机 - 题目53"></div>
									<div class="status-item unanswered" data-question="54" data-tooltip="感知机 - 题目54"></div>
									<div class="status-item unanswered" data-question="55" data-tooltip="感知机 - 题目55"></div>
									<div class="status-item unanswered" data-question="56" data-tooltip="感知机 - 题目56"></div>
									<div class="status-item unanswered" data-question="57" data-tooltip="感知机 - 题目57"></div>
									<div class="status-item unanswered" data-question="58" data-tooltip="感知机 - 题目58"></div>
									<div class="status-item unanswered" data-question="59" data-tooltip="感知机 - 题目59"></div>
									<div class="status-item unanswered" data-question="60" data-tooltip="感知机 - 题目60"></div>
								</div>
							</div>
							
							<!-- CNN：61-80题 -->
							<div class="status-subcategory">
								<h5>CNN</h5>
								<div class="status-grid">
									<div class="status-item unanswered" data-question="61" data-tooltip="CNN - 题目61"></div>
									<div class="status-item unanswered" data-question="62" data-tooltip="CNN - 题目62"></div>
									<div class="status-item unanswered" data-question="63" data-tooltip="CNN - 题目63"></div>
									<div class="status-item unanswered" data-question="64" data-tooltip="CNN - 题目64"></div>
									<div class="status-item unanswered" data-question="65" data-tooltip="CNN - 题目65"></div>
									<div class="status-item unanswered" data-question="66" data-tooltip="CNN - 题目66"></div>
									<div class="status-item unanswered" data-question="67" data-tooltip="CNN - 题目67"></div>
									<div class="status-item unanswered" data-question="68" data-tooltip="CNN - 题目68"></div>
									<div class="status-item unanswered" data-question="69" data-tooltip="CNN - 题目69"></div>
									<div class="status-item unanswered" data-question="70" data-tooltip="CNN - 题目70"></div>
									<div class="status-item unanswered" data-question="71" data-tooltip="CNN - 题目71"></div>
									<div class="status-item unanswered" data-question="72" data-tooltip="CNN - 题目72"></div>
									<div class="status-item unanswered" data-question="73" data-tooltip="CNN - 题目73"></div>
									<div class="status-item unanswered" data-question="74" data-tooltip="CNN - 题目74"></div>
									<div class="status-item unanswered" data-question="75" data-tooltip="CNN - 题目75"></div>
									<div class="status-item unanswered" data-question="76" data-tooltip="CNN - 题目76"></div>
									<div class="status-item unanswered" data-question="77" data-tooltip="CNN - 题目77"></div>
									<div class="status-item unanswered" data-question="78" data-tooltip="CNN - 题目78"></div>
									<div class="status-item unanswered" data-question="79" data-tooltip="CNN - 题目79"></div>
									<div class="status-item unanswered" data-question="80" data-tooltip="CNN - 题目80"></div>
								</div>
							</div>
							
							<!-- RNN：81-100题 -->
							<div class="status-subcategory">
								<h5>RNN</h5>
								<div class="status-grid">
									<div class="status-item unanswered" data-question="81" data-tooltip="RNN - 题目81"></div>
									<div class="status-item unanswered" data-question="82" data-tooltip="RNN - 题目82"></div>
									<div class="status-item unanswered" data-question="83" data-tooltip="RNN - 题目83"></div>
									<div class="status-item unanswered" data-question="84" data-tooltip="RNN - 题目84"></div>
									<div class="status-item unanswered" data-question="85" data-tooltip="RNN - 题目85"></div>
									<div class="status-item unanswered" data-question="86" data-tooltip="RNN - 题目86"></div>
									<div class="status-item unanswered" data-question="87" data-tooltip="RNN - 题目87"></div>
									<div class="status-item unanswered" data-question="88" data-tooltip="RNN - 题目88"></div>
									<div class="status-item unanswered" data-question="89" data-tooltip="RNN - 题目89"></div>
									<div class="status-item unanswered" data-question="90" data-tooltip="RNN - 题目90"></div>
									<div class="status-item unanswered" data-question="91" data-tooltip="RNN - 题目91"></div>
									<div class="status-item unanswered" data-question="92" data-tooltip="RNN - 题目92"></div>
									<div class="status-item unanswered" data-question="93" data-tooltip="RNN - 题目93"></div>
									<div class="status-item unanswered" data-question="94" data-tooltip="RNN - 题目94"></div>
									<div class="status-item unanswered" data-question="95" data-tooltip="RNN - 题目95"></div>
									<div class="status-item unanswered" data-question="96" data-tooltip="RNN - 题目96"></div>
									<div class="status-item unanswered" data-question="97" data-tooltip="RNN - 题目97"></div>
									<div class="status-item unanswered" data-question="98" data-tooltip="RNN - 题目98"></div>
									<div class="status-item unanswered" data-question="99" data-tooltip="RNN - 题目99"></div>
									<div class="status-item unanswered" data-question="100" data-tooltip="RNN - 题目100"></div>
								</div>
							</div>
							
							<!-- Transformer：101-110题 -->
							<div class="status-subcategory">
								<h5>Transformer</h5>
								<div class="status-grid">
									<div class="status-item unanswered" data-question="101" data-tooltip="Transformer - 题目101"></div>
									<div class="status-item unanswered" data-question="102" data-tooltip="Transformer - 题目102"></div>
									<div class="status-item unanswered" data-question="103" data-tooltip="Transformer - 题目103"></div>
									<div class="status-item unanswered" data-question="104" data-tooltip="Transformer - 题目104"></div>
									<div class="status-item unanswered" data-question="105" data-tooltip="Transformer - 题目105"></div>
									<div class="status-item unanswered" data-question="106" data-tooltip="Transformer - 题目106"></div>
									<div class="status-item unanswered" data-question="107" data-tooltip="Transformer - 题目107"></div>
									<div class="status-item unanswered" data-question="108" data-tooltip="Transformer - 题目108"></div>
									<div class="status-item unanswered" data-question="109" data-tooltip="Transformer - 题目109"></div>
									<div class="status-item unanswered" data-question="110" data-tooltip="Transformer - 题目110"></div>
								</div>
							</div>
						</div>
					</div>

					<!-- 综合应用模块进度 -->
					<div class="status-category">
						<h4>综合应用模块 (55题)</h4>
						<div class="status-grid">
							<!-- 架构选择与对比：111-130题 -->
							<div class="status-subcategory">
								<h5>架构选择与对比</h5>
								<div class="status-grid">
									<div class="status-item unanswered" data-question="111" data-tooltip="架构选择与对比 - 题目111"></div>
									<div class="status-item unanswered" data-question="112" data-tooltip="架构选择与对比 - 题目112"></div>
									<div class="status-item unanswered" data-question="113" data-tooltip="架构选择与对比 - 题目113"></div>
									<div class="status-item unanswered" data-question="114" data-tooltip="架构选择与对比 - 题目114"></div>
									<div class="status-item unanswered" data-question="115" data-tooltip="架构选择与对比 - 题目115"></div>
									<div class="status-item unanswered" data-question="116" data-tooltip="架构选择与对比 - 题目116"></div>
									<div class="status-item unanswered" data-question="117" data-tooltip="架构选择与对比 - 题目117"></div>
									<div class="status-item unanswered" data-question="118" data-tooltip="架构选择与对比 - 题目118"></div>
									<div class="status-item unanswered" data-question="119" data-tooltip="架构选择与对比 - 题目119"></div>
									<div class="status-item unanswered" data-question="120" data-tooltip="架构选择与对比 - 题目120"></div>
									<div class="status-item unanswered" data-question="121" data-tooltip="架构选择与对比 - 题目121"></div>
									<div class="status-item unanswered" data-question="122" data-tooltip="架构选择与对比 - 题目122"></div>
									<div class="status-item unanswered" data-question="123" data-tooltip="架构选择与对比 - 题目123"></div>
									<div class="status-item unanswered" data-question="124" data-tooltip="架构选择与对比 - 题目124"></div>
									<div class="status-item unanswered" data-question="125" data-tooltip="架构选择与对比 - 题目125"></div>
									<div class="status-item unanswered" data-question="126" data-tooltip="架构选择与对比 - 题目126"></div>
									<div class="status-item unanswered" data-question="127" data-tooltip="架构选择与对比 - 题目127"></div>
									<div class="status-item unanswered" data-question="128" data-tooltip="架构选择与对比 - 题目128"></div>
									<div class="status-item unanswered" data-question="129" data-tooltip="架构选择与对比 - 题目129"></div>
									<div class="status-item unanswered" data-question="130" data-tooltip="架构选择与对比 - 题目130"></div>
								</div>
							</div>
							
							<!-- 问题诊断与调优：131-150题 -->
							<div class="status-subcategory">
								<h5>问题诊断与调优</h5>
								<div class="status-grid">
									<div class="status-item unanswered" data-question="131" data-tooltip="问题诊断与调优 - 题目131"></div>
									<div class="status-item unanswered" data-question="132" data-tooltip="问题诊断与调优 - 题目132"></div>
									<div class="status-item unanswered" data-question="133" data-tooltip="问题诊断与调优 - 题目133"></div>
									<div class="status-item unanswered" data-question="134" data-tooltip="问题诊断与调优 - 题目134"></div>
									<div class="status-item unanswered" data-question="135" data-tooltip="问题诊断与调优 - 题目135"></div>
									<div class="status-item unanswered" data-question="136" data-tooltip="问题诊断与调优 - 题目136"></div>
									<div class="status-item unanswered" data-question="137" data-tooltip="问题诊断与调优 - 题目137"></div>
									<div class="status-item unanswered" data-question="138" data-tooltip="问题诊断与调优 - 题目138"></div>
									<div class="status-item unanswered" data-question="139" data-tooltip="问题诊断与调优 - 题目139"></div>
									<div class="status-item unanswered" data-question="140" data-tooltip="问题诊断与调优 - 题目140"></div>
									<div class="status-item unanswered" data-question="141" data-tooltip="问题诊断与调优 - 题目141"></div>
									<div class="status-item unanswered" data-question="142" data-tooltip="问题诊断与调优 - 题目142"></div>
									<div class="status-item unanswered" data-question="143" data-tooltip="问题诊断与调优 - 题目143"></div>
									<div class="status-item unanswered" data-question="144" data-tooltip="问题诊断与调优 - 题目144"></div>
									<div class="status-item unanswered" data-question="145" data-tooltip="问题诊断与调优 - 题目145"></div>
									<div class="status-item unanswered" data-question="146" data-tooltip="问题诊断与调优 - 题目146"></div>
									<div class="status-item unanswered" data-question="147" data-tooltip="问题诊断与调优 - 题目147"></div>
									<div class="status-item unanswered" data-question="148" data-tooltip="问题诊断与调优 - 题目148"></div>
									<div class="status-item unanswered" data-question="149" data-tooltip="问题诊断与调优 - 题目149"></div>
									<div class="status-item unanswered" data-question="150" data-tooltip="问题诊断与调优 - 题目150"></div>
								</div>
							</div>
							
							<!-- 前沿与交叉领域：151-165题 -->
							<div class="status-subcategory">
								<h5>前沿与交叉领域</h5>
								<div class="status-grid">
									<div class="status-item unanswered" data-question="151" data-tooltip="前沿与交叉领域 - 题目151"></div>
									<div class="status-item unanswered" data-question="152" data-tooltip="前沿与交叉领域 - 题目152"></div>
									<div class="status-item unanswered" data-question="153" data-tooltip="前沿与交叉领域 - 题目153"></div>
									<div class="status-item unanswered" data-question="154" data-tooltip="前沿与交叉领域 - 题目154"></div>
									<div class="status-item unanswered" data-question="155" data-tooltip="前沿与交叉领域 - 题目155"></div>
									<div class="status-item unanswered" data-question="156" data-tooltip="前沿与交叉领域 - 题目156"></div>
									<div class="status-item unanswered" data-question="157" data-tooltip="前沿与交叉领域 - 题目157"></div>
									<div class="status-item unanswered" data-question="158" data-tooltip="前沿与交叉领域 - 题目158"></div>
									<div class="status-item unanswered" data-question="159" data-tooltip="前沿与交叉领域 - 题目159"></div>
									<div class="status-item unanswered" data-question="160" data-tooltip="前沿与交叉领域 - 题目160"></div>
									<div class="status-item unanswered" data-question="161" data-tooltip="前沿与交叉领域 - 题目161"></div>
									<div class="status-item unanswered" data-question="162" data-tooltip="前沿与交叉领域 - 题目162"></div>
									<div class="status-item unanswered" data-question="163" data-tooltip="前沿与交叉领域 - 题目163"></div>
									<div class="status-item unanswered" data-question="164" data-tooltip="前沿与交叉领域 - 题目164"></div>
									<div class="status-item unanswered" data-question="165" data-tooltip="前沿与交叉领域 - 题目165"></div>
								</div>
							</div>
						</div>
					</div>
				</div>
			</div>
		</div>
	<!-- End of exerciseMenu -->
    <!-- 修复开始练习按钮点击无响应和页面滚动问题 -->
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // 保持body和html的overflow为hidden，只让中间的#middle区域滚动
            
            // 修复开始练习按钮点击无响应问题
            const startButtons = document.querySelectorAll(".start-exercise-btn");
            console.log("找到的开始练习按钮数量:", startButtons.length);
            startButtons.forEach((button, index) => {
                console.log(`为按钮 ${index} 添加事件监听器`);
                // 移除可能存在的旧事件监听器
                button.onclick = null;
                button.addEventListener("click", function(event) {
                    event.stopPropagation();
                    console.log("按钮被点击了");
                    const card = this.closest(".exercise-card");
                    console.log("找到的练习卡片:", card);
                    if (card) {
                        const content = card.querySelector(".card-content");
                        const questions = card.querySelector(".card-questions");
                        console.log("找到的card-content:", content);
                        console.log("找到的card-questions:", questions);
                        if (content && questions) {
                            content.style.display = "none";
                            questions.style.display = "block";
                            questions.scrollIntoView({ behavior: "smooth", block: "center" });
                            console.log("已切换到题目视图");
                        } else {
                            console.log("未找到card-content或card-questions元素");
                        }
                    } else {
                        console.log("未找到练习卡片");
                    }
                });
            });
            
            // 为收起练习按钮添加事件监听器
            const closeButtons = document.querySelectorAll(".close-exercise-btn");
            console.log("找到的收起练习按钮数量:", closeButtons.length);
            closeButtons.forEach((button, index) => {
                console.log(`为收起按钮 ${index} 添加事件监听器`);
                // 移除可能存在的旧事件监听器
                button.onclick = null;
                button.addEventListener("click", function(event) {
                    event.stopPropagation();
                    console.log("收起按钮被点击了");
                    const card = this.closest(".exercise-card");
                    console.log("找到的练习卡片:", card);
                    if (card) {
                        const content = card.querySelector(".card-content");
                        const questions = card.querySelector(".card-questions");
                        console.log("找到的card-content:", content);
                        console.log("找到的card-questions:", questions);
                        if (content && questions) {
                            questions.style.display = "none";
                            content.style.display = "block";
                            content.scrollIntoView({ behavior: "smooth", block: "center" });
                            console.log("已收起练习");
                        } else {
                            console.log("未找到card-content或card-questions元素");
                        }
                    } else {
                        console.log("未找到练习卡片");
                    }
                });
            });
            
            // 直接为提交答案按钮添加事件监听器，确保点击有反应
            const submitButtons = document.querySelectorAll(".submit-quiz-btn");
            console.log("找到的提交答案按钮数量:", submitButtons.length);
            submitButtons.forEach((button, index) => {
                console.log(`为提交按钮 ${index} 添加事件监听器`);
                // 移除可能存在的旧事件监听器
                button.onclick = null;
                button.addEventListener("click", function(event) {
                    event.stopPropagation();
                    console.log("提交按钮被点击了");
                    const category = this.getAttribute("data-category");
                    console.log("按钮的data-category属性值:", category);
                    if (category) {
                        // 这里直接实现提交答案的逻辑，不依赖外部函数
                        console.log(`准备处理${category}的答案提交`);
                        
                        // 首先找到对应的练习卡片
                        const card = this.closest(".exercise-card");
                        console.log("找到的练习卡片:", card);
                        if (!card) {
                            console.error("未找到练习卡片");
                            return;
                        }
                        
                        const questionsContainer = card.querySelector(".card-questions");
                        if (!questionsContainer) {
                            console.error("未找到题目容器");
                            return;
                        }
                        
                        const questions = questionsContainer.querySelectorAll(".question");
                        if (!questions.length) {
                            console.error("未找到题目");
                            return;
                        }
                        
                        // 定义正确答案，与exercises.ts中的定义一致
                        const quizAnswers = {
                            overview_quiz: {
                                1: "B",
                                2: "C",
                                3: "A"
                            },
                            overfitting_quiz: {
                                1: "C",
                                2: "B",
                                3: "A"
                            },
                            mlp_quiz: {
                                1: "A",
                                2: "B",
                                3: "D"
                            },
                            cnn_quiz: {
                                1: "B",
                                2: "A",
                                3: "C"
                            },
                            modern_cnn_quiz: {
                                1: "C",
                                2: "B",
                                3: "D"
                            },
                            rnn_quiz: {
                                1: "A",
                                2: "D",
                                3: "B"
                            },
                            modern_rnn_quiz: {
                                1: "B",
                                2: "C",
                                3: "A"
                            },
                            transformer_quiz: {
                                1: "C",
                                2: "A",
                                3: "D"
                            },
                            activations_quiz: {
                                1: "B",
                                2: "A",
                                3: "C"
                            }
                        };
                        
                        const answers = quizAnswers[category] || {};
                        let score = 0;
                        
                        // 提取category的前缀部分（例如从overview_quiz提取overview）
                        const categoryPrefix = category.split('_')[0];
                        
                        questions.forEach((qEl, index) => {
                            const questionIndex = index + 1;
                            const selected = qEl.querySelector(`input[name="${categoryPrefix}_q${questionIndex}"]:checked`);
                            
                            const existingFeedback = qEl.querySelector(".answer-feedback");
                            if (existingFeedback) existingFeedback.remove();
                            
                            const feedback = document.createElement("div");
                            feedback.classList.add("answer-feedback");
                            
                            const correct = answers[questionIndex];
                            if (selected) {
                                if (selected.value === correct) {
                                    score++;
                                    feedback.classList.add("correct");
                                    feedback.textContent = `✓ 回答正确！正确答案：${correct}`;
                                } else {
                                    feedback.classList.add("incorrect");
                                    feedback.textContent = `✗ 回答错误。正确答案：${correct}`;
                                }
                            } else {
                                feedback.classList.add("incorrect");
                                feedback.textContent = `⚠ 请先选择一个选项。正确答案：${correct}`;
                            }
                            
                            qEl.appendChild(feedback);
                        });
                        
                        // 更新卡片分数显示
                        const scoreElement = card.querySelector(".score-display");
                        if (scoreElement) {
                            scoreElement.textContent = `本次得分：${score} / ${questions.length}`;
                            scoreElement.style.display = "block";
                        }
                        
                        console.log("答案提交完成，得分：", score);
                    } else {
                        console.log("未找到data-category属性");
                    }
                });
            });
            
            // 实现练习页面导航联动功能
            document.addEventListener('DOMContentLoaded', () => {
                // 建立导航项和练习卡片的映射关系
                const exerciseMapping = {
                    'neural-layers': 'neural_layers_quiz',
                    'activations': 'activations_quiz',
                    'perceptron': 'perceptron_quiz',
                    'cnn': 'cnn_quiz',
                    'rnn': 'rnn_quiz',
                    'transformer': 'transformer_quiz',
                    'arch-selection': 'arch_selection_quiz',
                    'problem-diagnosis': 'problem_diagnosis_quiz',
                    'advanced': 'advanced_quiz'
                };
                
                // 获取所有左侧导航栏的子分类项
                const navItems = document.querySelectorAll('.exercise-left-nav .nav-subcategory[data-subcategory]');
                
                // 为每个子分类项添加点击事件监听器
                navItems.forEach(item => {
                    item.addEventListener('click', () => {
                        const subcategory = item.getAttribute('data-subcategory');
                        const exerciseId = exerciseMapping[subcategory];
                        
                        if (exerciseId) {
                            // 移除所有导航项的active状态
                            navItems.forEach(nav => nav.classList.remove('active'));
                            // 添加当前导航项的active状态
                            item.classList.add('active');
                            
                            // 找到对应的练习卡片
                            const targetCard = document.querySelector(`.exercise-card[data-exercise="${exerciseId}"]`);
                            
                            if (targetCard) {
                                // 获取中间内容区域容器
                                const centerContent = document.querySelector('.exercise-center-content');
                                
                                if (centerContent) {
                                    // 计算卡片相对于容器的位置
                                    const cardRect = targetCard.getBoundingClientRect();
                                    const containerRect = centerContent.getBoundingClientRect();
                                    
                                    // 滚动到卡片位置（留出一些顶部间距）
                                    const scrollTop = centerContent.scrollTop + cardRect.top - containerRect.top - 20;
                                    
                                    centerContent.scrollTo({
                                        top: Math.max(0, scrollTop),
                                        behavior: 'smooth'
                                    });
                                    
                                    // 高亮显示对应的题目卡片
                                    const allCards = document.querySelectorAll('.exercise-center-content .exercise-card');
                                    allCards.forEach(card => card.classList.remove('active'));
                                    targetCard.classList.add('active');
                                    
                                    // 添加一个短暂的视觉反馈（2秒后移除卡片高亮，但保留导航项高亮）
                                    setTimeout(() => {
                                        targetCard.classList.remove('active');
                                    }, 2000);
                                } else {
                                    // 如果没有找到容器，使用默认的滚动方法
                                    targetCard.scrollIntoView({
                                        behavior: 'smooth',
                                        block: 'start'
                                    });
                                    
                                    // 高亮显示对应的题目卡片
                                    const allCards = document.querySelectorAll('.exercise-center-content .exercise-card');
                                    allCards.forEach(card => card.classList.remove('active'));
                                    targetCard.classList.add('active');
                                    
                                    setTimeout(() => {
                                        targetCard.classList.remove('active');
                                    }, 2000);
                                }
                            } else {
                                console.log(`未找到对应的练习卡片: ${exerciseId}`);
                            }
                        } else {
                            console.log(`未找到对应的映射关系: ${subcategory}`);
                        }
                    });
                });
            });
        });
        
        // 处理跳过登录按钮
        document.addEventListener('DOMContentLoaded', function() {
            const skipLoginButton = document.getElementById('skipLoginButton');
            if (skipLoginButton) {
                skipLoginButton.addEventListener('click', function() {
                    // 隐藏登录页面
                    const loginPage = document.getElementById('loginPage');
                    if (loginPage) {
                        loginPage.style.display = 'none';
                    }
                    
                    // 显示主页
                    const main = document.getElementById('main');
                    if (main) {
                        main.classList.remove('hidden');
                    }
                });
            }
        });
    </script>
</body>
</html>

